
<!DOCTYPE html>

<html data-content_root="" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>langchain_google_vertexai.chat_models â€” ðŸ¦œðŸ”— LangChain  documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/autodoc_pydantic.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
<script src="../../_static/doctools.js"></script>
<script src="../../_static/sphinx_highlight.js"></script>
<script src="../../_static/clipboard.min.js"></script>
<script src="../../_static/copybutton.js"></script>
<script src="../../_static/design-tabs.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = '_modules/langchain_google_vertexai/chat_models';</script>
<link href="../../_static/favicon.png" rel="icon"/>
<link href="../../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Sep 05, 2024" name="docbuild:last-update"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../../index.html">
<img alt="ðŸ¦œðŸ”— LangChain  documentation - Home" class="logo__image only-light" src="../../_static/wordmark-api.svg"/>
<script>document.write(`<img src="../../_static/wordmark-api-dark.svg" class="logo__image only-dark" alt="ðŸ¦œðŸ”— LangChain  documentation - Home"/>`);</script>
</a></div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-external" href="https://api.python.langchain.com/">
    Legacy reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item navbar-persistent--container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="navbar-persistent--mobile">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-external" href="https://api.python.langchain.com/">
    Legacy reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../index.html">Module code</a></li>
<li aria-current="page" class="breadcrumb-item active">langchain_go...</li>
</ul>
</nav>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<h1>Source code for langchain_google_vertexai.chat_models</h1><div class="highlight"><pre>
<span></span><span class="sd">"""Wrapper around Google VertexAI chat-based models."""</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>  <span class="c1"># noqa</span>
<span class="kn">import</span> <span class="nn">ast</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">AsyncIterator</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
    <span class="n">Literal</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">TypedDict</span><span class="p">,</span>
    <span class="n">overload</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">proto</span>  <span class="c1"># type: ignore[import-untyped]</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">telemetry</span>

<span class="kn">from</span> <span class="nn">langchain_core.callbacks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">,</span>
    <span class="n">CallbackManagerForLLMRun</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.language_models</span> <span class="kn">import</span> <span class="n">LanguageModelInput</span>
<span class="kn">from</span> <span class="nn">langchain_core.language_models.chat_models</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseChatModel</span><span class="p">,</span>
    <span class="n">LangSmithParams</span><span class="p">,</span>
    <span class="n">generate_from_stream</span><span class="p">,</span>
    <span class="n">agenerate_from_stream</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AIMessage</span><span class="p">,</span>
    <span class="n">AIMessageChunk</span><span class="p">,</span>
    <span class="n">BaseMessage</span><span class="p">,</span>
    <span class="n">FunctionMessage</span><span class="p">,</span>
    <span class="n">HumanMessage</span><span class="p">,</span>
    <span class="n">SystemMessage</span><span class="p">,</span>
    <span class="n">ToolCall</span><span class="p">,</span>
    <span class="n">ToolMessage</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages.ai</span> <span class="kn">import</span> <span class="n">UsageMetadata</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages.tool</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">tool_call_chunk</span><span class="p">,</span>
    <span class="n">tool_call</span> <span class="k">as</span> <span class="n">create_tool_call</span><span class="p">,</span>
    <span class="n">invalid_tool_call</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers.base</span> <span class="kn">import</span> <span class="n">OutputParserLike</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers.openai_tools</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">JsonOutputToolsParser</span><span class="p">,</span>
    <span class="n">PydanticToolsParser</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers.openai_tools</span> <span class="kn">import</span> <span class="n">parse_tool_calls</span>
<span class="kn">from</span> <span class="nn">langchain_core.outputs</span> <span class="kn">import</span> <span class="n">ChatGeneration</span><span class="p">,</span> <span class="n">ChatGenerationChunk</span><span class="p">,</span> <span class="n">ChatResult</span>
<span class="kn">from</span> <span class="nn">langchain_core.pydantic_v1</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">root_validator</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">Runnable</span><span class="p">,</span> <span class="n">RunnablePassthrough</span><span class="p">,</span> <span class="n">RunnableGenerator</span>
<span class="kn">from</span> <span class="nn">langchain_core.utils.function_calling</span> <span class="kn">import</span> <span class="n">convert_to_openai_tool</span>
<span class="kn">from</span> <span class="nn">langchain_core.utils.pydantic</span> <span class="kn">import</span> <span class="n">is_basemodel_subclass</span>
<span class="kn">from</span> <span class="nn">vertexai.generative_models</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">Tool</span> <span class="k">as</span> <span class="n">VertexTool</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">vertexai.generative_models._generative_models</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">ToolConfig</span><span class="p">,</span>
    <span class="n">SafetySettingsType</span><span class="p">,</span>
    <span class="n">GenerationConfigType</span><span class="p">,</span>
    <span class="n">GenerationResponse</span><span class="p">,</span>
    <span class="n">_convert_schema_dict_to_gapic</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">vertexai.language_models</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">ChatMessage</span><span class="p">,</span>
    <span class="n">ChatModel</span><span class="p">,</span>
    <span class="n">ChatSession</span><span class="p">,</span>
    <span class="n">CodeChatModel</span><span class="p">,</span>
    <span class="n">CodeChatSession</span><span class="p">,</span>
    <span class="n">InputOutputTextPair</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">vertexai.preview.language_models</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
    <span class="n">ChatModel</span> <span class="k">as</span> <span class="n">PreviewChatModel</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">vertexai.preview.language_models</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">CodeChatModel</span> <span class="k">as</span> <span class="n">PreviewCodeChatModel</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">google.cloud.aiplatform_v1beta1.types</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Blob</span><span class="p">,</span>
    <span class="n">Candidate</span><span class="p">,</span>
    <span class="n">Part</span><span class="p">,</span>
    <span class="n">HarmCategory</span><span class="p">,</span>
    <span class="n">Content</span><span class="p">,</span>
    <span class="n">FileData</span><span class="p">,</span>
    <span class="n">FunctionCall</span><span class="p">,</span>
    <span class="n">FunctionResponse</span><span class="p">,</span>
    <span class="n">GenerateContentRequest</span><span class="p">,</span>
    <span class="n">GenerationConfig</span><span class="p">,</span>
    <span class="n">SafetySetting</span><span class="p">,</span>
    <span class="n">Tool</span> <span class="k">as</span> <span class="n">GapicTool</span><span class="p">,</span>
    <span class="n">ToolConfig</span> <span class="k">as</span> <span class="n">GapicToolConfig</span><span class="p">,</span>
    <span class="n">VideoMetadata</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_google_vertexai._base</span> <span class="kn">import</span> <span class="n">_VertexAICommon</span><span class="p">,</span> <span class="n">GoogleModelFamily</span>
<span class="kn">from</span> <span class="nn">langchain_google_vertexai._image_utils</span> <span class="kn">import</span> <span class="n">ImageBytesLoader</span>
<span class="kn">from</span> <span class="nn">langchain_google_vertexai._utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">create_retry_decorator</span><span class="p">,</span>
    <span class="n">get_generation_info</span><span class="p">,</span>
    <span class="n">_format_model_name</span><span class="p">,</span>
    <span class="n">is_gemini_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_google_vertexai.functions_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_format_tool_config</span><span class="p">,</span>
    <span class="n">_ToolConfigDict</span><span class="p">,</span>
    <span class="n">_tool_choice_to_tool_config</span><span class="p">,</span>
    <span class="n">_ToolChoiceType</span><span class="p">,</span>
    <span class="n">_ToolsType</span><span class="p">,</span>
    <span class="n">_format_to_gapic_tool</span><span class="p">,</span>
    <span class="n">_ToolType</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="n">_allowed_params</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"temperature"</span><span class="p">,</span>
    <span class="s2">"top_k"</span><span class="p">,</span>
    <span class="s2">"top_p"</span><span class="p">,</span>
    <span class="s2">"response_mime_type"</span><span class="p">,</span>
    <span class="s2">"response_schema"</span><span class="p">,</span>
    <span class="s2">"temperature"</span><span class="p">,</span>
    <span class="s2">"max_output_tokens"</span><span class="p">,</span>
    <span class="s2">"presence_penalty"</span><span class="p">,</span>
    <span class="s2">"frequency_penalty"</span><span class="p">,</span>
    <span class="s2">"candidate_count"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">_allowed_params_prediction_service</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"request"</span><span class="p">,</span> <span class="s2">"timeout"</span><span class="p">,</span> <span class="s2">"metadata"</span><span class="p">]</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">_ChatHistory</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Represents a context and a history of messages."""</span>

    <span class="n">history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">_GeminiGenerateContentKwargs</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span>
    <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">VertexTool</span><span class="p">]]</span>
    <span class="n">tool_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ToolConfig</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_parse_chat_history</span><span class="p">(</span><span class="n">history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">_ChatHistory</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Parse a sequence of messages into history.</span>

<span class="sd">    Args:</span>
<span class="sd">        history: The list of messages to re-create the history of the chat.</span>
<span class="sd">    Returns:</span>
<span class="sd">        A parsed chat history.</span>
<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If a sequence of message has a SystemMessage not at the</span>
<span class="sd">        first place.</span>
<span class="sd">    """</span>

    <span class="n">vertex_messages</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="p">[],</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">message</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">):</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">content</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">vertex_message</span> <span class="o">=</span> <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="s2">"bot"</span><span class="p">)</span>
            <span class="n">vertex_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vertex_message</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
            <span class="n">vertex_message</span> <span class="o">=</span> <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="s2">"user"</span><span class="p">)</span>
            <span class="n">vertex_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vertex_message</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Unexpected message with type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">message</span><span class="p">)</span><span class="si">}</span><span class="s2"> at the position </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">."</span>
            <span class="p">)</span>
    <span class="n">chat_history</span> <span class="o">=</span> <span class="n">_ChatHistory</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">history</span><span class="o">=</span><span class="n">vertex_messages</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">chat_history</span>


<span class="k">def</span> <span class="nf">_parse_chat_history_gemini</span><span class="p">(</span>
    <span class="n">history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
    <span class="n">project</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">convert_system_message_to_human</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Content</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">Content</span><span class="p">]]:</span>
    <span class="k">def</span> <span class="nf">_convert_to_prompt</span><span class="p">(</span><span class="n">part</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Part</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">part</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">Part</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">part</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">part</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Message's content is expected to be a dict, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">part</span><span class="p">)</span><span class="si">}</span><span class="s2">!"</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">part</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"text"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Part</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">part</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">part</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"tool_use"</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">part</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"text"</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">Part</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">part</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">part</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"image_url"</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">part</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">][</span><span class="s2">"url"</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">ImageBytesLoader</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">project</span><span class="p">)</span><span class="o">.</span><span class="n">load_gapic_part</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

        <span class="c1"># Handle media type like LangChain.js</span>
        <span class="c1"># https://github.com/langchain-ai/langchainjs/blob/e536593e2585f1dd7b0afc187de4d07cb40689ba/libs/langchain-google-common/src/utils/gemini.ts#L93-L106</span>
        <span class="k">if</span> <span class="n">part</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"media"</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">"mime_type"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">part</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Missing mime_type in media part: </span><span class="si">{</span><span class="n">part</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="n">mime_type</span> <span class="o">=</span> <span class="n">part</span><span class="p">[</span><span class="s2">"mime_type"</span><span class="p">]</span>
            <span class="n">proto_part</span> <span class="o">=</span> <span class="n">Part</span><span class="p">()</span>

            <span class="k">if</span> <span class="s2">"data"</span> <span class="ow">in</span> <span class="n">part</span><span class="p">:</span>
                <span class="n">proto_part</span><span class="o">.</span><span class="n">inline_data</span> <span class="o">=</span> <span class="n">Blob</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">part</span><span class="p">[</span><span class="s2">"data"</span><span class="p">],</span> <span class="n">mime_type</span><span class="o">=</span><span class="n">mime_type</span><span class="p">)</span>
            <span class="k">elif</span> <span class="s2">"file_uri"</span> <span class="ow">in</span> <span class="n">part</span><span class="p">:</span>
                <span class="n">proto_part</span><span class="o">.</span><span class="n">file_data</span> <span class="o">=</span> <span class="n">FileData</span><span class="p">(</span>
                    <span class="n">file_uri</span><span class="o">=</span><span class="n">part</span><span class="p">[</span><span class="s2">"file_uri"</span><span class="p">],</span> <span class="n">mime_type</span><span class="o">=</span><span class="n">mime_type</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Media part must have either data or file_uri: </span><span class="si">{</span><span class="n">part</span><span class="si">}</span><span class="s2">"</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="s2">"video_metadata"</span> <span class="ow">in</span> <span class="n">part</span><span class="p">:</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="n">VideoMetadata</span><span class="p">(</span><span class="n">part</span><span class="p">[</span><span class="s2">"video_metadata"</span><span class="p">])</span>
                <span class="n">proto_part</span><span class="o">.</span><span class="n">video_metadata</span> <span class="o">=</span> <span class="n">metadata</span>
            <span class="k">return</span> <span class="n">proto_part</span>

        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Only text, image_url, and media types are supported!"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_convert_to_parts</span><span class="p">(</span><span class="n">message</span><span class="p">:</span> <span class="n">BaseMessage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Part</span><span class="p">]:</span>
        <span class="n">raw_content</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span>

        <span class="c1"># If a user sends a multimodal request with agents, then the full input</span>
        <span class="c1"># will be sent as a string due to the ChatPromptTemplate formatting.</span>
        <span class="c1"># Because of this, we need to first try to convert the string to its</span>
        <span class="c1"># native type (such as list or dict) so that results can be properly</span>
        <span class="c1"># appended to the prompt, otherwise they will all be parsed as Text</span>
        <span class="c1"># rather than `inline_data`.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">raw_content</span> <span class="o">=</span> <span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">(</span><span class="n">raw_content</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">SyntaxError</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="c1"># A linting error is thrown here because it does not think this line is</span>
        <span class="c1"># reachable due to typing, but mypy is wrong so we ignore the lint</span>
        <span class="c1"># error.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_content</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="n">raw_content</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">raw_content</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">raw_content</span> <span class="o">=</span> <span class="p">[</span><span class="n">raw_content</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">raw_part</span> <span class="ow">in</span> <span class="n">raw_content</span><span class="p">:</span>
            <span class="n">part</span> <span class="o">=</span> <span class="n">_convert_to_prompt</span><span class="p">(</span><span class="n">raw_part</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">part</span><span class="p">:</span>
                <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">part</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="n">vertex_messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Content</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">system_parts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Part</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">system_instruction</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># the last AI Message before a sequence of tool calls</span>
    <span class="n">prev_ai_message</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AIMessage</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">message</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">):</span>
            <span class="n">prev_ai_message</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"SystemMessage should be the first in the history."</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">system_instruction</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">"Detected more than one SystemMessage in the list of messages."</span>
                    <span class="s2">"Gemini APIs support the insertion of only one SystemMessage."</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">convert_system_message_to_human</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">"gemini models released from April 2024 support"</span>
                    <span class="s2">"SystemMessages natively. For best performances,"</span>
                    <span class="s2">"when working with these models,"</span>
                    <span class="s2">"set convert_system_message_to_human to False"</span>
                <span class="p">)</span>
                <span class="n">system_parts</span> <span class="o">=</span> <span class="n">_convert_to_parts</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="n">system_instruction</span> <span class="o">=</span> <span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">"user"</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">_convert_to_parts</span><span class="p">(</span><span class="n">message</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
            <span class="n">prev_ai_message</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">role</span> <span class="o">=</span> <span class="s2">"user"</span>
            <span class="n">parts</span> <span class="o">=</span> <span class="n">_convert_to_parts</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">system_parts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">"System message should be immediately followed by HumanMessage"</span>
                    <span class="p">)</span>
                <span class="n">parts</span> <span class="o">=</span> <span class="n">system_parts</span> <span class="o">+</span> <span class="n">parts</span>
                <span class="n">system_parts</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">vertex_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">parts</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">prev_ai_message</span> <span class="o">=</span> <span class="n">message</span>
            <span class="n">role</span> <span class="o">=</span> <span class="s2">"model"</span>

            <span class="n">parts</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">:</span>
                <span class="n">parts</span> <span class="o">=</span> <span class="n">_convert_to_parts</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">tc</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>
                <span class="n">function_call</span> <span class="o">=</span> <span class="n">FunctionCall</span><span class="p">({</span><span class="s2">"name"</span><span class="p">:</span> <span class="n">tc</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span> <span class="s2">"args"</span><span class="p">:</span> <span class="n">tc</span><span class="p">[</span><span class="s2">"args"</span><span class="p">]})</span>
                <span class="n">parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Part</span><span class="p">(</span><span class="n">function_call</span><span class="o">=</span><span class="n">function_call</span><span class="p">))</span>

            <span class="n">prev_content</span> <span class="o">=</span> <span class="n">vertex_messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">prev_content_is_model</span> <span class="o">=</span> <span class="n">prev_content</span> <span class="ow">and</span> <span class="n">prev_content</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="s2">"model"</span>
            <span class="k">if</span> <span class="n">prev_content_is_model</span><span class="p">:</span>
                <span class="n">prev_parts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">prev_content</span><span class="o">.</span><span class="n">parts</span><span class="p">)</span>
                <span class="n">prev_parts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
                <span class="n">vertex_messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">prev_parts</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">vertex_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">parts</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">FunctionMessage</span><span class="p">):</span>
            <span class="n">prev_ai_message</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">role</span> <span class="o">=</span> <span class="s2">"function"</span>

            <span class="n">part</span> <span class="o">=</span> <span class="n">Part</span><span class="p">(</span>
                <span class="n">function_response</span><span class="o">=</span><span class="n">FunctionResponse</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="n">message</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">response</span><span class="o">=</span><span class="p">{</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">}</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="n">part</span><span class="p">]</span>

            <span class="n">prev_content</span> <span class="o">=</span> <span class="n">vertex_messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">prev_content_is_function</span> <span class="o">=</span> <span class="n">prev_content</span> <span class="ow">and</span> <span class="n">prev_content</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="s2">"function"</span>

            <span class="k">if</span> <span class="n">prev_content_is_function</span><span class="p">:</span>
                <span class="n">prev_parts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">prev_content</span><span class="o">.</span><span class="n">parts</span><span class="p">)</span>
                <span class="n">prev_parts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
                <span class="c1"># replacing last message</span>
                <span class="n">vertex_messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">prev_parts</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">vertex_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">parts</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">ToolMessage</span><span class="p">):</span>
            <span class="n">role</span> <span class="o">=</span> <span class="s2">"function"</span>

            <span class="c1"># message.name can be null for ToolMessage</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">name</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">prev_ai_message</span><span class="p">:</span>
                    <span class="n">tool_call_id</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_call_id</span>
                    <span class="n">tool_call</span><span class="p">:</span> <span class="n">ToolCall</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="n">t</span>
                            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">prev_ai_message</span><span class="o">.</span><span class="n">tool_calls</span>
                            <span class="k">if</span> <span class="n">t</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]</span> <span class="o">==</span> <span class="n">tool_call_id</span>
                        <span class="p">),</span>
                        <span class="kc">None</span><span class="p">,</span>
                    <span class="p">)</span>

                    <span class="k">if</span> <span class="n">tool_call</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="p">(</span>
                                <span class="s2">"Message name is empty and can't find"</span>
                                <span class="o">+</span> <span class="sa">f</span><span class="s2">"corresponding tool call for id: '$</span><span class="si">{</span><span class="n">tool_call_id</span><span class="si">}</span><span class="s2">'"</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                    <span class="n">name</span> <span class="o">=</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span>

            <span class="k">def</span> <span class="nf">_parse_content</span><span class="p">(</span><span class="n">raw_content</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_content</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">raw_content</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">raw_content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">content</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">raw_content</span><span class="p">)</span>
                        <span class="c1"># json.loads("2") returns 2 since it's a valid json</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                            <span class="k">return</span> <span class="n">content</span>
                    <span class="k">except</span> <span class="n">json</span><span class="o">.</span><span class="n">JSONDecodeError</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">raw_content</span><span class="p">}</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">parsed_content</span> <span class="o">=</span> <span class="p">[</span><span class="n">_parse_content</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parsed_content</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">merged_content</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="k">for</span> <span class="n">content_piece</span> <span class="ow">in</span> <span class="n">parsed_content</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">content_piece</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">merged_content</span><span class="p">:</span>
                                <span class="n">merged_content</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                            <span class="n">merged_content</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">"Expected content to be a str, got a list with &gt; 1 element."</span>
                        <span class="s2">"Merging values together"</span>
                    <span class="p">)</span>
                    <span class="n">content</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="s2">""</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">merged_content</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">content</span> <span class="o">=</span> <span class="n">parsed_content</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">_parse_content</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

            <span class="n">part</span> <span class="o">=</span> <span class="n">Part</span><span class="p">(</span>
                <span class="n">function_response</span><span class="o">=</span><span class="n">FunctionResponse</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">response</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">parts</span> <span class="o">=</span> <span class="p">[</span><span class="n">part</span><span class="p">]</span>

            <span class="n">prev_content</span> <span class="o">=</span> <span class="n">vertex_messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">prev_content_is_function</span> <span class="o">=</span> <span class="n">prev_content</span> <span class="ow">and</span> <span class="n">prev_content</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="s2">"function"</span>

            <span class="k">if</span> <span class="n">prev_content_is_function</span><span class="p">:</span>
                <span class="n">prev_parts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">prev_content</span><span class="o">.</span><span class="n">parts</span><span class="p">)</span>
                <span class="n">prev_parts</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span>
                <span class="c1"># replacing last message</span>
                <span class="n">vertex_messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">prev_parts</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">vertex_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Content</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="n">parts</span><span class="o">=</span><span class="n">parts</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Unexpected message with type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">message</span><span class="p">)</span><span class="si">}</span><span class="s2"> at the position </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">."</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">system_instruction</span><span class="p">,</span> <span class="n">vertex_messages</span>


<span class="k">def</span> <span class="nf">_parse_examples</span><span class="p">(</span><span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">"Expect examples to have an even amount of messages, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span><span class="si">}</span><span class="s2">."</span>
        <span class="p">)</span>
    <span class="n">example_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">example</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Expected the first message in a part to be from human, got "</span>
                    <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">example</span><span class="p">)</span><span class="si">}</span><span class="s2"> for the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th message."</span>
                <span class="p">)</span>
            <span class="n">input_text</span> <span class="o">=</span> <span class="n">example</span><span class="o">.</span><span class="n">content</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Expected the second message in a part to be from AI, got "</span>
                    <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">example</span><span class="p">)</span><span class="si">}</span><span class="s2"> for the </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">th message."</span>
                <span class="p">)</span>
            <span class="n">pair</span> <span class="o">=</span> <span class="n">InputOutputTextPair</span><span class="p">(</span>
                <span class="n">input_text</span><span class="o">=</span><span class="n">input_text</span><span class="p">,</span> <span class="n">output_text</span><span class="o">=</span><span class="n">example</span><span class="o">.</span><span class="n">content</span>
            <span class="p">)</span>
            <span class="n">example_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">example_pairs</span>


<span class="k">def</span> <span class="nf">_get_question</span><span class="p">(</span><span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">HumanMessage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Get the human message at the end of a list of input messages to a chat model."""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">messages</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"You should provide at least one message to start the chat!"</span><span class="p">)</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">"Last message in the list should be from human, got </span><span class="si">{</span><span class="n">question</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2">."</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">question</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">_parse_response_candidate</span><span class="p">(</span>
    <span class="n">response_candidate</span><span class="p">:</span> <span class="s2">"Candidate"</span><span class="p">,</span> <span class="n">streaming</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AIMessage</span><span class="p">:</span>
    <span class="o">...</span>


<span class="nd">@overload</span>
<span class="k">def</span> <span class="nf">_parse_response_candidate</span><span class="p">(</span>
    <span class="n">response_candidate</span><span class="p">:</span> <span class="s2">"Candidate"</span><span class="p">,</span> <span class="n">streaming</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AIMessageChunk</span><span class="p">:</span>
    <span class="o">...</span>


<span class="k">def</span> <span class="nf">_parse_response_candidate</span><span class="p">(</span>
    <span class="n">response_candidate</span><span class="p">:</span> <span class="s2">"Candidate"</span><span class="p">,</span> <span class="n">streaming</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AIMessage</span><span class="p">:</span>
    <span class="n">content</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">additional_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">invalid_tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tool_call_chunks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">response_candidate</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">parts</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">text</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">part</span><span class="o">.</span><span class="n">text</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">text</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">content</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">text</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">content</span> <span class="o">=</span> <span class="p">[</span><span class="n">content</span><span class="p">,</span> <span class="n">text</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">"Unexpected content type"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">part</span><span class="o">.</span><span class="n">function_call</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">"function_call"</span> <span class="ow">in</span> <span class="n">additional_kwargs</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="s2">"This model can reply with multiple "</span>
                        <span class="s2">"function calls in one response. "</span>
                        <span class="s2">"Please don't rely on `additional_kwargs.function_call` "</span>
                        <span class="s2">"as only the last one will be saved."</span>
                        <span class="s2">"Use `tool_calls` instead."</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">function_call</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"name"</span><span class="p">:</span> <span class="n">part</span><span class="o">.</span><span class="n">function_call</span><span class="o">.</span><span class="n">name</span><span class="p">}</span>
            <span class="c1"># dump to match other function calling llm for now</span>
            <span class="n">function_call_args_dict</span> <span class="o">=</span> <span class="n">proto</span><span class="o">.</span><span class="n">Message</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">function_call</span><span class="p">)[</span><span class="s2">"args"</span><span class="p">]</span>
            <span class="n">function_call</span><span class="p">[</span><span class="s2">"arguments"</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span>
                <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">function_call_args_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">function_call_args_dict</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span> <span class="o">=</span> <span class="n">function_call</span>

            <span class="k">if</span> <span class="n">streaming</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">function_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"index"</span><span class="p">)</span>
                <span class="n">tool_call_chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">tool_call_chunk</span><span class="p">(</span>
                        <span class="n">name</span><span class="o">=</span><span class="n">function_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">),</span>
                        <span class="n">args</span><span class="o">=</span><span class="n">function_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"arguments"</span><span class="p">),</span>
                        <span class="nb">id</span><span class="o">=</span><span class="n">function_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())),</span>
                        <span class="n">index</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="k">if</span> <span class="n">index</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">tool_calls_dicts</span> <span class="o">=</span> <span class="n">parse_tool_calls</span><span class="p">(</span>
                        <span class="p">[{</span><span class="s2">"function"</span><span class="p">:</span> <span class="n">function_call</span><span class="p">}],</span>
                        <span class="n">return_id</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">tool_calls</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">create_tool_call</span><span class="p">(</span>
                                <span class="n">name</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span>
                                <span class="n">args</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"args"</span><span class="p">],</span>
                                <span class="nb">id</span><span class="o">=</span><span class="n">tool_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())),</span>
                            <span class="p">)</span>
                            <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">tool_calls_dicts</span>
                        <span class="p">]</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">invalid_tool_calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">invalid_tool_call</span><span class="p">(</span>
                            <span class="n">name</span><span class="o">=</span><span class="n">function_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">),</span>
                            <span class="n">args</span><span class="o">=</span><span class="n">function_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"arguments"</span><span class="p">),</span>
                            <span class="nb">id</span><span class="o">=</span><span class="n">function_call</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())),</span>
                            <span class="n">error</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
    <span class="k">if</span> <span class="n">content</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">content</span> <span class="o">=</span> <span class="s2">""</span>

    <span class="k">if</span> <span class="n">streaming</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">AIMessageChunk</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]],</span> <span class="n">content</span><span class="p">),</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="n">tool_call_chunks</span><span class="o">=</span><span class="n">tool_call_chunks</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">AIMessage</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]],</span> <span class="n">content</span><span class="p">),</span>
        <span class="n">tool_calls</span><span class="o">=</span><span class="n">tool_calls</span><span class="p">,</span>
        <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
        <span class="n">invalid_tool_calls</span><span class="o">=</span><span class="n">invalid_tool_calls</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_completion_with_retry</span><span class="p">(</span>
    <span class="n">generation_method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Use tenacity to retry the completion call."""</span>
    <span class="n">retry_decorator</span> <span class="o">=</span> <span class="n">create_retry_decorator</span><span class="p">(</span>
        <span class="n">max_retries</span><span class="o">=</span><span class="n">max_retries</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span>
    <span class="p">)</span>

    <span class="nd">@retry_decorator</span>
    <span class="k">def</span> <span class="nf">_completion_with_retry_inner</span><span class="p">(</span><span class="n">generation_method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">generation_method</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">_allowed_params_prediction_service</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"is_gemini"</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_completion_with_retry_inner</span><span class="p">(</span>
        <span class="n">generation_method</span><span class="p">,</span>
        <span class="o">**</span><span class="n">params</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">_acompletion_with_retry</span><span class="p">(</span>
    <span class="n">generation_method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Use tenacity to retry the completion call."""</span>
    <span class="n">retry_decorator</span> <span class="o">=</span> <span class="n">create_retry_decorator</span><span class="p">(</span>
        <span class="n">max_retries</span><span class="o">=</span><span class="n">max_retries</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span>
    <span class="p">)</span>

    <span class="nd">@retry_decorator</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">_completion_with_retry_inner</span><span class="p">(</span>
        <span class="n">generation_method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="k">await</span> <span class="n">generation_method</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">_allowed_params_prediction_service</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"is_gemini"</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="k">await</span> <span class="n">_completion_with_retry_inner</span><span class="p">(</span>
        <span class="n">generation_method</span><span class="p">,</span>
        <span class="o">**</span><span class="n">params</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="ChatVertexAI"><a class="viewcode-back" href="../../google_vertexai/chat_models/langchain_google_vertexai.chat_models.ChatVertexAI.html#langchain_google_vertexai.chat_models.ChatVertexAI">[docs]</a><span class="k">class</span> <span class="nc">ChatVertexAI</span><span class="p">(</span><span class="n">_VertexAICommon</span><span class="p">,</span> <span class="n">BaseChatModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Google Cloud Vertex AI chat model integration.</span>

<span class="sd">    Setup:</span>
<span class="sd">        You must have the langchain-google-vertexai Python package installed</span>
<span class="sd">        .. code-block:: bash</span>

<span class="sd">            pip install -U langchain-google-vertexai</span>

<span class="sd">        And either:</span>
<span class="sd">            - Have credentials configured for your environment (gcloud, workload identity, etc...)</span>
<span class="sd">            - Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variable</span>

<span class="sd">        This codebase uses the google.auth library which first looks for the application</span>
<span class="sd">        credentials variable mentioned above, and then looks for system-level auth.</span>

<span class="sd">        For more information, see:</span>
<span class="sd">        https://cloud.google.com/docs/authentication/application-default-credentials#GAC</span>
<span class="sd">        and https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth.</span>

<span class="sd">    Key init args â€” completion params:</span>
<span class="sd">        model: str</span>
<span class="sd">            Name of ChatVertexAI model to use. e.g. "gemini-1.5-flash-001",</span>
<span class="sd">            "gemini-1.5-pro-001", etc.</span>
<span class="sd">        temperature: Optional[float]</span>
<span class="sd">            Sampling temperature.</span>
<span class="sd">        max_tokens: Optional[int]</span>
<span class="sd">            Max number of tokens to generate.</span>
<span class="sd">        stop: Optional[List[str]]</span>
<span class="sd">            Default stop sequences.</span>
<span class="sd">        safety_settings: Optional[Dict[vertexai.generative_models.HarmCategory, vertexai.generative_models.HarmBlockThreshold]]</span>
<span class="sd">            The default safety settings to use for all generations.</span>

<span class="sd">    Key init args â€” client params:</span>
<span class="sd">        max_retries: int</span>
<span class="sd">            Max number of retries.</span>
<span class="sd">        credentials: Optional[google.auth.credentials.Credentials]</span>
<span class="sd">            The default custom credentials to use when making API calls. If not</span>
<span class="sd">            provided, credentials will be ascertained from the environment.</span>
<span class="sd">        project: Optional[str]</span>
<span class="sd">            The default GCP project to use when making Vertex API calls.</span>
<span class="sd">        location: str = "us-central1"</span>
<span class="sd">            The default location to use when making API calls.</span>
<span class="sd">        request_parallelism: int = 5</span>
<span class="sd">            The amount of parallelism allowed for requests issued to VertexAI models.</span>
<span class="sd">            Default is 5.</span>
<span class="sd">        base_url: Optional[str]</span>
<span class="sd">            Base URL for API requests.</span>

<span class="sd">    See full list of supported init args and their descriptions in the params section.</span>

<span class="sd">    Instantiate:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_google_vertexai import ChatVertexAI</span>

<span class="sd">            llm = ChatVertexAI(</span>
<span class="sd">                model="gemini-1.5-flash-001",</span>
<span class="sd">                temperature=0,</span>
<span class="sd">                max_tokens=None,</span>
<span class="sd">                max_retries=6,</span>
<span class="sd">                stop=None,</span>
<span class="sd">                # other params...</span>
<span class="sd">            )</span>

<span class="sd">    Invoke:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            messages = [</span>
<span class="sd">                ("system", "You are a helpful translator. Translate the user sentence to French."),</span>
<span class="sd">                ("human", "I love programming."),</span>
<span class="sd">            ]</span>
<span class="sd">            llm.invoke(messages)</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessage(content="J'adore programmer. \n", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 17, 'candidates_token_count': 7, 'total_token_count': 24}}, id='run-925ce305-2268-44c4-875f-dde9128520ad-0')</span>

<span class="sd">    Stream:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            for chunk in llm.stream(messages):</span>
<span class="sd">                print(chunk)</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessageChunk(content='J', response_metadata={'is_blocked': False, 'safety_ratings': [], 'citation_metadata': None}, id='run-9df01d73-84d9-42db-9d6b-b1466a019e89')</span>
<span class="sd">            AIMessageChunk(content="'adore programmer. \n", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None}, id='run-9df01d73-84d9-42db-9d6b-b1466a019e89')</span>
<span class="sd">            AIMessageChunk(content='', response_metadata={'is_blocked': False, 'safety_ratings': [], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 17, 'candidates_token_count': 7, 'total_token_count': 24}}, id='run-9df01d73-84d9-42db-9d6b-b1466a019e89')</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            stream = llm.stream(messages)</span>
<span class="sd">            full = next(stream)</span>
<span class="sd">            for chunk in stream:</span>
<span class="sd">                full += chunk</span>
<span class="sd">            full</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessageChunk(content="J'adore programmer. \n", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 17, 'candidates_token_count': 7, 'total_token_count': 24}}, id='run-b7f7492c-4cb5-42d0-8fc3-dce9b293b0fb')</span>

<span class="sd">    Async:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            await llm.ainvoke(messages)</span>

<span class="sd">            # stream:</span>
<span class="sd">            # async for chunk in (await llm.astream(messages))</span>

<span class="sd">            # batch:</span>
<span class="sd">            # await llm.abatch([messages])</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessage(content="J'adore programmer. \n", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 17, 'candidates_token_count': 7, 'total_token_count': 24}}, id='run-925ce305-2268-44c4-875f-dde9128520ad-0')</span>

<span class="sd">    Tool calling:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_core.pydantic_v1 import BaseModel, Field</span>

<span class="sd">            class GetWeather(BaseModel):</span>
<span class="sd">                '''Get the current weather in a given location'''</span>

<span class="sd">                location: str = Field(..., description="The city and state, e.g. San Francisco, CA")</span>

<span class="sd">            class GetPopulation(BaseModel):</span>
<span class="sd">                '''Get the current population in a given location'''</span>

<span class="sd">                location: str = Field(..., description="The city and state, e.g. San Francisco, CA")</span>

<span class="sd">            llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])</span>
<span class="sd">            ai_msg = llm_with_tools.invoke("Which city is hotter today and which is bigger: LA or NY?")</span>
<span class="sd">            ai_msg.tool_calls</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            [{'name': 'GetWeather',</span>
<span class="sd">              'args': {'location': 'Los Angeles, CA'},</span>
<span class="sd">              'id': '2a2401fa-40db-470d-83ce-4e52de910d9e'},</span>
<span class="sd">             {'name': 'GetWeather',</span>
<span class="sd">              'args': {'location': 'New York City, NY'},</span>
<span class="sd">              'id': '96761deb-ab7f-4ef9-b4b4-6d44562fc46e'},</span>
<span class="sd">             {'name': 'GetPopulation',</span>
<span class="sd">              'args': {'location': 'Los Angeles, CA'},</span>
<span class="sd">              'id': '9147d532-abee-43a2-adb5-12f164300484'},</span>
<span class="sd">             {'name': 'GetPopulation',</span>
<span class="sd">              'args': {'location': 'New York City, NY'},</span>
<span class="sd">              'id': 'c43374ea-bde5-49ca-8487-5b83ebeea1e6'}]</span>

<span class="sd">        See ``ChatVertexAI.bind_tools()`` method for more.</span>

<span class="sd">    Structured output:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            from typing import Optional</span>

<span class="sd">            from langchain_core.pydantic_v1 import BaseModel, Field</span>

<span class="sd">            class Joke(BaseModel):</span>
<span class="sd">                '''Joke to tell user.'''</span>

<span class="sd">                setup: str = Field(description="The setup of the joke")</span>
<span class="sd">                punchline: str = Field(description="The punchline to the joke")</span>
<span class="sd">                rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")</span>

<span class="sd">            structured_llm = llm.with_structured_output(Joke)</span>
<span class="sd">            structured_llm.invoke("Tell me a joke about cats")</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            Joke(setup='What do you call a cat that loves to bowl?', punchline='An alley cat!', rating=None)</span>

<span class="sd">        See ``ChatVertexAI.with_structured_output()`` for more.</span>

<span class="sd">    Image input:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            import base64</span>
<span class="sd">            import httpx</span>
<span class="sd">            from langchain_core.messages import HumanMessage</span>

<span class="sd">            image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"</span>
<span class="sd">            image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")</span>
<span class="sd">            message = HumanMessage(</span>
<span class="sd">                content=[</span>
<span class="sd">                    {"type": "text", "text": "describe the weather in this image"},</span>
<span class="sd">                    {</span>
<span class="sd">                        "type": "image_url",</span>
<span class="sd">                        "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},</span>
<span class="sd">                    },</span>
<span class="sd">                ],</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg = llm.invoke([message])</span>
<span class="sd">            ai_msg.content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            'The weather in this image appears to be sunny and pleasant. The sky is a bright blue with scattered white clouds, suggesting a clear and mild day. The lush green grass indicates recent rainfall or sufficient moisture. The absence of strong shadows suggests that the sun is high in the sky, possibly late afternoon. Overall, the image conveys a sense of tranquility and warmth, characteristic of a beautiful summer day. \n'</span>

<span class="sd">        You can also point to GCS files which is faster / more efficient because bytes are transferred back and forth.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            llm.invoke(</span>
<span class="sd">                [</span>
<span class="sd">                    HumanMessage(</span>
<span class="sd">                        [</span>
<span class="sd">                            "What's in the image?",</span>
<span class="sd">                            {</span>
<span class="sd">                                "type": "media",</span>
<span class="sd">                                "file_uri": "gs://cloud-samples-data/generative-ai/image/scones.jpg",</span>
<span class="sd">                                "mime_type": "image/jpeg",</span>
<span class="sd">                            },</span>
<span class="sd">                        ]</span>
<span class="sd">                    )</span>
<span class="sd">                ]</span>
<span class="sd">            ).content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            'The image is of five blueberry scones arranged on a piece of baking paper. \n\nHere is a list of what is in the picture:\n* **Five blueberry scones:** They are scattered across the parchment paper, dusted with powdered sugar.  \n* **Two cups of coffee:**  Two white cups with saucers. One appears full, the other partially drunk.\n* **A bowl of blueberries:** A brown bowl is filled with fresh blueberries, placed near the scones.\n* **A spoon:**  A silver spoon with the words "Let\'s Jam" rests on the paper.\n* **Pink peonies:** Several pink peonies lie beside the scones, adding a touch of color.\n* **Baking paper:** The scones, cups, bowl, and spoon are arranged on a piece of white baking paper, splattered with purple.  The paper is crinkled and sits on a dark surface. \n\nThe image has a rustic and delicious feel, suggesting a cozy and enjoyable breakfast or brunch setting. \n'</span>

<span class="sd">    Video input:</span>
<span class="sd">        **NOTE**: Currently only supported for ``gemini-...-vision`` models.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            llm = ChatVertexAI(model="gemini-1.0-pro-vision")</span>

<span class="sd">            llm.invoke(</span>
<span class="sd">                [</span>
<span class="sd">                    HumanMessage(</span>
<span class="sd">                        [</span>
<span class="sd">                            "What's in the video?",</span>
<span class="sd">                            {</span>
<span class="sd">                                "type": "media",</span>
<span class="sd">                                "file_uri": "gs://cloud-samples-data/video/animals.mp4",</span>
<span class="sd">                                "mime_type": "video/mp4",</span>
<span class="sd">                            },</span>
<span class="sd">                        ]</span>
<span class="sd">                    )</span>
<span class="sd">                ]</span>
<span class="sd">            ).content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">             'The video is about a new feature in Google Photos called "Zoomable Selfies". The feature allows users to take selfies with animals at the zoo. The video shows several examples of people taking selfies with animals, including a tiger, an elephant, and a sea otter. The video also shows how the feature works. Users simply need to open the Google Photos app and select the "Zoomable Selfies" option. Then, they need to choose an animal from the list of available animals. The app will then guide the user through the process of taking the selfie.'</span>

<span class="sd">    Audio input:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_core.messages import HumanMessage</span>

<span class="sd">            llm = ChatVertexAI(model="gemini-1.5-flash-001")</span>

<span class="sd">            llm.invoke(</span>
<span class="sd">                [</span>
<span class="sd">                    HumanMessage(</span>
<span class="sd">                        [</span>
<span class="sd">                            "What's this audio about?",</span>
<span class="sd">                            {</span>
<span class="sd">                                "type": "media",</span>
<span class="sd">                                "file_uri": "gs://cloud-samples-data/generative-ai/audio/pixel.mp3",</span>
<span class="sd">                                "mime_type": "audio/mpeg",</span>
<span class="sd">                            },</span>
<span class="sd">                        ]</span>
<span class="sd">                    )</span>
<span class="sd">                ]</span>
<span class="sd">            ).content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            "This audio is an interview with two product managers from Google who work on Pixel feature drops. They discuss how feature drops are important for showcasing how Google devices are constantly improving and getting better. They also discuss some of the highlights of the January feature drop and the new features coming in the March drop for Pixel phones and Pixel watches. The interview concludes with discussion of how user feedback is extremely important to them in deciding which features to include in the feature drops. "</span>

<span class="sd">    Token usage:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm.invoke(messages)</span>
<span class="sd">            ai_msg.usage_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {'input_tokens': 17, 'output_tokens': 7, 'total_tokens': 24}</span>

<span class="sd">    Response metadata</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm.invoke(messages)</span>
<span class="sd">            ai_msg.response_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {'is_blocked': False,</span>
<span class="sd">             'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False},</span>
<span class="sd">              {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False},</span>
<span class="sd">              {'category': 'HARM_CATEGORY_HARASSMENT',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False},</span>
<span class="sd">              {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False}],</span>
<span class="sd">             'usage_metadata': {'prompt_token_count': 17,</span>
<span class="sd">              'candidates_token_count': 7,</span>
<span class="sd">              'total_token_count': 24}}</span>

<span class="sd">    Safety settings</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_google_vertexai import HarmBlockThreshold, HarmCategory</span>

<span class="sd">            llm = ChatVertexAI(</span>
<span class="sd">                model="gemini-1.5-pro",</span>
<span class="sd">                safety_settings={</span>
<span class="sd">                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,</span>
<span class="sd">                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,</span>
<span class="sd">                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,</span>
<span class="sd">                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,</span>
<span class="sd">                },</span>
<span class="sd">            )</span>

<span class="sd">            llm.invoke(messages).response_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {'is_blocked': False,</span>
<span class="sd">             'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False},</span>
<span class="sd">              {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False},</span>
<span class="sd">              {'category': 'HARM_CATEGORY_HARASSMENT',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False},</span>
<span class="sd">              {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',</span>
<span class="sd">               'probability_label': 'NEGLIGIBLE',</span>
<span class="sd">               'blocked': False}],</span>
<span class="sd">             'usage_metadata': {'prompt_token_count': 17,</span>
<span class="sd">              'candidates_token_count': 7,</span>
<span class="sd">              'total_token_count': 24}}</span>

<span class="sd">    """</span>  <span class="c1"># noqa: E501</span>

    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">"chat-bison-default"</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"model"</span><span class="p">)</span>
    <span class="s2">"Underlying model name."</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">convert_system_message_to_human</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""[Deprecated] Since new Gemini models support setting a System Message,</span>
<span class="sd">    setting this parameter to True is discouraged.</span>
<span class="sd">    """</span>
    <span class="n">response_mime_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Optional. Output response mimetype of the generated candidate text. Only</span>
<span class="sd">        supported in Gemini 1.5 and later models. Supported mimetype:</span>
<span class="sd">            * "text/plain": (default) Text output.</span>
<span class="sd">            * "application/json": JSON response in the candidates.</span>
<span class="sd">       The model also needs to be prompted to output the appropriate response</span>
<span class="sd">       type, otherwise the behavior is undefined. This is a preview feature.</span>
<span class="sd">    """</span>

    <span class="n">response_schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">""" Optional. Enforce an schema to the output. Only works when `response_mime_type`</span>
<span class="sd">        is set to `application/json`.</span>
<span class="sd">        The format of the dictionary should follow Open API schema.</span>
<span class="sd">    """</span>

    <span class="n">cached_content</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">""" Optional. Use the model in cache mode. Only supported in Gemini 1.5 and later </span>
<span class="sd">        models. Must be a string containing the cache name (A sequence of numbers)</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Needed for mypy typing to recognize model_name as a valid arg."""</span>
        <span class="k">if</span> <span class="n">model_name</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Configuration for this pydantic object."""</span>

        <span class="n">allow_population_by_field_name</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">arbitrary_types_allowed</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">is_lc_serializable</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_lc_namespace</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the namespace of the langchain object."""</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">"langchain"</span><span class="p">,</span> <span class="s2">"chat_models"</span><span class="p">,</span> <span class="s2">"vertexai"</span><span class="p">]</span>

    <span class="nd">@root_validator</span><span class="p">(</span><span class="n">pre</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">skip_on_failure</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">validate_environment</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Validate that the python package exists in environment."""</span>
        <span class="n">safety_settings</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"safety_settings"</span><span class="p">)</span>
        <span class="n">tuned_model_name</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tuned_model_name"</span><span class="p">)</span>
        <span class="n">values</span><span class="p">[</span><span class="s2">"model_family"</span><span class="p">]</span> <span class="o">=</span> <span class="n">GoogleModelFamily</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"chat-bison-default"</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">"Model_name will become a required arg for VertexAIEmbeddings "</span>
                <span class="s2">"starting from Sep-01-2024. Currently the default is set to "</span>
                <span class="s2">"chat-bison"</span>
            <span class="p">)</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"chat-bison"</span>

        <span class="k">if</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"full_model_name"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tuned_model_name"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"full_model_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_format_model_name</span><span class="p">(</span>
                <span class="n">values</span><span class="p">[</span><span class="s2">"tuned_model_name"</span><span class="p">],</span>
                <span class="n">location</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="s2">"location"</span><span class="p">],</span>
                <span class="n">project</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="s2">"project"</span><span class="p">],</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"full_model_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_format_model_name</span><span class="p">(</span>
                <span class="n">values</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">],</span>
                <span class="n">location</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="s2">"location"</span><span class="p">],</span>
                <span class="n">project</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="s2">"project"</span><span class="p">],</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">safety_settings</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_gemini_model</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="s2">"model_family"</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Safety settings are only supported for Gemini models"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">tuned_model_name</span><span class="p">:</span>
            <span class="n">generative_model_name</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"tuned_model_name"</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">generative_model_name</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_gemini_model</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="s2">"model_family"</span><span class="p">]):</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_init_vertexai</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"model_family"</span><span class="p">]</span> <span class="o">==</span> <span class="n">GoogleModelFamily</span><span class="o">.</span><span class="n">CODEY</span><span class="p">:</span>
                <span class="n">model_cls</span> <span class="o">=</span> <span class="n">CodeChatModel</span>
                <span class="n">model_cls_preview</span> <span class="o">=</span> <span class="n">PreviewCodeChatModel</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_cls</span> <span class="o">=</span> <span class="n">ChatModel</span>
                <span class="n">model_cls_preview</span> <span class="o">=</span> <span class="n">PreviewChatModel</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"client"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_cls</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">generative_model_name</span><span class="p">)</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"client_preview"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_cls_preview</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">generative_model_name</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">values</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_is_gemini_advanced</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_family</span> <span class="o">==</span> <span class="n">GoogleModelFamily</span><span class="o">.</span><span class="n">GEMINI_ADVANCED</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_default_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">updated_params</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_default_params</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_mime_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">updated_params</span><span class="p">[</span><span class="s2">"response_mime_type"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_mime_type</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_schema</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_mime_type</span> <span class="o">!=</span> <span class="s2">"application/json"</span><span class="p">:</span>
                <span class="n">error_message</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">"`response_schema` is only supported when "</span>
                    <span class="s2">"`response_mime_type` is set to `application/json`."</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_message</span><span class="p">)</span>

            <span class="n">gapic_response_schema</span> <span class="o">=</span> <span class="n">_convert_schema_dict_to_gapic</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">response_schema</span><span class="p">)</span>
            <span class="n">updated_params</span><span class="p">[</span><span class="s2">"response_schema"</span><span class="p">]</span> <span class="o">=</span> <span class="n">gapic_response_schema</span>

        <span class="k">return</span> <span class="n">updated_params</span>

    <span class="k">def</span> <span class="nf">_get_ls_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LangSmithParams</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Get standard params for tracing."""</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">ls_params</span> <span class="o">=</span> <span class="n">LangSmithParams</span><span class="p">(</span>
            <span class="n">ls_provider</span><span class="o">=</span><span class="s2">"google_vertexai"</span><span class="p">,</span>
            <span class="n">ls_model_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">ls_model_type</span><span class="o">=</span><span class="s2">"chat"</span><span class="p">,</span>
            <span class="n">ls_temperature</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">ls_max_tokens</span> <span class="o">:=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"max_output_tokens"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_output_tokens</span><span class="p">):</span>
            <span class="n">ls_params</span><span class="p">[</span><span class="s2">"ls_max_tokens"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ls_max_tokens</span>
        <span class="k">if</span> <span class="n">ls_stop</span> <span class="o">:=</span> <span class="n">stop</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stop"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">:</span>
            <span class="n">ls_params</span><span class="p">[</span><span class="s2">"ls_stop"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ls_stop</span>
        <span class="k">return</span> <span class="n">ls_params</span>

    <span class="k">def</span> <span class="nf">_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Generate next turn in the conversation.</span>

<span class="sd">        Args:</span>
<span class="sd">            messages: The history of the conversation as a list of messages. Code chat</span>
<span class="sd">                does not support context.</span>
<span class="sd">            stop: The list of stop words (optional).</span>
<span class="sd">            run_manager: The CallbackManager for LLM run, it's not used at the moment.</span>
<span class="sd">            stream: Whether to use the streaming endpoint.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The ChatResult that contains outputs generated by the model.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if the last message in the list is not from human.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">or</span> <span class="p">(</span><span class="n">stream</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">):</span>
            <span class="n">stream_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stream</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">generate_from_stream</span><span class="p">(</span><span class="n">stream_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_non_gemini</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_gemini</span><span class="p">(</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span>
            <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span>
            <span class="n">is_gemini</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_generation_config_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GenerationConfig</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Prepares GenerationConfig part of the request.</span>

<span class="sd">        https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1beta1#generationconfig</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="n">GenerationConfig</span><span class="p">(</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_prepare_params</span><span class="p">(</span>
                <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span>
                <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>
                <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">_allowed_params</span><span class="p">},</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_safety_settings_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">SafetySetting</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">"""Prepares SafetySetting part of the request.</span>

<span class="sd">        https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1beta1#safetysetting</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">safety_settings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">safety_settings</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_safety_settings_gemini</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">safety_settings</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">safety_settings</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">safety_settings</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">safety_settings</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">formatted_safety_settings</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">category</span><span class="p">,</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">safety_settings</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="n">category</span> <span class="o">=</span> <span class="n">HarmCategory</span><span class="p">[</span><span class="n">category</span><span class="p">]</span>  <span class="c1"># type: ignore[misc]</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="n">threshold</span> <span class="o">=</span> <span class="n">SafetySetting</span><span class="o">.</span><span class="n">HarmBlockThreshold</span><span class="p">[</span><span class="n">threshold</span><span class="p">]</span>  <span class="c1"># type: ignore[misc]</span>

                <span class="n">formatted_safety_settings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">SafetySetting</span><span class="p">(</span>
                        <span class="n">category</span><span class="o">=</span><span class="n">HarmCategory</span><span class="p">(</span><span class="n">category</span><span class="p">),</span>
                        <span class="n">threshold</span><span class="o">=</span><span class="n">SafetySetting</span><span class="o">.</span><span class="n">HarmBlockThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="p">),</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">formatted_safety_settings</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"safety_settings should be either"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_request_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ToolsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">functions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ToolsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tool_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">_ToolConfigDict</span><span class="p">,</span> <span class="n">ToolConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cached_content</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tool_choice</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ToolChoiceType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GenerateContentRequest</span><span class="p">:</span>
        <span class="n">system_instruction</span><span class="p">,</span> <span class="n">contents</span> <span class="o">=</span> <span class="n">_parse_chat_history_gemini</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="n">formatted_tools</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tools_gemini</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span> <span class="n">functions</span><span class="o">=</span><span class="n">functions</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tool_config</span><span class="p">:</span>
            <span class="n">tool_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tool_config_gemini</span><span class="p">(</span><span class="n">tool_config</span><span class="o">=</span><span class="n">tool_config</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">tool_choice</span><span class="p">:</span>
            <span class="n">all_names</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">f</span><span class="o">.</span><span class="n">name</span>
                <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="p">(</span><span class="n">formatted_tools</span> <span class="ow">or</span> <span class="p">[])</span>
                <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">tool</span><span class="o">.</span><span class="n">function_declarations</span>
            <span class="p">]</span>
            <span class="n">tool_config</span> <span class="o">=</span> <span class="n">_tool_choice_to_tool_config</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="n">all_names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="n">safety_settings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_safety_settings_gemini</span><span class="p">(</span><span class="n">safety_settings</span><span class="p">)</span>
        <span class="n">generation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generation_config_gemini</span><span class="p">(</span>
            <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cached_content</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">cached_content</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">selected_cached_content</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cached_content</span> <span class="ow">or</span> <span class="n">cached_content</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_request_from_cached_content</span><span class="p">(</span>
                <span class="n">cached_content</span><span class="o">=</span><span class="n">selected_cached_content</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">contents</span><span class="o">=</span><span class="n">contents</span><span class="p">,</span>
                <span class="n">system_instruction</span><span class="o">=</span><span class="n">system_instruction</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="n">formatted_tools</span><span class="p">,</span>
                <span class="n">tool_config</span><span class="o">=</span><span class="n">tool_config</span><span class="p">,</span>
                <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">full_model_name</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">GenerateContentRequest</span><span class="p">(</span>
            <span class="n">contents</span><span class="o">=</span><span class="n">contents</span><span class="p">,</span>
            <span class="n">system_instruction</span><span class="o">=</span><span class="n">system_instruction</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">formatted_tools</span><span class="p">,</span>
            <span class="n">tool_config</span><span class="o">=</span><span class="n">tool_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">full_model_name</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_request_from_cached_content</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cached_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">system_instruction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Content</span><span class="p">],</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">GapicTool</span><span class="p">]],</span>
        <span class="n">tool_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">_ToolConfigDict</span><span class="p">,</span> <span class="n">ToolConfig</span><span class="p">]],</span>
        <span class="n">contents</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Content</span><span class="p">],</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">SafetySetting</span><span class="p">]],</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GenerateContentRequest</span><span class="p">:</span>
        <span class="n">not_allowed_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="s2">"system_instructions"</span><span class="p">,</span> <span class="n">system_instruction</span><span class="p">),</span>
            <span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tools</span><span class="p">),</span>
            <span class="p">(</span><span class="s2">"tool_config"</span><span class="p">,</span> <span class="n">tool_config</span><span class="p">),</span>
        <span class="p">]</span>

        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">not_allowed_parameters</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">parameter</span><span class="p">:</span>
                <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Using cached content. Parameter `</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">` will be ignored. "</span>
                <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

        <span class="n">full_cache_name</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">"projects/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">project</span><span class="si">}</span><span class="s2">/locations/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">location</span><span class="si">}</span><span class="s2">/"</span>
            <span class="sa">f</span><span class="s2">"cachedContents/</span><span class="si">{</span><span class="n">cached_content</span><span class="si">}</span><span class="s2">"</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">GenerateContentRequest</span><span class="p">(</span>
            <span class="n">contents</span><span class="o">=</span><span class="n">contents</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">cached_content</span><span class="o">=</span><span class="n">full_cache_name</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_generate_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="n">request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request_gemini</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">_completion_with_retry</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prediction_client</span><span class="o">.</span><span class="n">generate_content</span><span class="p">,</span>
            <span class="n">max_retries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span><span class="p">,</span>
            <span class="n">request</span><span class="o">=</span><span class="n">request</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">default_metadata</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gemini_response_to_chat_result</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_agenerate_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">_acompletion_with_retry</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">async_prediction_client</span><span class="o">.</span><span class="n">generate_content</span><span class="p">,</span>
            <span class="n">max_retries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span><span class="p">,</span>
            <span class="n">request</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request_gemini</span><span class="p">(</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">),</span>
            <span class="n">is_gemini</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">default_metadata</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gemini_response_to_chat_result</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

<div class="viewcode-block" id="ChatVertexAI.get_num_tokens"><a class="viewcode-back" href="../../google_vertexai/chat_models/langchain_google_vertexai.chat_models.ChatVertexAI.html#langchain_google_vertexai.chat_models.ChatVertexAI.get_num_tokens">[docs]</a>    <span class="k">def</span> <span class="nf">get_num_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Get the number of tokens present in the text."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">:</span>
            <span class="c1"># https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1beta1#counttokensrequest</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">contents</span> <span class="o">=</span> <span class="n">_parse_chat_history_gemini</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">text</span><span class="p">)])</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prediction_client</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">"endpoint"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_model_name</span><span class="p">,</span>
                    <span class="s2">"model"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_model_name</span><span class="p">,</span>
                    <span class="s2">"contents"</span><span class="p">:</span> <span class="n">contents</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">total_tokens</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_num_tokens</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_tools_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ToolsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">functions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ToolsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">GapicTool</span><span class="p">]]:</span>
        <span class="k">if</span> <span class="n">tools</span> <span class="ow">and</span> <span class="n">functions</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">"Binding tools and functions together is not supported."</span><span class="p">,</span>
                <span class="s2">"Only tools will be used"</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">tools</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">_format_to_gapic_tool</span><span class="p">(</span><span class="n">tools</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">functions</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">_format_to_gapic_tool</span><span class="p">(</span><span class="n">functions</span><span class="p">)]</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_tool_config_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tool_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">_ToolConfigDict</span><span class="p">,</span> <span class="n">ToolConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GapicToolConfig</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">tool_config</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_config</span><span class="p">,</span> <span class="n">ToolConfig</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">_format_tool_config</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">_ToolConfigDict</span><span class="p">,</span> <span class="n">tool_config</span><span class="p">))</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_generate_non_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"safety_settings"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">question</span> <span class="o">=</span> <span class="n">_get_question</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="n">history</span> <span class="o">=</span> <span class="n">_parse_chat_history</span><span class="p">(</span><span class="n">messages</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"examples"</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">examples</span>
        <span class="n">msg_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">"candidate_count"</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">msg_params</span><span class="p">[</span><span class="s2">"candidate_count"</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"candidate_count"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">params</span><span class="p">[</span><span class="s2">"examples"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_parse_examples</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">telemetry</span><span class="o">.</span><span class="n">tool_context_manager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_user_agent</span><span class="p">):</span>
            <span class="n">chat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_chat</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">_completion_with_retry</span><span class="p">(</span>
                <span class="n">chat</span><span class="o">.</span><span class="n">send_message</span><span class="p">,</span>
                <span class="n">max_retries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span><span class="p">,</span>
                <span class="n">message</span><span class="o">=</span><span class="n">question</span><span class="o">.</span><span class="n">content</span><span class="p">,</span>
                <span class="o">**</span><span class="n">msg_params</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">raw_prediction_response</span><span class="o">.</span><span class="n">metadata</span>
        <span class="n">lc_usage</span> <span class="o">=</span> <span class="n">_get_usage_metadata_non_gemini</span><span class="p">(</span><span class="n">usage_metadata</span><span class="p">)</span>
        <span class="n">generations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">ChatGeneration</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">candidate</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">usage_metadata</span><span class="o">=</span><span class="n">lc_usage</span><span class="p">),</span>
                <span class="n">generation_info</span><span class="o">=</span><span class="n">get_generation_info</span><span class="p">(</span>
                    <span class="n">candidate</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">,</span>
                    <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage_metadata</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="n">ChatResult</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="n">generations</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_agenerate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Asynchronously generate next turn in the conversation.</span>

<span class="sd">        Args:</span>
<span class="sd">            messages: The history of the conversation as a list of messages. Code chat</span>
<span class="sd">                does not support context.</span>
<span class="sd">            stop: The list of stop words (optional).</span>
<span class="sd">            run_manager: The CallbackManager for LLM run, it's not used at the moment.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The ChatResult that contains outputs generated by the model.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if the last message in the list is not from human.</span>
<span class="sd">        """</span>
        <span class="n">should_stream</span> <span class="o">=</span> <span class="n">stream</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">or</span> <span class="p">(</span><span class="n">stream</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">should_stream</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">"ChatVertexAI does not currently support async streaming."</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agenerate_non_gemini</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">should_stream</span><span class="p">:</span>
            <span class="n">stream_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_astream</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="k">await</span> <span class="n">agenerate_from_stream</span><span class="p">(</span><span class="n">stream_iter</span><span class="p">)</span>

        <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agenerate_gemini</span><span class="p">(</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span>
            <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_agenerate_non_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"safety_settings"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">question</span> <span class="o">=</span> <span class="n">_get_question</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="n">history</span> <span class="o">=</span> <span class="n">_parse_chat_history</span><span class="p">(</span><span class="n">messages</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"examples"</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">examples</span>
        <span class="n">msg_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">"candidate_count"</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">msg_params</span><span class="p">[</span><span class="s2">"candidate_count"</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"candidate_count"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">params</span><span class="p">[</span><span class="s2">"examples"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_parse_examples</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">telemetry</span><span class="o">.</span><span class="n">tool_context_manager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_user_agent</span><span class="p">):</span>
            <span class="n">chat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_chat</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">_acompletion_with_retry</span><span class="p">(</span>
                <span class="n">chat</span><span class="o">.</span><span class="n">send_message_async</span><span class="p">,</span>
                <span class="n">message</span><span class="o">=</span><span class="n">question</span><span class="o">.</span><span class="n">content</span><span class="p">,</span>
                <span class="n">max_retries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span><span class="p">,</span>
                <span class="o">**</span><span class="n">msg_params</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">raw_prediction_response</span><span class="o">.</span><span class="n">metadata</span>
        <span class="n">lc_usage</span> <span class="o">=</span> <span class="n">_get_usage_metadata_non_gemini</span><span class="p">(</span><span class="n">usage_metadata</span><span class="p">)</span>
        <span class="n">generations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">ChatGeneration</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">candidate</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">usage_metadata</span><span class="o">=</span><span class="n">lc_usage</span><span class="p">),</span>
                <span class="n">generation_info</span><span class="o">=</span><span class="n">get_generation_info</span><span class="p">(</span>
                    <span class="n">candidate</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">,</span>
                    <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage_metadata</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="n">ChatResult</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="n">generations</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">:</span>
            <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stream_non_gemini</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stream_gemini</span><span class="p">(</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">_stream_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request_gemini</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">response_iter</span> <span class="o">=</span> <span class="n">_completion_with_retry</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prediction_client</span><span class="o">.</span><span class="n">stream_generate_content</span><span class="p">,</span>
            <span class="n">max_retries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span><span class="p">,</span>
            <span class="n">request</span><span class="o">=</span><span class="n">request</span><span class="p">,</span>
            <span class="n">is_gemini</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">default_metadata</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">total_lc_usage</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">response_chunk</span> <span class="ow">in</span> <span class="n">response_iter</span><span class="p">:</span>
            <span class="n">chunk</span><span class="p">,</span> <span class="n">total_lc_usage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gemini_chunk_to_generation_chunk</span><span class="p">(</span>
                <span class="n">response_chunk</span><span class="p">,</span> <span class="n">prev_total_usage</span><span class="o">=</span><span class="n">total_lc_usage</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">run_manager</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">chunk</span>

    <span class="k">def</span> <span class="nf">_stream_non_gemini</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">question</span> <span class="o">=</span> <span class="n">_get_question</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="n">history</span> <span class="o">=</span> <span class="n">_parse_chat_history</span><span class="p">(</span><span class="n">messages</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"examples"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">params</span><span class="p">[</span><span class="s2">"examples"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_parse_examples</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">telemetry</span><span class="o">.</span><span class="n">tool_context_manager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_user_agent</span><span class="p">):</span>
            <span class="n">chat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_chat</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
            <span class="n">responses</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">send_message_streaming</span><span class="p">(</span><span class="n">question</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                    <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
                <span class="k">yield</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
                    <span class="n">message</span><span class="o">=</span><span class="n">AIMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">),</span>
                    <span class="n">generation_info</span><span class="o">=</span><span class="n">get_generation_info</span><span class="p">(</span>
                        <span class="n">response</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">,</span>
                        <span class="n">usage_metadata</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">raw_prediction_response</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_astream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_model</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="n">request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request_gemini</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">response_iter</span> <span class="o">=</span> <span class="n">_acompletion_with_retry</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">async_prediction_client</span><span class="o">.</span><span class="n">stream_generate_content</span><span class="p">,</span>
            <span class="n">max_retries</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_retries</span><span class="p">,</span>
            <span class="n">request</span><span class="o">=</span><span class="n">request</span><span class="p">,</span>
            <span class="n">is_gemini</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">total_lc_usage</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">response_chunk</span> <span class="ow">in</span> <span class="k">await</span> <span class="n">response_iter</span><span class="p">:</span>
            <span class="n">chunk</span><span class="p">,</span> <span class="n">total_lc_usage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gemini_chunk_to_generation_chunk</span><span class="p">(</span>
                <span class="n">response_chunk</span><span class="p">,</span> <span class="n">prev_total_usage</span><span class="o">=</span><span class="n">total_lc_usage</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">run_manager</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">await</span> <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">chunk</span>

<div class="viewcode-block" id="ChatVertexAI.with_structured_output"><a class="viewcode-back" href="../../google_vertexai/chat_models/langchain_google_vertexai.chat_models.ChatVertexAI.html#langchain_google_vertexai.chat_models.ChatVertexAI.with_structured_output">[docs]</a>    <span class="k">def</span> <span class="nf">with_structured_output</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">BaseModel</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">include_raw</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">BaseModel</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">"""Model wrapper that returns outputs formatted to match the given schema.</span>

<span class="sd">        .. versionchanged:: 1.1.0</span>

<span class="sd">            Return type corrected in version 1.1.0. Previously if a dict schema</span>
<span class="sd">            was provided then the output had the form</span>
<span class="sd">            ``[{"args": {}, "name": "schema_name"}]`` where the output was a list with</span>
<span class="sd">            a single dict and the "args" of the one dict corresponded to the schema.</span>
<span class="sd">            As of `1.1.0` this has been fixed so that the schema (the value</span>
<span class="sd">            corresponding to the old "args" key) is returned directly.</span>

<span class="sd">        Args:</span>
<span class="sd">            schema: The output schema as a dict or a Pydantic class. If a Pydantic class</span>
<span class="sd">                then the model output will be an object of that class. If a dict then</span>
<span class="sd">                the model output will be a dict. With a Pydantic class the returned</span>
<span class="sd">                attributes will be validated, whereas with a dict they will not be. If</span>
<span class="sd">                `method` is "function_calling" and `schema` is a dict, then the dict</span>
<span class="sd">                must match the OpenAI function-calling spec.</span>
<span class="sd">            include_raw: If False then only the parsed structured output is returned. If</span>
<span class="sd">                an error occurs during model output parsing it will be raised. If True</span>
<span class="sd">                then both the raw model response (a BaseMessage) and the parsed model</span>
<span class="sd">                response will be returned. If an error occurs during output parsing it</span>
<span class="sd">                will be caught and returned as well. The final output is always a dict</span>
<span class="sd">                with keys "raw", "parsed", and "parsing_error".</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Runnable that takes any ChatModel input. If include_raw is True then a</span>
<span class="sd">            dict with keys â€” raw: BaseMessage, parsed: Optional[_DictOrPydantic],</span>
<span class="sd">            parsing_error: Optional[BaseException]. If include_raw is False then just</span>
<span class="sd">            _DictOrPydantic is returned, where _DictOrPydantic depends on the schema.</span>
<span class="sd">            If schema is a Pydantic class then _DictOrPydantic is the Pydantic class.</span>
<span class="sd">            If schema is a dict then _DictOrPydantic is a dict.</span>

<span class="sd">        Example: Pydantic schema, exclude raw:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                from langchain_core.pydantic_v1 import BaseModel</span>
<span class="sd">                from langchain_google_vertexai import ChatVertexAI</span>

<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>
<span class="sd">                    answer: str</span>
<span class="sd">                    justification: str</span>

<span class="sd">                llm = ChatVertexAI(model_name="gemini-pro", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(AnswerWithJustification)</span>

<span class="sd">                structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")</span>
<span class="sd">                # -&gt; AnswerWithJustification(</span>
<span class="sd">                #     answer='They weigh the same.', justification='A pound is a pound.'</span>
<span class="sd">                # )</span>

<span class="sd">        Example: Pydantic schema, include raw:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                from langchain_core.pydantic_v1 import BaseModel</span>
<span class="sd">                from langchain_google_vertexai import ChatVertexAI</span>

<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>
<span class="sd">                    answer: str</span>
<span class="sd">                    justification: str</span>

<span class="sd">                llm = ChatVertexAI(model_name="gemini-pro", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)</span>

<span class="sd">                structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{"answer":"They weigh the same.","justification":"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ."}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),</span>
<span class="sd">                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),</span>
<span class="sd">                #     'parsing_error': None</span>
<span class="sd">                # }</span>

<span class="sd">        Example: Dict schema, exclude raw:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                from langchain_core.pydantic_v1 import BaseModel</span>
<span class="sd">                from langchain_core.utils.function_calling import convert_to_openai_function</span>
<span class="sd">                from langchain_google_vertexai import ChatVertexAI</span>

<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>
<span class="sd">                    answer: str</span>
<span class="sd">                    justification: str</span>

<span class="sd">                dict_schema = convert_to_openai_function(AnswerWithJustification)</span>
<span class="sd">                llm = ChatVertexAI(model_name="gemini-pro", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(dict_schema)</span>

<span class="sd">                structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'answer': 'They weigh the same',</span>
<span class="sd">                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'</span>
<span class="sd">                # }</span>

<span class="sd">        """</span>  <span class="c1"># noqa: E501</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Received unsupported arguments </span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_basemodel_subclass</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
            <span class="n">parser</span><span class="p">:</span> <span class="n">OutputParserLike</span> <span class="o">=</span> <span class="n">PydanticToolsParser</span><span class="p">(</span>
                <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">schema</span><span class="p">],</span> <span class="n">first_tool_only</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">parser</span> <span class="o">=</span> <span class="n">JsonOutputToolsParser</span><span class="p">(</span><span class="n">first_tool_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">|</span> <span class="n">RunnableGenerator</span><span class="p">(</span>
                <span class="n">_yield_args</span>
            <span class="p">)</span>
        <span class="n">tool_choice</span> <span class="o">=</span> <span class="n">_get_tool_name</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_gemini_advanced</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">llm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">([</span><span class="n">schema</span><span class="p">],</span> <span class="n">tool_choice</span><span class="o">=</span><span class="n">tool_choice</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">include_raw</span><span class="p">:</span>
            <span class="n">parser_with_fallback</span> <span class="o">=</span> <span class="n">RunnablePassthrough</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
                <span class="n">parsed</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="s2">"raw"</span><span class="p">)</span> <span class="o">|</span> <span class="n">parser</span><span class="p">,</span> <span class="n">parsing_error</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span>
            <span class="p">)</span><span class="o">.</span><span class="n">with_fallbacks</span><span class="p">(</span>
                <span class="p">[</span><span class="n">RunnablePassthrough</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">parsed</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span><span class="p">)],</span>
                <span class="n">exception_key</span><span class="o">=</span><span class="s2">"parsing_error"</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">"raw"</span><span class="p">:</span> <span class="n">llm</span><span class="p">}</span> <span class="o">|</span> <span class="n">parser_with_fallback</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span></div>

<div class="viewcode-block" id="ChatVertexAI.bind_tools"><a class="viewcode-back" href="../../google_vertexai/chat_models/langchain_google_vertexai.chat_models.ChatVertexAI.html#langchain_google_vertexai.chat_models.ChatVertexAI.bind_tools">[docs]</a>    <span class="k">def</span> <span class="nf">bind_tools</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">_ToolsType</span><span class="p">,</span>
        <span class="n">tool_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ToolConfigDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tool_choice</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">_ToolChoiceType</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">BaseMessage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Bind tool-like objects to this chat model.</span>

<span class="sd">        Assumes model is compatible with Vertex tool-calling API.</span>

<span class="sd">        Args:</span>
<span class="sd">            tools: A list of tool definitions to bind to this chat model.</span>
<span class="sd">                Can be a pydantic model, callable, or BaseTool. Pydantic</span>
<span class="sd">                models, callables, and BaseTools will be automatically converted to</span>
<span class="sd">                their schema dictionary representation.</span>
<span class="sd">            **kwargs: Any additional parameters to pass to the</span>
<span class="sd">                :class:`~langchain.runnable.Runnable` constructor.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">tool_choice</span> <span class="ow">and</span> <span class="n">tool_config</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Must specify at most one of tool_choice and tool_config, received "</span>
                <span class="sa">f</span><span class="s2">"both:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">tool_choice</span><span class="si">=}</span><span class="se">\n\n</span><span class="si">{</span><span class="n">tool_config</span><span class="si">=}</span><span class="s2">"</span>
            <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">formatted_tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">tool</span><span class="p">)</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">]</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">formatted_tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">_format_to_gapic_tool</span><span class="p">(</span><span class="n">tools</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">tool_choice</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"tool_choice"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tool_choice</span>
        <span class="k">elif</span> <span class="n">tool_config</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"tool_config"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tool_config</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">formatted_tools</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">history</span><span class="p">:</span> <span class="n">_ChatHistory</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">ChatSession</span><span class="p">,</span> <span class="n">CodeChatSession</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_family</span> <span class="o">==</span> <span class="n">GoogleModelFamily</span><span class="o">.</span><span class="n">CODEY</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">start_chat</span><span class="p">(</span>
                <span class="n">context</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">context</span><span class="p">,</span> <span class="n">message_history</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">start_chat</span><span class="p">(</span><span class="n">message_history</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_gemini_response_to_chat_result</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">:</span> <span class="n">GenerationResponse</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="n">generations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">usage</span> <span class="o">=</span> <span class="n">proto</span><span class="o">.</span><span class="n">Message</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">usage_metadata</span><span class="p">)</span>
        <span class="n">lc_usage</span> <span class="o">=</span> <span class="n">_get_usage_metadata_gemini</span><span class="p">(</span><span class="n">usage</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span><span class="p">:</span>
            <span class="n">info</span> <span class="o">=</span> <span class="n">get_generation_info</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="n">is_gemini</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage</span><span class="p">)</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">_parse_response_candidate</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
                <span class="n">message</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">lc_usage</span>
            <span class="n">generations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ChatGeneration</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">generation_info</span><span class="o">=</span><span class="n">info</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">usage</span><span class="p">:</span>
                <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"usage_metadata"</span><span class="p">:</span> <span class="n">usage</span><span class="p">}</span>
                <span class="n">message</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">lc_usage</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">generations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">ChatGeneration</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">generation_info</span><span class="o">=</span><span class="n">generation_info</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">ChatResult</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="n">generations</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_gemini_chunk_to_generation_chunk</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">response_chunk</span><span class="p">:</span> <span class="n">GenerationResponse</span><span class="p">,</span>
        <span class="n">prev_total_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UsageMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UsageMetadata</span><span class="p">]]:</span>
        <span class="c1"># return an empty completion message if there's no candidates</span>
        <span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">proto</span><span class="o">.</span><span class="n">Message</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="n">response_chunk</span><span class="o">.</span><span class="n">usage_metadata</span><span class="p">)</span>

        <span class="c1"># Gather langchain (standard) usage metadata</span>
        <span class="c1"># Note: some models (e.g., gemini-1.5-pro with image inputs) return</span>
        <span class="c1"># cumulative sums of token counts.</span>
        <span class="n">total_lc_usage</span> <span class="o">=</span> <span class="n">_get_usage_metadata_gemini</span><span class="p">(</span><span class="n">usage_metadata</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">total_lc_usage</span> <span class="ow">and</span> <span class="n">prev_total_usage</span><span class="p">:</span>
            <span class="n">lc_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UsageMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="n">UsageMetadata</span><span class="p">(</span>
                <span class="n">input_tokens</span><span class="o">=</span><span class="n">total_lc_usage</span><span class="p">[</span><span class="s2">"input_tokens"</span><span class="p">]</span>
                <span class="o">-</span> <span class="n">prev_total_usage</span><span class="p">[</span><span class="s2">"input_tokens"</span><span class="p">],</span>
                <span class="n">output_tokens</span><span class="o">=</span><span class="n">total_lc_usage</span><span class="p">[</span><span class="s2">"output_tokens"</span><span class="p">]</span>
                <span class="o">-</span> <span class="n">prev_total_usage</span><span class="p">[</span><span class="s2">"output_tokens"</span><span class="p">],</span>
                <span class="n">total_tokens</span><span class="o">=</span><span class="n">total_lc_usage</span><span class="p">[</span><span class="s2">"total_tokens"</span><span class="p">]</span>
                <span class="o">-</span> <span class="n">prev_total_usage</span><span class="p">[</span><span class="s2">"total_tokens"</span><span class="p">],</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lc_usage</span> <span class="o">=</span> <span class="n">total_lc_usage</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">response_chunk</span><span class="o">.</span><span class="n">candidates</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">AIMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lc_usage</span><span class="p">:</span>
                <span class="n">message</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">lc_usage</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">top_candidate</span> <span class="o">=</span> <span class="n">response_chunk</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">_parse_response_candidate</span><span class="p">(</span><span class="n">top_candidate</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lc_usage</span><span class="p">:</span>
                <span class="n">message</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">lc_usage</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="n">get_generation_info</span><span class="p">(</span>
                <span class="n">top_candidate</span><span class="p">,</span>
                <span class="n">is_gemini</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># is_blocked is part of "safety_ratings" list</span>
            <span class="c1"># but if it's True/False then chunks can't be marged</span>
            <span class="n">generation_info</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"is_blocked"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">generation_info</span><span class="o">=</span><span class="n">generation_info</span><span class="p">,</span>
        <span class="p">),</span> <span class="n">total_lc_usage</span></div>


<span class="k">def</span> <span class="nf">_yield_args</span><span class="p">(</span><span class="n">tool_call_chunks</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">tc</span> <span class="ow">in</span> <span class="n">tool_call_chunks</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">tc</span><span class="p">[</span><span class="s2">"args"</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_get_usage_metadata_gemini</span><span class="p">(</span><span class="n">raw_metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UsageMetadata</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""Get UsageMetadata from raw response metadata."""</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">raw_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"prompt_token_count"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">raw_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"candidates_token_count"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">raw_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"total_token_count"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">count</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="p">[</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">output_tokens</span><span class="p">,</span> <span class="n">total_tokens</span><span class="p">]):</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">UsageMetadata</span><span class="p">(</span>
            <span class="n">input_tokens</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">,</span>
            <span class="n">output_tokens</span><span class="o">=</span><span class="n">output_tokens</span><span class="p">,</span>
            <span class="n">total_tokens</span><span class="o">=</span><span class="n">total_tokens</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_usage_metadata_non_gemini</span><span class="p">(</span><span class="n">raw_metadata</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UsageMetadata</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""Get UsageMetadata from raw response metadata."""</span>
    <span class="n">token_usage</span> <span class="o">=</span> <span class="n">raw_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tokenMetadata"</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"inputTokenCount"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"totalTokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"outputTokenCount"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"totalTokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_tokens</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">output_tokens</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">UsageMetadata</span><span class="p">(</span>
            <span class="n">input_tokens</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">,</span>
            <span class="n">output_tokens</span><span class="o">=</span><span class="n">output_tokens</span><span class="p">,</span>
            <span class="n">total_tokens</span><span class="o">=</span><span class="n">input_tokens</span> <span class="o">+</span> <span class="n">output_tokens</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_tool_name</span><span class="p">(</span><span class="n">tool</span><span class="p">:</span> <span class="n">_ToolType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">vertexai_tool</span> <span class="o">=</span> <span class="n">_format_to_gapic_tool</span><span class="p">([</span><span class="n">tool</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">vertexai_tool</span><span class="o">.</span><span class="n">function_declarations</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</article>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      Â© Copyright 2023, LangChain Inc.
      <br/>
</p>
</div>
</div>
</div>
</footer>
</body>
</html>