
<!DOCTYPE html>

<html data-content_root="" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>langchain_openai.chat_models.base â€” ðŸ¦œðŸ”— LangChain  documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/autodoc_pydantic.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/sphinx_highlight.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/design-tabs.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = '_modules/langchain_openai/chat_models/base';</script>
<link href="../../../_static/favicon.png" rel="icon"/>
<link href="../../../search.html" rel="search" title="Search"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Sep 04, 2024" name="docbuild:last-update"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../../../index.html">
<img alt="ðŸ¦œðŸ”— LangChain  documentation - Home" class="logo__image only-light" src="../../../_static/wordmark-api.svg"/>
<script>document.write(`<img src="../../../_static/wordmark-api-dark.svg" class="logo__image only-dark" alt="ðŸ¦œðŸ”— LangChain  documentation - Home"/>`);</script>
</a></div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-external" href="https://api.python.langchain.com/">
    Legacy reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item navbar-persistent--container">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="navbar-persistent--mobile">
<form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar hide-on-wide">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item">
<a class="nav-link nav-internal" href="../../../reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-external" href="https://api.python.langchain.com/">
    Legacy reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../../index.html">Module code</a></li>
<li aria-current="page" class="breadcrumb-item active">langchain_op...</li>
</ul>
</nav>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<h1>Source code for langchain_openai.chat_models.base</h1><div class="highlight"><pre>
<span></span><span class="sd">"""OpenAI chat wrapper."""</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">ceil</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">AsyncIterator</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Literal</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">TypedDict</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">urlparse</span>

<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">tiktoken</span>
<span class="kn">from</span> <span class="nn">langchain_core.callbacks</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">,</span>
    <span class="n">CallbackManagerForLLMRun</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.language_models</span> <span class="kn">import</span> <span class="n">LanguageModelInput</span>
<span class="kn">from</span> <span class="nn">langchain_core.language_models.chat_models</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BaseChatModel</span><span class="p">,</span>
    <span class="n">LangSmithParams</span><span class="p">,</span>
    <span class="n">agenerate_from_stream</span><span class="p">,</span>
    <span class="n">generate_from_stream</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AIMessage</span><span class="p">,</span>
    <span class="n">AIMessageChunk</span><span class="p">,</span>
    <span class="n">BaseMessage</span><span class="p">,</span>
    <span class="n">BaseMessageChunk</span><span class="p">,</span>
    <span class="n">ChatMessage</span><span class="p">,</span>
    <span class="n">ChatMessageChunk</span><span class="p">,</span>
    <span class="n">FunctionMessage</span><span class="p">,</span>
    <span class="n">FunctionMessageChunk</span><span class="p">,</span>
    <span class="n">HumanMessage</span><span class="p">,</span>
    <span class="n">HumanMessageChunk</span><span class="p">,</span>
    <span class="n">InvalidToolCall</span><span class="p">,</span>
    <span class="n">SystemMessage</span><span class="p">,</span>
    <span class="n">SystemMessageChunk</span><span class="p">,</span>
    <span class="n">ToolCall</span><span class="p">,</span>
    <span class="n">ToolMessage</span><span class="p">,</span>
    <span class="n">ToolMessageChunk</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages.ai</span> <span class="kn">import</span> <span class="n">UsageMetadata</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages.tool</span> <span class="kn">import</span> <span class="n">tool_call_chunk</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">JsonOutputParser</span><span class="p">,</span> <span class="n">PydanticOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers.base</span> <span class="kn">import</span> <span class="n">OutputParserLike</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers.openai_tools</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">JsonOutputKeyToolsParser</span><span class="p">,</span>
    <span class="n">PydanticToolsParser</span><span class="p">,</span>
    <span class="n">make_invalid_tool_call</span><span class="p">,</span>
    <span class="n">parse_tool_call</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.outputs</span> <span class="kn">import</span> <span class="n">ChatGeneration</span><span class="p">,</span> <span class="n">ChatGenerationChunk</span><span class="p">,</span> <span class="n">ChatResult</span>
<span class="kn">from</span> <span class="nn">langchain_core.pydantic_v1</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">SecretStr</span><span class="p">,</span> <span class="n">root_validator</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">Runnable</span><span class="p">,</span> <span class="n">RunnableMap</span><span class="p">,</span> <span class="n">RunnablePassthrough</span><span class="p">,</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables.config</span> <span class="kn">import</span> <span class="n">run_in_executor</span>
<span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">BaseTool</span>
<span class="kn">from</span> <span class="nn">langchain_core.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_to_secret_str</span><span class="p">,</span>
    <span class="n">get_from_dict_or_env</span><span class="p">,</span>
    <span class="n">get_pydantic_field_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.utils.function_calling</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_to_openai_function</span><span class="p">,</span>
    <span class="n">convert_to_openai_tool</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.utils.pydantic</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PydanticBaseModel</span><span class="p">,</span>
    <span class="n">TypeBaseModel</span><span class="p">,</span>
    <span class="n">is_basemodel_subclass</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.utils.utils</span> <span class="kn">import</span> <span class="n">build_extra_kwargs</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_convert_dict_to_message</span><span class="p">(</span><span class="n">_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">BaseMessage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Convert a dictionary to a LangChain message.</span>

<span class="sd">    Args:</span>
<span class="sd">        _dict: The dictionary.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The LangChain message.</span>
<span class="sd">    """</span>
    <span class="n">role</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"role"</span><span class="p">)</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">)</span>
    <span class="n">id_</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"user"</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"assistant"</span><span class="p">:</span>
        <span class="c1"># Fix for azure</span>
        <span class="c1"># Also OpenAI returns None for tool invocations</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">""</span>
        <span class="n">additional_kwargs</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">function_call</span> <span class="o">:=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"function_call"</span><span class="p">):</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">function_call</span><span class="p">)</span>
        <span class="n">tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">invalid_tool_calls</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">raw_tool_calls</span> <span class="o">:=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tool_calls"</span><span class="p">):</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw_tool_calls</span>
            <span class="k">for</span> <span class="n">raw_tool_call</span> <span class="ow">in</span> <span class="n">raw_tool_calls</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">tool_calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parse_tool_call</span><span class="p">(</span><span class="n">raw_tool_call</span><span class="p">,</span> <span class="n">return_id</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">invalid_tool_calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">make_invalid_tool_call</span><span class="p">(</span><span class="n">raw_tool_call</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                    <span class="p">)</span>
        <span class="k">return</span> <span class="n">AIMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span>
            <span class="n">tool_calls</span><span class="o">=</span><span class="n">tool_calls</span><span class="p">,</span>
            <span class="n">invalid_tool_calls</span><span class="o">=</span><span class="n">invalid_tool_calls</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"system"</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"function"</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">FunctionMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">)),</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"tool"</span><span class="p">:</span>
        <span class="n">additional_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">"name"</span> <span class="ow">in</span> <span class="n">_dict</span><span class="p">:</span>
            <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dict</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">ToolMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span>
            <span class="n">tool_call_id</span><span class="o">=</span><span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tool_call_id"</span><span class="p">)),</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>


<span class="k">def</span> <span class="nf">_format_message_content</span><span class="p">(</span><span class="n">content</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Format message content."""</span>
    <span class="k">if</span> <span class="n">content</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="c1"># Remove unexpected block types</span>
        <span class="n">formatted_content</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">content</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="ow">and</span> <span class="s2">"type"</span> <span class="ow">in</span> <span class="n">block</span>
                <span class="ow">and</span> <span class="n">block</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"tool_use"</span>
            <span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">formatted_content</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">formatted_content</span> <span class="o">=</span> <span class="n">content</span>

    <span class="k">return</span> <span class="n">formatted_content</span>


<span class="k">def</span> <span class="nf">_convert_message_to_dict</span><span class="p">(</span><span class="n">message</span><span class="p">:</span> <span class="n">BaseMessage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Convert a LangChain message to a dictionary.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: The LangChain message.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The dictionary.</span>
<span class="sd">    """</span>
    <span class="n">message_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">_format_message_content</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="o">:=</span> <span class="n">message</span><span class="o">.</span><span class="n">name</span> <span class="ow">or</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">))</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span>

    <span class="c1"># populate role and additional message data</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">ChatMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">role</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"user"</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"assistant"</span>
        <span class="k">if</span> <span class="s2">"function_call"</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span> <span class="ow">or</span> <span class="n">message</span><span class="o">.</span><span class="n">invalid_tool_calls</span><span class="p">:</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">_lc_tool_call_to_openai_tool_call</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span> <span class="k">for</span> <span class="n">tc</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span>
            <span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
                <span class="n">_lc_invalid_tool_call_to_openai_tool_call</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">tc</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">invalid_tool_calls</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="s2">"tool_calls"</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">:</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span>
            <span class="n">tool_call_supported_props</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"id"</span><span class="p">,</span> <span class="s2">"type"</span><span class="p">,</span> <span class="s2">"function"</span><span class="p">}</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tool_call</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">tool_call_supported_props</span><span class="p">}</span>
                <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="c1"># If tool calls present, content null value should be None not empty string.</span>
        <span class="k">if</span> <span class="s2">"function_call"</span> <span class="ow">in</span> <span class="n">message_dict</span> <span class="ow">or</span> <span class="s2">"tool_calls"</span> <span class="ow">in</span> <span class="n">message_dict</span><span class="p">:</span>
            <span class="n">message_dict</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message_dict</span><span class="p">[</span><span class="s2">"content"</span><span class="p">]</span> <span class="ow">or</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"system"</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">FunctionMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"function"</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">ToolMessage</span><span class="p">):</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"role"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"tool"</span>
        <span class="n">message_dict</span><span class="p">[</span><span class="s2">"tool_call_id"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_call_id</span>

        <span class="n">supported_props</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"content"</span><span class="p">,</span> <span class="s2">"role"</span><span class="p">,</span> <span class="s2">"tool_call_id"</span><span class="p">}</span>
        <span class="n">message_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">message_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">supported_props</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Got unknown type </span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">message_dict</span>


<span class="k">def</span> <span class="nf">_convert_delta_to_message_chunk</span><span class="p">(</span>
    <span class="n">_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">default_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">BaseMessageChunk</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseMessageChunk</span><span class="p">:</span>
    <span class="n">id_</span> <span class="o">=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">)</span>
    <span class="n">role</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"role"</span><span class="p">))</span>
    <span class="n">content</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"content"</span><span class="p">)</span> <span class="ow">or</span> <span class="s2">""</span><span class="p">)</span>
    <span class="n">additional_kwargs</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"function_call"</span><span class="p">):</span>
        <span class="n">function_call</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_dict</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">])</span>
        <span class="k">if</span> <span class="s2">"name"</span> <span class="ow">in</span> <span class="n">function_call</span> <span class="ow">and</span> <span class="n">function_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">function_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">""</span>
        <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"function_call"</span><span class="p">]</span> <span class="o">=</span> <span class="n">function_call</span>
    <span class="n">tool_call_chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">raw_tool_calls</span> <span class="o">:=</span> <span class="n">_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"tool_calls"</span><span class="p">):</span>
        <span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"tool_calls"</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw_tool_calls</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">tool_call_chunks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">tool_call_chunk</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="n">rtc</span><span class="p">[</span><span class="s2">"function"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">),</span>
                    <span class="n">args</span><span class="o">=</span><span class="n">rtc</span><span class="p">[</span><span class="s2">"function"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"arguments"</span><span class="p">),</span>
                    <span class="nb">id</span><span class="o">=</span><span class="n">rtc</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"id"</span><span class="p">),</span>
                    <span class="n">index</span><span class="o">=</span><span class="n">rtc</span><span class="p">[</span><span class="s2">"index"</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">rtc</span> <span class="ow">in</span> <span class="n">raw_tool_calls</span>
            <span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"user"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">HumanMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">HumanMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"assistant"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">AIMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">AIMessageChunk</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">additional_kwargs</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">,</span>
            <span class="n">tool_call_chunks</span><span class="o">=</span><span class="n">tool_call_chunks</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"system"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">SystemMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SystemMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"function"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">FunctionMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">FunctionMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">_dict</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="o">==</span> <span class="s2">"tool"</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">ToolMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ToolMessageChunk</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">_dict</span><span class="p">[</span><span class="s2">"tool_call_id"</span><span class="p">],</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">role</span> <span class="ow">or</span> <span class="n">default_class</span> <span class="o">==</span> <span class="n">ChatMessageChunk</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ChatMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_class</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">id_</span><span class="p">)</span>  <span class="c1"># type: ignore</span>


<span class="k">def</span> <span class="nf">_convert_chunk_to_generation_chunk</span><span class="p">(</span>
    <span class="n">chunk</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">default_chunk_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">,</span> <span class="n">base_generation_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
    <span class="n">token_usage</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"usage"</span><span class="p">)</span>
    <span class="n">choices</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"choices"</span><span class="p">,</span> <span class="p">[])</span>
    <span class="n">usage_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UsageMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">UsageMetadata</span><span class="p">(</span>
            <span class="n">input_tokens</span><span class="o">=</span><span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"prompt_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">output_tokens</span><span class="o">=</span><span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"completion_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">total_tokens</span><span class="o">=</span><span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"total_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">token_usage</span>
        <span class="k">else</span> <span class="kc">None</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">choices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># logprobs is implicitly None</span>
        <span class="n">generation_chunk</span> <span class="o">=</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">default_chunk_class</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span> <span class="n">usage_metadata</span><span class="o">=</span><span class="n">usage_metadata</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">generation_chunk</span>

    <span class="n">choice</span> <span class="o">=</span> <span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">choice</span><span class="p">[</span><span class="s2">"delta"</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">message_chunk</span> <span class="o">=</span> <span class="n">_convert_delta_to_message_chunk</span><span class="p">(</span>
        <span class="n">choice</span><span class="p">[</span><span class="s2">"delta"</span><span class="p">],</span> <span class="n">default_chunk_class</span>
    <span class="p">)</span>
    <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">base_generation_info</span><span class="p">}</span> <span class="k">if</span> <span class="n">base_generation_info</span> <span class="k">else</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">finish_reason</span> <span class="o">:=</span> <span class="n">choice</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">):</span>
        <span class="n">generation_info</span><span class="p">[</span><span class="s2">"finish_reason"</span><span class="p">]</span> <span class="o">=</span> <span class="n">finish_reason</span>
        <span class="k">if</span> <span class="n">model_name</span> <span class="o">:=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model"</span><span class="p">):</span>
            <span class="n">generation_info</span><span class="p">[</span><span class="s2">"model_name"</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="k">if</span> <span class="n">system_fingerprint</span> <span class="o">:=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"system_fingerprint"</span><span class="p">):</span>
            <span class="n">generation_info</span><span class="p">[</span><span class="s2">"system_fingerprint"</span><span class="p">]</span> <span class="o">=</span> <span class="n">system_fingerprint</span>

    <span class="n">logprobs</span> <span class="o">=</span> <span class="n">choice</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logprobs"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">logprobs</span><span class="p">:</span>
        <span class="n">generation_info</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span> <span class="o">=</span> <span class="n">logprobs</span>

    <span class="k">if</span> <span class="n">usage_metadata</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message_chunk</span><span class="p">,</span> <span class="n">AIMessageChunk</span><span class="p">):</span>
        <span class="n">message_chunk</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="n">usage_metadata</span>

    <span class="n">generation_chunk</span> <span class="o">=</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message_chunk</span><span class="p">,</span> <span class="n">generation_info</span><span class="o">=</span><span class="n">generation_info</span> <span class="ow">or</span> <span class="kc">None</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">generation_chunk</span>


<span class="k">class</span> <span class="nc">_FunctionCall</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>


<span class="n">_BM</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">"_BM"</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="n">BaseModel</span><span class="p">)</span>
<span class="n">_DictOrPydanticClass</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Type</span><span class="p">[</span><span class="n">_BM</span><span class="p">],</span> <span class="n">Type</span><span class="p">]</span>
<span class="n">_DictOrPydantic</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">_BM</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">_AllReturnType</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">raw</span><span class="p">:</span> <span class="n">BaseMessage</span>
    <span class="n">parsed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_DictOrPydantic</span><span class="p">]</span>
    <span class="n">parsing_error</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="ne">BaseException</span><span class="p">]</span>


<div class="viewcode-block" id="BaseChatOpenAI"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_together.chat_models.BaseChatOpenAI">[docs]</a><span class="k">class</span> <span class="nc">BaseChatOpenAI</span><span class="p">(</span><span class="n">BaseChatModel</span><span class="p">):</span>
    <span class="n">client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">async_client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">root_client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">root_async_client</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#: :meta private:</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">"gpt-3.5-turbo"</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"model"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Model name to use."""</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="w">    </span><span class="sd">"""What sampling temperature to use."""</span>
    <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Holds any model parameters valid for `create` call not explicitly specified."""</span>
    <span class="n">openai_api_key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SecretStr</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"api_key"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Automatically inferred from env var `OPENAI_API_KEY` if not provided."""</span>
    <span class="n">openai_api_base</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"base_url"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Base URL path for API requests, leave blank if not using a proxy or service </span>
<span class="sd">        emulator."""</span>
    <span class="n">openai_organization</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"organization"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Automatically inferred from env var `OPENAI_ORG_ID` if not provided."""</span>
    <span class="c1"># to support explicit proxy for OpenAI</span>
    <span class="n">openai_proxy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">request_timeout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">Any</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"timeout"</span>
    <span class="p">)</span>
<span class="w">    </span><span class="sd">"""Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or </span>
<span class="sd">        None."""</span>
    <span class="n">max_retries</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
<span class="w">    </span><span class="sd">"""Maximum number of retries to make when generating."""</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Penalizes repeated tokens."""</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Penalizes repeated tokens according to frequency."""</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Seed for generation"""</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Whether to return logprobs."""</span>
    <span class="n">top_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Number of most likely tokens to return at each token position, each with</span>
<span class="sd">     an associated log probability. `logprobs` must be set to true </span>
<span class="sd">     if this parameter is used."""</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Modify the likelihood of specified tokens appearing in the completion."""</span>
    <span class="n">streaming</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""Whether to stream the results or not."""</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="w">    </span><span class="sd">"""Number of chat completions to generate for each prompt."""</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Total probability mass of tokens to consider at each step."""</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Maximum number of tokens to generate."""</span>
    <span class="n">tiktoken_model_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""The model name to pass to tiktoken when using this class. </span>
<span class="sd">    Tiktoken is used to count the number of tokens in documents to constrain </span>
<span class="sd">    them to be under a certain limit. By default, when set to None, this will </span>
<span class="sd">    be the same as the embedding model name. However, there are some cases </span>
<span class="sd">    where you may want to use this Embedding class with a model name not </span>
<span class="sd">    supported by tiktoken. This can include when using Azure embeddings or </span>
<span class="sd">    when using one of the many model providers that expose an OpenAI-like </span>
<span class="sd">    API but with different models. In those cases, in order to avoid erroring </span>
<span class="sd">    when tiktoken is called, you can specify a model name to use here."""</span>
    <span class="n">default_headers</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">default_query</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Configure a custom httpx client. See the</span>
    <span class="c1"># [httpx documentation](https://www.python-httpx.org/api/#client) for more details.</span>
    <span class="n">http_client</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Optional httpx.Client. Only used for sync invocations. Must specify </span>
<span class="sd">        http_async_client as well if you'd like a custom client for async invocations.</span>
<span class="sd">    """</span>
    <span class="n">http_async_client</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Optional httpx.AsyncClient. Only used for async invocations. Must specify </span>
<span class="sd">        http_client as well if you'd like a custom client for sync invocations."""</span>
    <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">"stop_sequences"</span><span class="p">)</span>
<span class="w">    </span><span class="sd">"""Default stop sequences."""</span>
    <span class="n">extra_body</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="w">    </span><span class="sd">"""Optional additional JSON properties to include in the request parameters when</span>
<span class="sd">    making requests to OpenAI compatible APIs, such as vLLM."""</span>
    <span class="n">include_response_headers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""Whether to include response headers in the output message response_metadata."""</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Configuration for this pydantic object."""</span>

        <span class="n">allow_population_by_field_name</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="nd">@root_validator</span><span class="p">(</span><span class="n">pre</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">build_extra</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Build extra kwargs from additional params that were passed in."""</span>
        <span class="n">all_required_field_names</span> <span class="o">=</span> <span class="n">get_pydantic_field_names</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
        <span class="n">extra</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model_kwargs"</span><span class="p">,</span> <span class="p">{})</span>
        <span class="n">values</span><span class="p">[</span><span class="s2">"model_kwargs"</span><span class="p">]</span> <span class="o">=</span> <span class="n">build_extra_kwargs</span><span class="p">(</span>
            <span class="n">extra</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">all_required_field_names</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">values</span>

    <span class="nd">@root_validator</span><span class="p">(</span><span class="n">pre</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">skip_on_failure</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">validate_environment</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Validate that api key and python package exists in environment."""</span>
        <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"n"</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"n must be at least 1."</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"n"</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">values</span><span class="p">[</span><span class="s2">"streaming"</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"n must be 1 when streaming."</span><span class="p">)</span>

        <span class="n">values</span><span class="p">[</span><span class="s2">"openai_api_key"</span><span class="p">]</span> <span class="o">=</span> <span class="n">convert_to_secret_str</span><span class="p">(</span>
            <span class="n">get_from_dict_or_env</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="s2">"openai_api_key"</span><span class="p">,</span> <span class="s2">"OPENAI_API_KEY"</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Check OPENAI_ORGANIZATION for backwards compatibility.</span>
        <span class="n">values</span><span class="p">[</span><span class="s2">"openai_organization"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"openai_organization"</span><span class="p">]</span>
            <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"OPENAI_ORG_ID"</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"OPENAI_ORGANIZATION"</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">values</span><span class="p">[</span><span class="s2">"openai_api_base"</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_api_base"</span><span class="p">]</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span>
            <span class="s2">"OPENAI_API_BASE"</span>
        <span class="p">)</span>
        <span class="n">values</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_from_dict_or_env</span><span class="p">(</span>
            <span class="n">values</span><span class="p">,</span> <span class="s2">"openai_proxy"</span><span class="p">,</span> <span class="s2">"OPENAI_PROXY"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">""</span>
        <span class="p">)</span>

        <span class="n">client_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"api_key"</span><span class="p">:</span> <span class="p">(</span>
                <span class="n">values</span><span class="p">[</span><span class="s2">"openai_api_key"</span><span class="p">]</span><span class="o">.</span><span class="n">get_secret_value</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_api_key"</span><span class="p">]</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">),</span>
            <span class="s2">"organization"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_organization"</span><span class="p">],</span>
            <span class="s2">"base_url"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_api_base"</span><span class="p">],</span>
            <span class="s2">"timeout"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"request_timeout"</span><span class="p">],</span>
            <span class="s2">"max_retries"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"max_retries"</span><span class="p">],</span>
            <span class="s2">"default_headers"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"default_headers"</span><span class="p">],</span>
            <span class="s2">"default_query"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"default_query"</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"http_client"</span><span class="p">]</span> <span class="ow">or</span> <span class="n">values</span><span class="p">[</span><span class="s2">"http_async_client"</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="n">openai_proxy</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span>
            <span class="n">http_client</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"http_client"</span><span class="p">]</span>
            <span class="n">http_async_client</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"http_async_client"</span><span class="p">]</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Cannot specify 'openai_proxy' if one of "</span>
                <span class="s2">"'http_client'/'http_async_client' is already specified. Received:</span><span class="se">\n</span><span class="s2">"</span>
                <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">openai_proxy</span><span class="si">=}</span><span class="se">\n</span><span class="si">{</span><span class="n">http_client</span><span class="si">=}</span><span class="se">\n</span><span class="si">{</span><span class="n">http_async_client</span><span class="si">=}</span><span class="s2">"</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"client"</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">values</span><span class="p">[</span><span class="s2">"http_client"</span><span class="p">]:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">import</span> <span class="nn">httpx</span>
                <span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                        <span class="s2">"Could not import httpx python package. "</span>
                        <span class="s2">"Please install it with `pip install httpx`."</span>
                    <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
                <span class="n">values</span><span class="p">[</span><span class="s2">"http_client"</span><span class="p">]</span> <span class="o">=</span> <span class="n">httpx</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">proxy</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">])</span>
            <span class="n">sync_specific</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"http_client"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"http_client"</span><span class="p">]}</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"root_client"</span><span class="p">]</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="o">**</span><span class="n">client_params</span><span class="p">,</span> <span class="o">**</span><span class="n">sync_specific</span><span class="p">)</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"client"</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"root_client"</span><span class="p">]</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">values</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"async_client"</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">values</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">values</span><span class="p">[</span><span class="s2">"http_async_client"</span><span class="p">]:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">import</span> <span class="nn">httpx</span>
                <span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                        <span class="s2">"Could not import httpx python package. "</span>
                        <span class="s2">"Please install it with `pip install httpx`."</span>
                    <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
                <span class="n">values</span><span class="p">[</span><span class="s2">"http_async_client"</span><span class="p">]</span> <span class="o">=</span> <span class="n">httpx</span><span class="o">.</span><span class="n">AsyncClient</span><span class="p">(</span>
                    <span class="n">proxy</span><span class="o">=</span><span class="n">values</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="n">async_specific</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"http_client"</span><span class="p">:</span> <span class="n">values</span><span class="p">[</span><span class="s2">"http_async_client"</span><span class="p">]}</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"root_async_client"</span><span class="p">]</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">AsyncOpenAI</span><span class="p">(</span>
                <span class="o">**</span><span class="n">client_params</span><span class="p">,</span> <span class="o">**</span><span class="n">async_specific</span>
            <span class="p">)</span>
            <span class="n">values</span><span class="p">[</span><span class="s2">"async_client"</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="s2">"root_async_client"</span><span class="p">]</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span>
        <span class="k">return</span> <span class="n">values</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_default_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the default parameters for calling OpenAI API."""</span>
        <span class="n">exclude_if_none</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"presence_penalty"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="s2">"frequency_penalty"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="s2">"seed"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
            <span class="s2">"top_p"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
            <span class="s2">"logprobs"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">logprobs</span><span class="p">,</span>
            <span class="s2">"top_logprobs"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_logprobs</span><span class="p">,</span>
            <span class="s2">"logit_bias"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">logit_bias</span><span class="p">,</span>
            <span class="s2">"stop"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span> <span class="ow">or</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># also exclude empty list for this</span>
            <span class="s2">"max_tokens"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="s2">"extra_body"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_body</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"model"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="s2">"stream"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">,</span>
            <span class="s2">"n"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
            <span class="s2">"temperature"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">exclude_if_none</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">},</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">model_kwargs</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span> <span class="nf">_combine_llm_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm_outputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">overall_token_usage</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">system_fingerprint</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">llm_outputs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Happens in streaming</span>
                <span class="k">continue</span>
            <span class="n">token_usage</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">"token_usage"</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">token_usage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">token_usage</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">overall_token_usage</span><span class="p">:</span>
                        <span class="n">overall_token_usage</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">v</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">overall_token_usage</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
            <span class="k">if</span> <span class="n">system_fingerprint</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">system_fingerprint</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"system_fingerprint"</span><span class="p">)</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"token_usage"</span><span class="p">:</span> <span class="n">overall_token_usage</span><span class="p">,</span> <span class="s2">"model_name"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">system_fingerprint</span><span class="p">:</span>
            <span class="n">combined</span><span class="p">[</span><span class="s2">"system_fingerprint"</span><span class="p">]</span> <span class="o">=</span> <span class="n">system_fingerprint</span>
        <span class="k">return</span> <span class="n">combined</span>

    <span class="k">def</span> <span class="nf">_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">default_chunk_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">BaseMessageChunk</span><span class="p">]</span> <span class="o">=</span> <span class="n">AIMessageChunk</span>
        <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span> <span class="ow">and</span> <span class="n">is_basemodel_subclass</span><span class="p">(</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"response_format"</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="c1"># TODO: Add support for streaming with Pydantic response_format.</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">"Streaming with Pydantic response_format not yet supported."</span><span class="p">)</span>
            <span class="n">chat_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="n">chat_result</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>
            <span class="k">yield</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="n">AIMessageChunk</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">msg</span><span class="o">.</span><span class="n">dict</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">{</span><span class="s2">"type"</span><span class="p">,</span> <span class="s2">"additional_kwargs"</span><span class="p">}),</span>
                    <span class="c1"># preserve the "parsed" Pydantic object without converting to dict</span>
                    <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="n">generation_info</span><span class="o">=</span><span class="n">chat_result</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">generation_info</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">response</span><span class="p">:</span>
            <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">chunk</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
                <span class="n">generation_chunk</span> <span class="o">=</span> <span class="n">_convert_chunk_to_generation_chunk</span><span class="p">(</span>
                    <span class="n">chunk</span><span class="p">,</span>
                    <span class="n">default_chunk_class</span><span class="p">,</span>
                    <span class="n">base_generation_info</span> <span class="k">if</span> <span class="n">is_first_chunk</span> <span class="k">else</span> <span class="p">{},</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_chunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">default_chunk_class</span> <span class="o">=</span> <span class="n">generation_chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="vm">__class__</span>
                <span class="n">logprobs</span> <span class="o">=</span> <span class="p">(</span><span class="n">generation_chunk</span><span class="o">.</span><span class="n">generation_info</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logprobs"</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                    <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                        <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span><span class="p">,</span> <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span>
                    <span class="p">)</span>
                <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">yield</span> <span class="n">generation_chunk</span>

    <span class="k">def</span> <span class="nf">_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">:</span>
            <span class="n">stream_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stream</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">generate_from_stream</span><span class="p">(</span><span class="n">stream_iter</span><span class="p">)</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">generation_info</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">"Cannot currently include response headers when response_format is "</span>
                    <span class="s2">"specified."</span>
                <span class="p">)</span>
            <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chat_result</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">generation_info</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_request_payload</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_</span><span class="p">:</span> <span class="n">LanguageModelInput</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_input</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span><span class="o">.</span><span class="n">to_messages</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">stop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stop"</span><span class="p">]</span> <span class="o">=</span> <span class="n">stop</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">_convert_message_to_dict</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">],</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_create_chat_result</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">response</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">openai</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">],</span>
        <span class="n">generation_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="n">generations</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">response_dict</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">response</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">response</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="c1"># Sometimes the AI Model calling will get error, we should raise it.</span>
        <span class="c1"># Otherwise, the next code 'choices.extend(response["choices"])'</span>
        <span class="c1"># will throw a "TypeError: 'NoneType' object is not iterable" error</span>
        <span class="c1"># to mask the true error. Because 'response["choices"]' is None.</span>
        <span class="k">if</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"error"</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"error"</span><span class="p">))</span>

        <span class="n">token_usage</span> <span class="o">=</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"usage"</span><span class="p">,</span> <span class="p">{})</span>
        <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">response_dict</span><span class="p">[</span><span class="s2">"choices"</span><span class="p">]:</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">_convert_dict_to_message</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">"message"</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">token_usage</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
                <span class="n">message</span><span class="o">.</span><span class="n">usage_metadata</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">"input_tokens"</span><span class="p">:</span> <span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"prompt_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                    <span class="s2">"output_tokens"</span><span class="p">:</span> <span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"completion_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                    <span class="s2">"total_tokens"</span><span class="p">:</span> <span class="n">token_usage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"total_tokens"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="p">}</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="n">generation_info</span> <span class="ow">or</span> <span class="p">{}</span>
            <span class="n">generation_info</span><span class="p">[</span><span class="s2">"finish_reason"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">generation_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"finish_reason"</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="s2">"logprobs"</span> <span class="ow">in</span> <span class="n">res</span><span class="p">:</span>
                <span class="n">generation_info</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span>
            <span class="n">gen</span> <span class="o">=</span> <span class="n">ChatGeneration</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">generation_info</span><span class="o">=</span><span class="n">generation_info</span><span class="p">)</span>
            <span class="n">generations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
        <span class="n">llm_output</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"token_usage"</span><span class="p">:</span> <span class="n">token_usage</span><span class="p">,</span>
            <span class="s2">"model_name"</span><span class="p">:</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"model"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">),</span>
            <span class="s2">"system_fingerprint"</span><span class="p">:</span> <span class="n">response_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"system_fingerprint"</span><span class="p">,</span> <span class="s2">""</span><span class="p">),</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">openai</span><span class="o">.</span><span class="n">BaseModel</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span> <span class="s2">"choices"</span><span class="p">,</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">message</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="s2">"parsed"</span><span class="p">):</span>
                <span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"parsed"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">parsed</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="s2">"refusal"</span><span class="p">):</span>
                <span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"refusal"</span><span class="p">]</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">refusal</span>

        <span class="k">return</span> <span class="n">ChatResult</span><span class="p">(</span><span class="n">generations</span><span class="o">=</span><span class="n">generations</span><span class="p">,</span> <span class="n">llm_output</span><span class="o">=</span><span class="n">llm_output</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_astream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">default_chunk_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">BaseMessageChunk</span><span class="p">]</span> <span class="o">=</span> <span class="n">AIMessageChunk</span>
        <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span> <span class="ow">and</span> <span class="n">is_basemodel_subclass</span><span class="p">(</span>
            <span class="n">payload</span><span class="p">[</span><span class="s2">"response_format"</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="c1"># TODO: Add support for streaming with Pydantic response_format.</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">"Streaming with Pydantic response_format not yet supported."</span><span class="p">)</span>
            <span class="n">chat_result</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agenerate</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="n">chat_result</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>
            <span class="k">yield</span> <span class="n">ChatGenerationChunk</span><span class="p">(</span>
                <span class="n">message</span><span class="o">=</span><span class="n">AIMessageChunk</span><span class="p">(</span>
                    <span class="o">**</span><span class="n">msg</span><span class="o">.</span><span class="n">dict</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">{</span><span class="s2">"type"</span><span class="p">,</span> <span class="s2">"additional_kwargs"</span><span class="p">}),</span>
                    <span class="c1"># preserve the "parsed" Pydantic object without converting to dict</span>
                    <span class="n">additional_kwargs</span><span class="o">=</span><span class="n">msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="n">generation_info</span><span class="o">=</span><span class="n">chat_result</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">generation_info</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">base_generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">response</span><span class="p">:</span>
            <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">chunk</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
                <span class="n">generation_chunk</span> <span class="o">=</span> <span class="n">_convert_chunk_to_generation_chunk</span><span class="p">(</span>
                    <span class="n">chunk</span><span class="p">,</span>
                    <span class="n">default_chunk_class</span><span class="p">,</span>
                    <span class="n">base_generation_info</span> <span class="k">if</span> <span class="n">is_first_chunk</span> <span class="k">else</span> <span class="p">{},</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">generation_chunk</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">default_chunk_class</span> <span class="o">=</span> <span class="n">generation_chunk</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="vm">__class__</span>
                <span class="n">logprobs</span> <span class="o">=</span> <span class="p">(</span><span class="n">generation_chunk</span><span class="o">.</span><span class="n">generation_info</span> <span class="ow">or</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"logprobs"</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">run_manager</span><span class="p">:</span>
                    <span class="k">await</span> <span class="n">run_manager</span><span class="o">.</span><span class="n">on_llm_new_token</span><span class="p">(</span>
                        <span class="n">generation_chunk</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="n">generation_chunk</span><span class="p">,</span> <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span>
                    <span class="p">)</span>
                <span class="n">is_first_chunk</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">yield</span> <span class="n">generation_chunk</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_agenerate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span>
        <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">run_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncCallbackManagerForLLMRun</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ChatResult</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">streaming</span><span class="p">:</span>
            <span class="n">stream_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_astream</span><span class="p">(</span>
                <span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="n">run_manager</span><span class="o">=</span><span class="n">run_manager</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="k">await</span> <span class="n">agenerate_from_stream</span><span class="p">(</span><span class="n">stream_iter</span><span class="p">)</span>
        <span class="n">payload</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_request_payload</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">generation_info</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s2">"response_format"</span> <span class="ow">in</span> <span class="n">payload</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">"Cannot currently include response headers when response_format is "</span>
                    <span class="s2">"specified."</span>
                <span class="p">)</span>
            <span class="n">payload</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"stream"</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">root_async_client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
                <span class="o">**</span><span class="n">payload</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_response_headers</span><span class="p">:</span>
            <span class="n">raw_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">with_raw_response</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">generation_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"headers"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">raw_response</span><span class="o">.</span><span class="n">headers</span><span class="p">)}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_client</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">**</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">return</span> <span class="k">await</span> <span class="n">run_in_executor</span><span class="p">(</span>
            <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_chat_result</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">generation_info</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_identifying_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the identifying parameters."""</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"model_name"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_params</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">_get_invocation_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the parameters used to invoke the model."""</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">"model"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="o">**</span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_get_invocation_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">),</span>
            <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_params</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_get_ls_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">stop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LangSmithParams</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Get standard params for tracing."""</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_invocation_params</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">ls_params</span> <span class="o">=</span> <span class="n">LangSmithParams</span><span class="p">(</span>
            <span class="n">ls_provider</span><span class="o">=</span><span class="s2">"openai"</span><span class="p">,</span>
            <span class="n">ls_model_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">ls_model_type</span><span class="o">=</span><span class="s2">"chat"</span><span class="p">,</span>
            <span class="n">ls_temperature</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"temperature"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">ls_max_tokens</span> <span class="o">:=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"max_tokens"</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_tokens</span><span class="p">):</span>
            <span class="n">ls_params</span><span class="p">[</span><span class="s2">"ls_max_tokens"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ls_max_tokens</span>
        <span class="k">if</span> <span class="n">ls_stop</span> <span class="o">:=</span> <span class="n">stop</span> <span class="ow">or</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stop"</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ls_params</span><span class="p">[</span><span class="s2">"ls_stop"</span><span class="p">]</span> <span class="o">=</span> <span class="n">ls_stop</span>
        <span class="k">return</span> <span class="n">ls_params</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_llm_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Return type of chat model."""</span>
        <span class="k">return</span> <span class="s2">"openai-chat"</span>

    <span class="k">def</span> <span class="nf">_get_encoding_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">Encoding</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tiktoken_model_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tiktoken_model_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="s2">"cl100k_base"</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">encoding</span>

<div class="viewcode-block" id="BaseChatOpenAI.get_token_ids"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_together.chat_models.BaseChatOpenAI.get_token_ids">[docs]</a>    <span class="k">def</span> <span class="nf">get_token_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the tokens present in the text with tiktoken package."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_get_token_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">custom_get_token_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="c1"># tiktoken NOT supported for Python 3.7 or below</span>
        <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">7</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_token_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">encoding_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_encoding_model</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">encoding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span></div>

    <span class="c1"># TODO: Count bound tools as part of input.</span>
<div class="viewcode-block" id="BaseChatOpenAI.get_num_tokens_from_messages"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_together.chat_models.BaseChatOpenAI.get_num_tokens_from_messages">[docs]</a>    <span class="k">def</span> <span class="nf">get_num_tokens_from_messages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.</span>

<span class="sd">        **Requirements**: You must have the ``pillow`` installed if you want to count</span>
<span class="sd">        image tokens if you are specifying the image as a base64 string, and you must</span>
<span class="sd">        have both ``pillow`` and ``httpx`` installed if you are specifying the image</span>
<span class="sd">        as a URL. If these aren't installed image inputs will be ignored in token</span>
<span class="sd">        counting.</span>

<span class="sd">        OpenAI reference: https://github.com/openai/openai-cookbook/blob/</span>
<span class="sd">        main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb"""</span>
        <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">7</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_num_tokens_from_messages</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_encoding_model</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-3.5-turbo-0301"</span><span class="p">):</span>
            <span class="c1"># every message follows &lt;im_start&gt;{role/name}\n{content}&lt;im_end&gt;\n</span>
            <span class="n">tokens_per_message</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="c1"># if there's a name, the role is omitted</span>
            <span class="n">tokens_per_name</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-3.5-turbo"</span><span class="p">)</span> <span class="ow">or</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"gpt-4"</span><span class="p">):</span>
            <span class="n">tokens_per_message</span> <span class="o">=</span> <span class="mi">3</span>
            <span class="n">tokens_per_name</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"get_num_tokens_from_messages() is not presently implemented "</span>
                <span class="sa">f</span><span class="s2">"for model </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">. See "</span>
                <span class="s2">"https://platform.openai.com/docs/guides/text-generation/managing-tokens"</span>  <span class="c1"># noqa: E501</span>
                <span class="s2">" for information on how messages are converted to tokens."</span>
            <span class="p">)</span>
        <span class="n">num_tokens</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">messages_dict</span> <span class="o">=</span> <span class="p">[</span><span class="n">_convert_message_to_dict</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages_dict</span><span class="p">:</span>
            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="n">tokens_per_message</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># This is an inferred approximation. OpenAI does not document how to</span>
                <span class="c1"># count tool message tokens.</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">"tool_call_id"</span><span class="p">:</span>
                    <span class="n">num_tokens</span> <span class="o">+=</span> <span class="mi">3</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="c1"># content or tool calls</span>
                    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">value</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">val</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"text"</span><span class="p">:</span>
                            <span class="n">text</span> <span class="o">=</span> <span class="n">val</span><span class="p">[</span><span class="s2">"text"</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">val</span>
                            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
                        <span class="k">elif</span> <span class="n">val</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"image_url"</span><span class="p">:</span>
                            <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"detail"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"low"</span><span class="p">:</span>
                                <span class="n">num_tokens</span> <span class="o">+=</span> <span class="mi">85</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">image_size</span> <span class="o">=</span> <span class="n">_url_to_size</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="s2">"image_url"</span><span class="p">][</span><span class="s2">"url"</span><span class="p">])</span>
                                <span class="k">if</span> <span class="ow">not</span> <span class="n">image_size</span><span class="p">:</span>
                                    <span class="k">continue</span>
                                <span class="n">num_tokens</span> <span class="o">+=</span> <span class="n">_count_image_tokens</span><span class="p">(</span><span class="o">*</span><span class="n">image_size</span><span class="p">)</span>
                        <span class="c1"># Tool/function call token counting is not documented by OpenAI.</span>
                        <span class="c1"># This is an approximation.</span>
                        <span class="k">elif</span> <span class="n">val</span><span class="p">[</span><span class="s2">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"function"</span><span class="p">:</span>
                            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span>
                                <span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"arguments"</span><span class="p">])</span>
                            <span class="p">)</span>
                            <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">val</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]))</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">"Unrecognized content block type</span><span class="se">\n\n</span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">"</span>
                            <span class="p">)</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">value</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Cast str(value) in case the message value is not a string</span>
                    <span class="c1"># This occurs with function messages</span>
                    <span class="n">num_tokens</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">"name"</span><span class="p">:</span>
                    <span class="n">num_tokens</span> <span class="o">+=</span> <span class="n">tokens_per_name</span>
        <span class="c1"># every reply is primed with &lt;im_start&gt;assistant</span>
        <span class="n">num_tokens</span> <span class="o">+=</span> <span class="mi">3</span>
        <span class="k">return</span> <span class="n">num_tokens</span></div>

<div class="viewcode-block" id="BaseChatOpenAI.bind_functions"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_together.chat_models.BaseChatOpenAI.bind_functions">[docs]</a>    <span class="k">def</span> <span class="nf">bind_functions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">functions</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Type</span><span class="p">[</span><span class="n">BaseModel</span><span class="p">],</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">BaseTool</span><span class="p">]],</span>
        <span class="n">function_call</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">_FunctionCall</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"auto"</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">BaseMessage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Bind functions (and other objects) to this chat model.</span>

<span class="sd">        Assumes model is compatible with OpenAI function-calling API.</span>

<span class="sd">        NOTE: Using bind_tools is recommended instead, as the `functions` and</span>
<span class="sd">            `function_call` request parameters are officially marked as deprecated by</span>
<span class="sd">            OpenAI.</span>

<span class="sd">        Args:</span>
<span class="sd">            functions: A list of function definitions to bind to this chat model.</span>
<span class="sd">                Can be  a dictionary, pydantic model, or callable. Pydantic</span>
<span class="sd">                models and callables will be automatically converted to</span>
<span class="sd">                their schema dictionary representation.</span>
<span class="sd">            function_call: Which function to require the model to call.</span>
<span class="sd">                Must be the name of the single provided function or</span>
<span class="sd">                "auto" to automatically determine which function to call</span>
<span class="sd">                (if any).</span>
<span class="sd">            **kwargs: Any additional parameters to pass to the</span>
<span class="sd">                :class:`~langchain.runnable.Runnable` constructor.</span>
<span class="sd">        """</span>

        <span class="n">formatted_functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">convert_to_openai_function</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">functions</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">function_call</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">function_call</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">{</span><span class="s2">"name"</span><span class="p">:</span> <span class="n">function_call</span><span class="p">}</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">function_call</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">function_call</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"auto"</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">function_call</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">function_call</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">formatted_functions</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">"When specifying `function_call`, you must provide exactly one "</span>
                    <span class="s2">"function."</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">function_call</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">formatted_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]</span> <span class="o">!=</span> <span class="n">function_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Function call </span><span class="si">{</span><span class="n">function_call</span><span class="si">}</span><span class="s2"> was specified, but the only "</span>
                    <span class="sa">f</span><span class="s2">"provided function was </span><span class="si">{</span><span class="n">formatted_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'name'</span><span class="p">]</span><span class="si">}</span><span class="s2">."</span>
                <span class="p">)</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">kwargs</span><span class="p">,</span> <span class="s2">"function_call"</span><span class="p">:</span> <span class="n">function_call</span><span class="p">}</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">functions</span><span class="o">=</span><span class="n">formatted_functions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseChatOpenAI.bind_tools"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_together.chat_models.BaseChatOpenAI.bind_tools">[docs]</a>    <span class="k">def</span> <span class="nf">bind_tools</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">BaseTool</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tool_choice</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">"auto"</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">,</span> <span class="s2">"required"</span><span class="p">,</span> <span class="s2">"any"</span><span class="p">],</span> <span class="nb">bool</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">BaseMessage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Bind tool-like objects to this chat model.</span>

<span class="sd">        Assumes model is compatible with OpenAI tool-calling API.</span>

<span class="sd">        .. versionchanged:: 0.1.21</span>

<span class="sd">            Support for ``strict`` argument added.</span>

<span class="sd">        Args:</span>
<span class="sd">            tools: A list of tool definitions to bind to this chat model.</span>
<span class="sd">                Supports any tool definition handled by</span>
<span class="sd">                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.</span>
<span class="sd">            tool_choice: Which tool to require the model to call.</span>
<span class="sd">                Options are:</span>
<span class="sd">                    - str of the form ``"&lt;&lt;tool_name&gt;&gt;"``: calls &lt;&lt;tool_name&gt;&gt; tool.</span>
<span class="sd">                    - ``"auto"``: automatically selects a tool (including no tool).</span>
<span class="sd">                    - ``"none"``: does not call a tool.</span>
<span class="sd">                    - ``"any"`` or ``"required"`` or ``True``: force at least one tool to be called.</span>
<span class="sd">                    - dict of the form ``{"type": "function", "function": {"name": &lt;&lt;tool_name&gt;&gt;}}``: calls &lt;&lt;tool_name&gt;&gt; tool.</span>
<span class="sd">                    - ``False`` or ``None``: no effect, default OpenAI behavior.</span>
<span class="sd">            strict: If True, model output is guaranteed to exactly match the JSON Schema</span>
<span class="sd">                provided in the tool definition. If True, the input schema will be</span>
<span class="sd">                validated according to</span>
<span class="sd">                https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.</span>
<span class="sd">                If False, input schema will not be validated and model output will not</span>
<span class="sd">                be validated.</span>
<span class="sd">                If None, ``strict`` argument will not be passed to the model.</span>

<span class="sd">                .. versionadded:: 0.1.21</span>

<span class="sd">            kwargs: Any additional parameters are passed directly to</span>
<span class="sd">                ``self.bind(**kwargs)``.</span>
<span class="sd">        """</span>  <span class="c1"># noqa: E501</span>

        <span class="n">formatted_tools</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">tool</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">tool_choice</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="c1"># tool_choice is a tool/function name</span>
                <span class="k">if</span> <span class="n">tool_choice</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"auto"</span><span class="p">,</span> <span class="s2">"none"</span><span class="p">,</span> <span class="s2">"any"</span><span class="p">,</span> <span class="s2">"required"</span><span class="p">):</span>
                    <span class="n">tool_choice</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span>
                        <span class="s2">"function"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"name"</span><span class="p">:</span> <span class="n">tool_choice</span><span class="p">},</span>
                    <span class="p">}</span>
                <span class="c1"># 'any' is not natively supported by OpenAI API.</span>
                <span class="c1"># We support 'any' since other models use this instead of 'required'.</span>
                <span class="k">if</span> <span class="n">tool_choice</span> <span class="o">==</span> <span class="s2">"any"</span><span class="p">:</span>
                    <span class="n">tool_choice</span> <span class="o">=</span> <span class="s2">"required"</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
                <span class="n">tool_choice</span> <span class="o">=</span> <span class="s2">"required"</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool_choice</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">tool_names</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">formatted_tool</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">formatted_tool</span> <span class="ow">in</span> <span class="n">formatted_tools</span>
                <span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="n">tool_name</span> <span class="o">==</span> <span class="n">tool_choice</span><span class="p">[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">tool_name</span> <span class="ow">in</span> <span class="n">tool_names</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">"Tool choice </span><span class="si">{</span><span class="n">tool_choice</span><span class="si">}</span><span class="s2"> was specified, but the only "</span>
                        <span class="sa">f</span><span class="s2">"provided tools were </span><span class="si">{</span><span class="n">tool_names</span><span class="si">}</span><span class="s2">."</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Unrecognized tool_choice type. Expected str, bool or dict. "</span>
                    <span class="sa">f</span><span class="s2">"Received: </span><span class="si">{</span><span class="n">tool_choice</span><span class="si">}</span><span class="s2">"</span>
                <span class="p">)</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"tool_choice"</span><span class="p">]</span> <span class="o">=</span> <span class="n">tool_choice</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">formatted_tools</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseChatOpenAI.with_structured_output"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_together.chat_models.BaseChatOpenAI.with_structured_output">[docs]</a>    <span class="k">def</span> <span class="nf">with_structured_output</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_DictOrPydanticClass</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span>
            <span class="s2">"function_calling"</span><span class="p">,</span> <span class="s2">"json_mode"</span><span class="p">,</span> <span class="s2">"json_schema"</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">"function_calling"</span><span class="p">,</span>
        <span class="n">include_raw</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Runnable</span><span class="p">[</span><span class="n">LanguageModelInput</span><span class="p">,</span> <span class="n">_DictOrPydantic</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Model wrapper that returns outputs formatted to match the given schema.</span>

<span class="sd">        Args:</span>
<span class="sd">            schema:</span>
<span class="sd">                The output schema. Can be passed in as:</span>

<span class="sd">                - an OpenAI function/tool schema,</span>
<span class="sd">                - a JSON Schema,</span>
<span class="sd">                - a TypedDict class (support added in 0.1.20),</span>
<span class="sd">                - or a Pydantic class.</span>

<span class="sd">                If ``schema`` is a Pydantic class then the model output will be a</span>
<span class="sd">                Pydantic instance of that class, and the model-generated fields will be</span>
<span class="sd">                validated by the Pydantic class. Otherwise the model output will be a</span>
<span class="sd">                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`</span>
<span class="sd">                for more on how to properly specify types and descriptions of</span>
<span class="sd">                schema fields when specifying a Pydantic or TypedDict class.</span>

<span class="sd">            method: The method for steering model generation, one of:</span>

<span class="sd">                - "function_calling":</span>
<span class="sd">                    Uses OpenAI's tool-calling (formerly called function calling)</span>
<span class="sd">                    API: https://platform.openai.com/docs/guides/function-calling</span>
<span class="sd">                - "json_schema":</span>
<span class="sd">                    Uses OpenAI's Structured Output API: https://platform.openai.com/docs/guides/structured-outputs</span>
<span class="sd">                    Supported for "gpt-4o-mini", "gpt-4o-2024-08-06", and later</span>
<span class="sd">                    models.</span>
<span class="sd">                - "json_mode":</span>
<span class="sd">                    Uses OpenAI's JSON mode. Note that if using JSON mode then you</span>
<span class="sd">                    must include instructions for formatting the output into the</span>
<span class="sd">                    desired schema into the model call:</span>
<span class="sd">                    https://platform.openai.com/docs/guides/structured-outputs/json-mode</span>

<span class="sd">                Learn more about the differences between the methods and which models</span>
<span class="sd">                support which methods here:</span>

<span class="sd">                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode</span>
<span class="sd">                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format</span>

<span class="sd">            include_raw:</span>
<span class="sd">                If False then only the parsed structured output is returned. If</span>
<span class="sd">                an error occurs during model output parsing it will be raised. If True</span>
<span class="sd">                then both the raw model response (a BaseMessage) and the parsed model</span>
<span class="sd">                response will be returned. If an error occurs during output parsing it</span>
<span class="sd">                will be caught and returned as well. The final output is always a dict</span>
<span class="sd">                with keys "raw", "parsed", and "parsing_error".</span>
<span class="sd">            strict:</span>

<span class="sd">                - True:</span>
<span class="sd">                    Model output is guaranteed to exactly match the schema.</span>
<span class="sd">                    The input schema will also be validated according to</span>
<span class="sd">                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</span>
<span class="sd">                - False:</span>
<span class="sd">                    Input schema will not be validated and model output will not be</span>
<span class="sd">                    validated.</span>
<span class="sd">                - None:</span>
<span class="sd">                    ``strict`` argument will not be passed to the model.</span>

<span class="sd">                If ``method`` is "json_schema" defaults to True. If ``method`` is</span>
<span class="sd">                "function_calling" or "json_mode" defaults to None. Can only be</span>
<span class="sd">                non-null if ``method`` is "function_calling" or "json_schema".</span>

<span class="sd">            kwargs: Additional keyword args aren't supported.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.</span>

<span class="sd">            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.</span>

<span class="sd">            | If ``include_raw`` is True, then Runnable outputs a dict with keys:</span>

<span class="sd">            - "raw": BaseMessage</span>
<span class="sd">            - "parsed": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.</span>
<span class="sd">            - "parsing_error": Optional[BaseException]</span>

<span class="sd">        .. versionchanged:: 0.1.20</span>

<span class="sd">            Added support for TypedDict class ``schema``.</span>

<span class="sd">        .. versionchanged:: 0.1.21</span>

<span class="sd">            Support for ``strict`` argument added.</span>
<span class="sd">            Support for ``method`` = "json_schema" added.</span>

<span class="sd">        .. note:: Planned breaking changes in version `0.2.0`</span>

<span class="sd">            - ``method`` default will be changed to "json_schema" from</span>
<span class="sd">                "function_calling".</span>
<span class="sd">            - ``strict`` will default to True when ``method`` is</span>
<span class="sd">                "function_calling" as of version `0.2.0`.</span>


<span class="sd">        .. dropdown:: Example: schema=Pydantic class, method="function_calling", include_raw=False, strict=True</span>

<span class="sd">            Note, OpenAI has a number of restrictions on what types of schemas can be</span>
<span class="sd">            provided if ``strict`` = True. When using Pydantic, our model cannot</span>
<span class="sd">            specify any Field metadata (like min/max constraints) and fields cannot</span>
<span class="sd">            have default values.</span>

<span class="sd">            See all constraints here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from typing import Optional</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>
<span class="sd">                from langchain_core.pydantic_v1 import BaseModel, Field</span>


<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>

<span class="sd">                    answer: str</span>
<span class="sd">                    justification: Optional[str] = Field(</span>
<span class="sd">                        default=..., description="A justification for the answer."</span>
<span class="sd">                    )</span>


<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(</span>
<span class="sd">                    AnswerWithJustification, strict=True</span>
<span class="sd">                )</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>

<span class="sd">                # -&gt; AnswerWithJustification(</span>
<span class="sd">                #     answer='They weigh the same',</span>
<span class="sd">                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'</span>
<span class="sd">                # )</span>

<span class="sd">        .. dropdown:: Example: schema=Pydantic class, method="function_calling", include_raw=True</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>
<span class="sd">                from langchain_core.pydantic_v1 import BaseModel</span>


<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>

<span class="sd">                    answer: str</span>
<span class="sd">                    justification: str</span>


<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(</span>
<span class="sd">                    AnswerWithJustification, include_raw=True</span>
<span class="sd">                )</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{"answer":"They weigh the same.","justification":"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ."}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),</span>
<span class="sd">                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),</span>
<span class="sd">                #     'parsing_error': None</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=TypedDict class, method="function_calling", include_raw=False</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                # IMPORTANT: If you are using Python &lt;=3.8, you need to import Annotated</span>
<span class="sd">                # from typing_extensions, not from typing.</span>
<span class="sd">                from typing_extensions import Annotated, TypedDict</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>


<span class="sd">                class AnswerWithJustification(TypedDict):</span>
<span class="sd">                    '''An answer to the user question along with justification for the answer.'''</span>

<span class="sd">                    answer: str</span>
<span class="sd">                    justification: Annotated[</span>
<span class="sd">                        Optional[str], None, "A justification for the answer."</span>
<span class="sd">                    ]</span>


<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(AnswerWithJustification)</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'answer': 'They weigh the same',</span>
<span class="sd">                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=OpenAI function schema, method="function_calling", include_raw=False</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>

<span class="sd">                oai_schema = {</span>
<span class="sd">                    'name': 'AnswerWithJustification',</span>
<span class="sd">                    'description': 'An answer to the user question along with justification for the answer.',</span>
<span class="sd">                    'parameters': {</span>
<span class="sd">                        'type': 'object',</span>
<span class="sd">                        'properties': {</span>
<span class="sd">                            'answer': {'type': 'string'},</span>
<span class="sd">                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}</span>
<span class="sd">                        },</span>
<span class="sd">                       'required': ['answer']</span>
<span class="sd">                   }</span>
<span class="sd">               }</span>

<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(oai_schema)</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "What weighs more a pound of bricks or a pound of feathers"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'answer': 'They weigh the same',</span>
<span class="sd">                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=Pydantic class, method="json_mode", include_raw=True</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                from langchain_openai import ChatOpenAI</span>
<span class="sd">                from langchain_core.pydantic_v1 import BaseModel</span>

<span class="sd">                class AnswerWithJustification(BaseModel):</span>
<span class="sd">                    answer: str</span>
<span class="sd">                    justification: str</span>

<span class="sd">                llm = ChatOpenAI(model="gpt-4o", temperature=0)</span>
<span class="sd">                structured_llm = llm.with_structured_output(</span>
<span class="sd">                    AnswerWithJustification,</span>
<span class="sd">                    method="json_mode",</span>
<span class="sd">                    include_raw=True</span>
<span class="sd">                )</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "Answer the following question. "</span>
<span class="sd">                    "Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n"</span>
<span class="sd">                    "What's heavier a pound of bricks or a pound of feathers?"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'raw': AIMessage(content='{\\n    "answer": "They are both the same weight.",\\n    "justification": "Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight." \\n}'),</span>
<span class="sd">                #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),</span>
<span class="sd">                #     'parsing_error': None</span>
<span class="sd">                # }</span>

<span class="sd">        .. dropdown:: Example: schema=None, method="json_mode", include_raw=True</span>

<span class="sd">            .. code-block::</span>

<span class="sd">                structured_llm = llm.with_structured_output(method="json_mode", include_raw=True)</span>

<span class="sd">                structured_llm.invoke(</span>
<span class="sd">                    "Answer the following question. "</span>
<span class="sd">                    "Make sure to return a JSON blob with keys 'answer' and 'justification'.\\n\\n"</span>
<span class="sd">                    "What's heavier a pound of bricks or a pound of feathers?"</span>
<span class="sd">                )</span>
<span class="sd">                # -&gt; {</span>
<span class="sd">                #     'raw': AIMessage(content='{\\n    "answer": "They are both the same weight.",\\n    "justification": "Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight." \\n}'),</span>
<span class="sd">                #     'parsed': {</span>
<span class="sd">                #         'answer': 'They are both the same weight.',</span>
<span class="sd">                #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'</span>
<span class="sd">                #     },</span>
<span class="sd">                #     'parsing_error': None</span>
<span class="sd">                # }</span>
<span class="sd">        """</span>  <span class="c1"># noqa: E501</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Received unsupported arguments </span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"json_mode"</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Argument `strict` is not supported with `method`='json_mode'"</span>
            <span class="p">)</span>
        <span class="n">is_pydantic_schema</span> <span class="o">=</span> <span class="n">_is_pydantic_class</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"function_calling"</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">"schema must be specified when method is not 'json_mode'. "</span>
                    <span class="s2">"Received None."</span>
                <span class="p">)</span>
            <span class="n">tool_name</span> <span class="o">=</span> <span class="n">convert_to_openai_tool</span><span class="p">(</span><span class="n">schema</span><span class="p">)[</span><span class="s2">"function"</span><span class="p">][</span><span class="s2">"name"</span><span class="p">]</span>
            <span class="n">llm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span>
                <span class="p">[</span><span class="n">schema</span><span class="p">],</span>
                <span class="n">tool_choice</span><span class="o">=</span><span class="n">tool_name</span><span class="p">,</span>
                <span class="n">parallel_tool_calls</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">is_pydantic_schema</span><span class="p">:</span>
                <span class="n">output_parser</span><span class="p">:</span> <span class="n">OutputParserLike</span> <span class="o">=</span> <span class="n">PydanticToolsParser</span><span class="p">(</span>
                    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">schema</span><span class="p">],</span>  <span class="c1"># type: ignore[list-item]</span>
                    <span class="n">first_tool_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># type: ignore[list-item]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_parser</span> <span class="o">=</span> <span class="n">JsonOutputKeyToolsParser</span><span class="p">(</span>
                    <span class="n">key_name</span><span class="o">=</span><span class="n">tool_name</span><span class="p">,</span> <span class="n">first_tool_only</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"json_mode"</span><span class="p">:</span>
            <span class="n">llm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">response_format</span><span class="o">=</span><span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_object"</span><span class="p">})</span>
            <span class="n">output_parser</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">schema</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="k">if</span> <span class="n">is_pydantic_schema</span>
                <span class="k">else</span> <span class="n">JsonOutputParser</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">"json_schema"</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">"schema must be specified when method is not 'json_mode'. "</span>
                    <span class="s2">"Received None."</span>
                <span class="p">)</span>
            <span class="n">strict</span> <span class="o">=</span> <span class="n">strict</span> <span class="k">if</span> <span class="n">strict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">True</span>
            <span class="n">response_format</span> <span class="o">=</span> <span class="n">_convert_to_openai_response_format</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>
            <span class="n">llm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">response_format</span><span class="o">=</span><span class="n">response_format</span><span class="p">)</span>
            <span class="n">output_parser</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">cast</span><span class="p">(</span><span class="n">Runnable</span><span class="p">,</span> <span class="n">_oai_structured_outputs_parser</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">is_pydantic_schema</span>
                <span class="k">else</span> <span class="n">JsonOutputParser</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Unrecognized method argument. Expected one of 'function_calling' or "</span>
                <span class="sa">f</span><span class="s2">"'json_mode'. Received: '</span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">'"</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">include_raw</span><span class="p">:</span>
            <span class="n">parser_assign</span> <span class="o">=</span> <span class="n">RunnablePassthrough</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
                <span class="n">parsed</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="s2">"raw"</span><span class="p">)</span> <span class="o">|</span> <span class="n">output_parser</span><span class="p">,</span> <span class="n">parsing_error</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="n">parser_none</span> <span class="o">=</span> <span class="n">RunnablePassthrough</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">parsed</span><span class="o">=</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">parser_with_fallback</span> <span class="o">=</span> <span class="n">parser_assign</span><span class="o">.</span><span class="n">with_fallbacks</span><span class="p">(</span>
                <span class="p">[</span><span class="n">parser_none</span><span class="p">],</span> <span class="n">exception_key</span><span class="o">=</span><span class="s2">"parsing_error"</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">RunnableMap</span><span class="p">(</span><span class="n">raw</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span> <span class="o">|</span> <span class="n">parser_with_fallback</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">output_parser</span></div></div>


<div class="viewcode-block" id="ChatOpenAI"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_together.chat_models.ChatOpenAI">[docs]</a><span class="k">class</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">BaseChatOpenAI</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""OpenAI chat model integration.</span>

<span class="sd">    .. dropdown:: Setup</span>
<span class="sd">        :open:</span>

<span class="sd">        Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.</span>

<span class="sd">        .. code-block:: bash</span>

<span class="sd">            pip install -U langchain-openai</span>
<span class="sd">            export OPENAI_API_KEY="your-api-key"</span>

<span class="sd">    .. dropdown:: Key init args â€” completion params</span>

<span class="sd">        model: str</span>
<span class="sd">            Name of OpenAI model to use.</span>
<span class="sd">        temperature: float</span>
<span class="sd">            Sampling temperature.</span>
<span class="sd">        max_tokens: Optional[int]</span>
<span class="sd">            Max number of tokens to generate.</span>
<span class="sd">        logprobs: Optional[bool]</span>
<span class="sd">            Whether to return logprobs.</span>
<span class="sd">        stream_options: Dict</span>
<span class="sd">            Configure streaming outputs, like whether to return token usage when</span>
<span class="sd">            streaming (``{"include_usage": True}``).</span>

<span class="sd">        See full list of supported init args and their descriptions in the params section.</span>

<span class="sd">    .. dropdown:: Key init args â€” client params</span>

<span class="sd">        timeout: Union[float, Tuple[float, float], Any, None]</span>
<span class="sd">            Timeout for requests.</span>
<span class="sd">        max_retries: int</span>
<span class="sd">            Max number of retries.</span>
<span class="sd">        api_key: Optional[str]</span>
<span class="sd">            OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.</span>
<span class="sd">        base_url: Optional[str]</span>
<span class="sd">            Base URL for API requests. Only specify if using a proxy or service</span>
<span class="sd">            emulator.</span>
<span class="sd">        organization: Optional[str]</span>
<span class="sd">            OpenAI organization ID. If not passed in will be read from env</span>
<span class="sd">            var OPENAI_ORG_ID.</span>

<span class="sd">        See full list of supported init args and their descriptions in the params section.</span>

<span class="sd">    .. dropdown:: Instantiate</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>

<span class="sd">            llm = ChatOpenAI(</span>
<span class="sd">                model="gpt-4o",</span>
<span class="sd">                temperature=0,</span>
<span class="sd">                max_tokens=None,</span>
<span class="sd">                timeout=None,</span>
<span class="sd">                max_retries=2,</span>
<span class="sd">                # api_key="...",</span>
<span class="sd">                # base_url="...",</span>
<span class="sd">                # organization="...",</span>
<span class="sd">                # other params...</span>
<span class="sd">            )</span>

<span class="sd">        **NOTE**: Any param which is not explicitly supported will be passed directly to the</span>
<span class="sd">        ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is</span>
<span class="sd">        invoked. For example:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_openai import ChatOpenAI</span>
<span class="sd">            import openai</span>

<span class="sd">            ChatOpenAI(..., frequency_penalty=0.2).invoke(...)</span>

<span class="sd">            # results in underlying API call of:</span>

<span class="sd">            openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)</span>

<span class="sd">            # which is also equivalent to:</span>

<span class="sd">            ChatOpenAI(...).invoke(..., frequency_penalty=0.2)</span>

<span class="sd">    .. dropdown:: Invoke</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            messages = [</span>
<span class="sd">                (</span>
<span class="sd">                    "system",</span>
<span class="sd">                    "You are a helpful translator. Translate the user sentence to French.",</span>
<span class="sd">                ),</span>
<span class="sd">                ("human", "I love programming."),</span>
<span class="sd">            ]</span>
<span class="sd">            llm.invoke(messages)</span>

<span class="sd">        .. code-block:: pycon</span>

<span class="sd">            AIMessage(</span>
<span class="sd">                content="J'adore la programmation.",</span>
<span class="sd">                response_metadata={</span>
<span class="sd">                    "token_usage": {</span>
<span class="sd">                        "completion_tokens": 5,</span>
<span class="sd">                        "prompt_tokens": 31,</span>
<span class="sd">                        "total_tokens": 36,</span>
<span class="sd">                    },</span>
<span class="sd">                    "model_name": "gpt-4o",</span>
<span class="sd">                    "system_fingerprint": "fp_43dfabdef1",</span>
<span class="sd">                    "finish_reason": "stop",</span>
<span class="sd">                    "logprobs": None,</span>
<span class="sd">                },</span>
<span class="sd">                id="run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0",</span>
<span class="sd">                usage_metadata={"input_tokens": 31, "output_tokens": 5, "total_tokens": 36},</span>
<span class="sd">            )</span>

<span class="sd">    .. dropdown:: Stream</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            for chunk in llm.stream(messages):</span>
<span class="sd">                print(chunk)</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessageChunk(content="", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(content="J", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(content="'adore", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(content=" la", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(</span>
<span class="sd">                content=" programmation", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span>
<span class="sd">            )</span>
<span class="sd">            AIMessageChunk(content=".", id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0")</span>
<span class="sd">            AIMessageChunk(</span>
<span class="sd">                content="",</span>
<span class="sd">                response_metadata={"finish_reason": "stop"},</span>
<span class="sd">                id="run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0",</span>
<span class="sd">            )</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            stream = llm.stream(messages)</span>
<span class="sd">            full = next(stream)</span>
<span class="sd">            for chunk in stream:</span>
<span class="sd">                full += chunk</span>
<span class="sd">            full</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessageChunk(</span>
<span class="sd">                content="J'adore la programmation.",</span>
<span class="sd">                response_metadata={"finish_reason": "stop"},</span>
<span class="sd">                id="run-bf917526-7f58-4683-84f7-36a6b671d140",</span>
<span class="sd">            )</span>

<span class="sd">    .. dropdown:: Async</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            await llm.ainvoke(messages)</span>

<span class="sd">            # stream:</span>
<span class="sd">            # async for chunk in (await llm.astream(messages))</span>

<span class="sd">            # batch:</span>
<span class="sd">            # await llm.abatch([messages])</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            AIMessage(</span>
<span class="sd">                content="J'adore la programmation.",</span>
<span class="sd">                response_metadata={</span>
<span class="sd">                    "token_usage": {</span>
<span class="sd">                        "completion_tokens": 5,</span>
<span class="sd">                        "prompt_tokens": 31,</span>
<span class="sd">                        "total_tokens": 36,</span>
<span class="sd">                    },</span>
<span class="sd">                    "model_name": "gpt-4o",</span>
<span class="sd">                    "system_fingerprint": "fp_43dfabdef1",</span>
<span class="sd">                    "finish_reason": "stop",</span>
<span class="sd">                    "logprobs": None,</span>
<span class="sd">                },</span>
<span class="sd">                id="run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0",</span>
<span class="sd">                usage_metadata={"input_tokens": 31, "output_tokens": 5, "total_tokens": 36},</span>
<span class="sd">            )</span>

<span class="sd">    .. dropdown:: Tool calling</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from langchain_core.pydantic_v1 import BaseModel, Field</span>


<span class="sd">            class GetWeather(BaseModel):</span>
<span class="sd">                '''Get the current weather in a given location'''</span>

<span class="sd">                location: str = Field(</span>
<span class="sd">                    ..., description="The city and state, e.g. San Francisco, CA"</span>
<span class="sd">                )</span>


<span class="sd">            class GetPopulation(BaseModel):</span>
<span class="sd">                '''Get the current population in a given location'''</span>

<span class="sd">                location: str = Field(</span>
<span class="sd">                    ..., description="The city and state, e.g. San Francisco, CA"</span>
<span class="sd">                )</span>


<span class="sd">            llm_with_tools = llm.bind_tools(</span>
<span class="sd">                [GetWeather, GetPopulation]</span>
<span class="sd">                # strict = True  # enforce tool args schema is respected</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg = llm_with_tools.invoke(</span>
<span class="sd">                "Which city is hotter today and which is bigger: LA or NY?"</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg.tool_calls</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            [</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetWeather",</span>
<span class="sd">                    "args": {"location": "Los Angeles, CA"},</span>
<span class="sd">                    "id": "call_6XswGD5Pqk8Tt5atYr7tfenU",</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetWeather",</span>
<span class="sd">                    "args": {"location": "New York, NY"},</span>
<span class="sd">                    "id": "call_ZVL15vA8Y7kXqOy3dtmQgeCi",</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetPopulation",</span>
<span class="sd">                    "args": {"location": "Los Angeles, CA"},</span>
<span class="sd">                    "id": "call_49CFW8zqC9W7mh7hbMLSIrXw",</span>
<span class="sd">                },</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetPopulation",</span>
<span class="sd">                    "args": {"location": "New York, NY"},</span>
<span class="sd">                    "id": "call_6ghfKxV264jEfe1mRIkS3PE7",</span>
<span class="sd">                },</span>
<span class="sd">            ]</span>

<span class="sd">        Note that ``openai &gt;= 1.32`` supports a ``parallel_tool_calls`` parameter</span>
<span class="sd">        that defaults to ``True``. This parameter can be set to ``False`` to</span>
<span class="sd">        disable parallel tool calls:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm_with_tools.invoke(</span>
<span class="sd">                "What is the weather in LA and NY?", parallel_tool_calls=False</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg.tool_calls</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            [</span>
<span class="sd">                {</span>
<span class="sd">                    "name": "GetWeather",</span>
<span class="sd">                    "args": {"location": "Los Angeles, CA"},</span>
<span class="sd">                    "id": "call_4OoY0ZR99iEvC7fevsH8Uhtz",</span>
<span class="sd">                }</span>
<span class="sd">            ]</span>

<span class="sd">        Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model</span>
<span class="sd">        using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by</span>
<span class="sd">        setting ``model_kwargs``.</span>

<span class="sd">        See ``ChatOpenAI.bind_tools()`` method for more.</span>

<span class="sd">    .. dropdown:: Structured output</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            from typing import Optional</span>

<span class="sd">            from langchain_core.pydantic_v1 import BaseModel, Field</span>


<span class="sd">            class Joke(BaseModel):</span>
<span class="sd">                '''Joke to tell user.'''</span>

<span class="sd">                setup: str = Field(description="The setup of the joke")</span>
<span class="sd">                punchline: str = Field(description="The punchline to the joke")</span>
<span class="sd">                rating: Optional[int] = Field(description="How funny the joke is, from 1 to 10")</span>


<span class="sd">            structured_llm = llm.with_structured_output(Joke)</span>
<span class="sd">            structured_llm.invoke("Tell me a joke about cats")</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            Joke(</span>
<span class="sd">                setup="Why was the cat sitting on the computer?",</span>
<span class="sd">                punchline="To keep an eye on the mouse!",</span>
<span class="sd">                rating=None,</span>
<span class="sd">            )</span>

<span class="sd">        See ``ChatOpenAI.with_structured_output()`` for more.</span>

<span class="sd">    .. dropdown:: JSON mode</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            json_llm = llm.bind(response_format={"type": "json_object"})</span>
<span class="sd">            ai_msg = json_llm.invoke(</span>
<span class="sd">                "Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]"</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg.content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            '\\n{\\n  "random_ints": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\n}'</span>

<span class="sd">    .. dropdown:: Image input</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import base64</span>
<span class="sd">            import httpx</span>
<span class="sd">            from langchain_core.messages import HumanMessage</span>

<span class="sd">            image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"</span>
<span class="sd">            image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")</span>
<span class="sd">            message = HumanMessage(</span>
<span class="sd">                content=[</span>
<span class="sd">                    {"type": "text", "text": "describe the weather in this image"},</span>
<span class="sd">                    {</span>
<span class="sd">                        "type": "image_url",</span>
<span class="sd">                        "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},</span>
<span class="sd">                    },</span>
<span class="sd">                ]</span>
<span class="sd">            )</span>
<span class="sd">            ai_msg = llm.invoke([message])</span>
<span class="sd">            ai_msg.content</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            "The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions."</span>

<span class="sd">    .. dropdown:: Token usage</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm.invoke(messages)</span>
<span class="sd">            ai_msg.usage_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {"input_tokens": 28, "output_tokens": 5, "total_tokens": 33}</span>

<span class="sd">        When streaming, set the ``stream_usage`` kwarg:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            stream = llm.stream(messages, stream_usage=True)</span>
<span class="sd">            full = next(stream)</span>
<span class="sd">            for chunk in stream:</span>
<span class="sd">                full += chunk</span>
<span class="sd">            full.usage_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {"input_tokens": 28, "output_tokens": 5, "total_tokens": 33}</span>

<span class="sd">        Alternatively, setting ``stream_usage`` when instantiating the model can be</span>
<span class="sd">        useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using</span>
<span class="sd">        methods like ``.with_structured_output``, which generate chains under the</span>
<span class="sd">        hood.</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            llm = ChatOpenAI(model="gpt-4o", stream_usage=True)</span>
<span class="sd">            structured_llm = llm.with_structured_output(...)</span>

<span class="sd">    .. dropdown:: Logprobs</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            logprobs_llm = llm.bind(logprobs=True)</span>
<span class="sd">            ai_msg = logprobs_llm.invoke(messages)</span>
<span class="sd">            ai_msg.response_metadata["logprobs"]</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {</span>
<span class="sd">                "content": [</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": "J",</span>
<span class="sd">                        "bytes": [74],</span>
<span class="sd">                        "logprob": -4.9617593e-06,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": "'adore",</span>
<span class="sd">                        "bytes": [39, 97, 100, 111, 114, 101],</span>
<span class="sd">                        "logprob": -0.25202933,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": " la",</span>
<span class="sd">                        "bytes": [32, 108, 97],</span>
<span class="sd">                        "logprob": -0.20141791,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": " programmation",</span>
<span class="sd">                        "bytes": [</span>
<span class="sd">                            32,</span>
<span class="sd">                            112,</span>
<span class="sd">                            114,</span>
<span class="sd">                            111,</span>
<span class="sd">                            103,</span>
<span class="sd">                            114,</span>
<span class="sd">                            97,</span>
<span class="sd">                            109,</span>
<span class="sd">                            109,</span>
<span class="sd">                            97,</span>
<span class="sd">                            116,</span>
<span class="sd">                            105,</span>
<span class="sd">                            111,</span>
<span class="sd">                            110,</span>
<span class="sd">                        ],</span>
<span class="sd">                        "logprob": -1.9361265e-07,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                    {</span>
<span class="sd">                        "token": ".",</span>
<span class="sd">                        "bytes": [46],</span>
<span class="sd">                        "logprob": -1.2233183e-05,</span>
<span class="sd">                        "top_logprobs": [],</span>
<span class="sd">                    },</span>
<span class="sd">                ]</span>
<span class="sd">            }</span>

<span class="sd">    .. dropdown:: Response metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            ai_msg = llm.invoke(messages)</span>
<span class="sd">            ai_msg.response_metadata</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            {</span>
<span class="sd">                "token_usage": {</span>
<span class="sd">                    "completion_tokens": 5,</span>
<span class="sd">                    "prompt_tokens": 28,</span>
<span class="sd">                    "total_tokens": 33,</span>
<span class="sd">                },</span>
<span class="sd">                "model_name": "gpt-4o",</span>
<span class="sd">                "system_fingerprint": "fp_319be4768e",</span>
<span class="sd">                "finish_reason": "stop",</span>
<span class="sd">                "logprobs": None,</span>
<span class="sd">            }</span>

<span class="sd">    """</span>  <span class="c1"># noqa: E501</span>

    <span class="n">stream_usage</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">"""Whether to include usage metadata in streaming output. If True, additional</span>
<span class="sd">    message chunks will be generated during the stream including usage metadata.</span>
<span class="sd">    """</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lc_secrets</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"openai_api_key"</span><span class="p">:</span> <span class="s2">"OPENAI_API_KEY"</span><span class="p">}</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_lc_namespace</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Get the namespace of the langchain object."""</span>
        <span class="k">return</span> <span class="p">[</span><span class="s2">"langchain"</span><span class="p">,</span> <span class="s2">"chat_models"</span><span class="p">,</span> <span class="s2">"openai"</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lc_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">attributes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_organization</span><span class="p">:</span>
            <span class="n">attributes</span><span class="p">[</span><span class="s2">"openai_organization"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_organization</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span><span class="p">:</span>
            <span class="n">attributes</span><span class="p">[</span><span class="s2">"openai_api_base"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_api_base</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span><span class="p">:</span>
            <span class="n">attributes</span><span class="p">[</span><span class="s2">"openai_proxy"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">openai_proxy</span>

        <span class="k">return</span> <span class="n">attributes</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">is_lc_serializable</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Return whether this model can be serialized by Langchain."""</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_should_stream_usage</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">stream_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Determine whether to include usage metadata in streaming output.</span>

<span class="sd">        For backwards compatibility, we check for `stream_options` passed</span>
<span class="sd">        explicitly to kwargs or in the model_kwargs and override self.stream_usage.</span>
<span class="sd">        """</span>
        <span class="n">stream_usage_sources</span> <span class="o">=</span> <span class="p">[</span>  <span class="c1"># order of preference</span>
            <span class="n">stream_usage</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stream_options"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"include_usage"</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"stream_options"</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"include_usage"</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stream_usage</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">source</span> <span class="ow">in</span> <span class="n">stream_usage_sources</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">source</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream_usage</span>

    <span class="k">def</span> <span class="nf">_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">stream_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Set default stream_options."""</span>
        <span class="n">stream_usage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_stream_usage</span><span class="p">(</span><span class="n">stream_usage</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># Note: stream_options is not a valid parameter for Azure OpenAI.</span>
        <span class="c1"># To support users proxying Azure through ChatOpenAI, here we only specify</span>
        <span class="c1"># stream_options if include_usage is set to True.</span>
        <span class="c1"># See https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new</span>
        <span class="c1"># for release notes.</span>
        <span class="k">if</span> <span class="n">stream_usage</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream_options"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"include_usage"</span><span class="p">:</span> <span class="n">stream_usage</span><span class="p">}</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_stream</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_astream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">stream_usage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">ChatGenerationChunk</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Set default stream_options."""</span>
        <span class="n">stream_usage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_stream_usage</span><span class="p">(</span><span class="n">stream_usage</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stream_usage</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">"stream_options"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"include_usage"</span><span class="p">:</span> <span class="n">stream_usage</span><span class="p">}</span>

        <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_astream</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">chunk</span></div>


<span class="k">def</span> <span class="nf">_is_pydantic_class</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_basemodel_subclass</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_lc_tool_call_to_openai_tool_call</span><span class="p">(</span><span class="n">tool_call</span><span class="p">:</span> <span class="n">ToolCall</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">],</span>
        <span class="s2">"function"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span> <span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span>
            <span class="s2">"arguments"</span><span class="p">:</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"args"</span><span class="p">]),</span>
        <span class="p">},</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">_lc_invalid_tool_call_to_openai_tool_call</span><span class="p">(</span>
    <span class="n">invalid_tool_call</span><span class="p">:</span> <span class="n">InvalidToolCall</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"function"</span><span class="p">,</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="n">invalid_tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">],</span>
        <span class="s2">"function"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span> <span class="n">invalid_tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span>
            <span class="s2">"arguments"</span><span class="p">:</span> <span class="n">invalid_tool_call</span><span class="p">[</span><span class="s2">"args"</span><span class="p">],</span>
        <span class="p">},</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">_url_to_size</span><span class="p">(</span><span class="n">image_source</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>  <span class="c1"># type: ignore[import]</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">"Unable to count image tokens. To count image tokens please install "</span>
            <span class="s2">"`pip install -U pillow httpx`."</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">_is_url</span><span class="p">(</span><span class="n">image_source</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">httpx</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">"Unable to count image tokens. To count image tokens please install "</span>
                <span class="s2">"`pip install -U httpx`."</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">httpx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">image_source</span><span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">))</span><span class="o">.</span><span class="n">size</span>
        <span class="k">return</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span>
    <span class="k">elif</span> <span class="n">_is_b64</span><span class="p">(</span><span class="n">image_source</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">encoded</span> <span class="o">=</span> <span class="n">image_source</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
        <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">size</span>
        <span class="k">return</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_count_image_tokens</span><span class="p">(</span><span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="c1"># Reference: https://platform.openai.com/docs/guides/vision/calculating-costs</span>
    <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">_resize</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">height</span> <span class="o">/</span> <span class="mi">512</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">width</span> <span class="o">/</span> <span class="mi">512</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">170</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="mi">85</span>


<span class="k">def</span> <span class="nf">_is_url</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">([</span><span class="n">result</span><span class="o">.</span><span class="n">scheme</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">netloc</span><span class="p">])</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Unable to parse URL: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_is_b64</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"data:image"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_resize</span><span class="p">(</span><span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">height</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="c1"># larger side must be &lt;= 2048</span>
    <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="mi">2048</span> <span class="ow">or</span> <span class="n">height</span> <span class="o">&gt;</span> <span class="mi">2048</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="n">height</span><span class="p">:</span>
            <span class="n">height</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">*</span> <span class="mi">2048</span><span class="p">)</span> <span class="o">//</span> <span class="n">width</span>
            <span class="n">width</span> <span class="o">=</span> <span class="mi">2048</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">width</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="mi">2048</span><span class="p">)</span> <span class="o">//</span> <span class="n">height</span>
            <span class="n">height</span> <span class="o">=</span> <span class="mi">2048</span>
    <span class="c1"># smaller side must be &lt;= 768</span>
    <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="mi">768</span> <span class="ow">and</span> <span class="n">height</span> <span class="o">&gt;</span> <span class="mi">768</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="n">height</span><span class="p">:</span>
            <span class="n">width</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="mi">768</span><span class="p">)</span> <span class="o">//</span> <span class="n">height</span>
            <span class="n">height</span> <span class="o">=</span> <span class="mi">768</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">height</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="mi">768</span><span class="p">)</span> <span class="o">//</span> <span class="n">height</span>
            <span class="n">width</span> <span class="o">=</span> <span class="mi">768</span>
    <span class="k">return</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span>


<span class="k">def</span> <span class="nf">_convert_to_openai_response_format</span><span class="p">(</span>
    <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Type</span><span class="p">],</span> <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">,</span> <span class="n">TypeBaseModel</span><span class="p">]:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_basemodel_subclass</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">schema</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">function</span> <span class="o">=</span> <span class="n">convert_to_openai_function</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>
        <span class="n">function</span><span class="p">[</span><span class="s2">"schema"</span><span class="p">]</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">"parameters"</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_schema"</span><span class="p">,</span> <span class="s2">"json_schema"</span><span class="p">:</span> <span class="n">function</span><span class="p">}</span>


<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">_oai_structured_outputs_parser</span><span class="p">(</span><span class="n">ai_msg</span><span class="p">:</span> <span class="n">AIMessage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PydanticBaseModel</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ai_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"parsed"</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ai_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"parsed"</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">ai_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"refusal"</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">OpenAIRefusalError</span><span class="p">(</span><span class="n">ai_msg</span><span class="o">.</span><span class="n">additional_kwargs</span><span class="p">[</span><span class="s2">"refusal"</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Structured Output response does not have a 'parsed' field nor a 'refusal' "</span>
            <span class="s2">"field."</span>
        <span class="p">)</span>


<div class="viewcode-block" id="OpenAIRefusalError"><a class="viewcode-back" href="../../../openai/chat_models/langchain_openai.chat_models.base.OpenAIRefusalError.html#langchain_together.chat_models.OpenAIRefusalError">[docs]</a><span class="k">class</span> <span class="nc">OpenAIRefusalError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Error raised when OpenAI Structured Outputs API returns a refusal.</span>

<span class="sd">    When using OpenAI's Structured Outputs API with user-generated input, the model</span>
<span class="sd">    may occasionally refuse to fulfill the request for safety reasons.</span>

<span class="sd">    See here for more on refusals:</span>
<span class="sd">    https://platform.openai.com/docs/guides/structured-outputs/refusals</span>

<span class="sd">    .. versionadded:: 0.1.21</span>
<span class="sd">    """</span></div>
</pre></div>
</article>
</div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      Â© Copyright 2023, LangChain Inc.
      <br/>
</p>
</div>
</div>
</div>
</footer>
</body>
</html>