
<!DOCTYPE html>

<html data-content_root="../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>ChatOpenAI — 🦜🔗 LangChain  documentation</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../_static/pygments.css?v=a746c00c" rel="stylesheet" type="text/css"/>
<link href="../../_static/autodoc_pydantic.css" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/sphinx-design.min.css?v=95c83b7e" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/custom.css?v=b95e2228" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<link as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" rel="preload"/>
<script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/documentation_options.js?v=3b5cce75"></script>
<script src="../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=f281be69"></script>
<script src="../../_static/design-tabs.js?v=f930bc37"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI';</script>
<link href="../../_static/favicon.png" rel="icon"/>
<link href="../../search.html" rel="search" title="Search"/>
<link href="langchain_openai.chat_models.base.OpenAIRefusalError.html" rel="next" title="OpenAIRefusalError"/>
<link href="langchain_openai.chat_models.base.BaseChatOpenAI.html" rel="prev" title="BaseChatOpenAI"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<meta content="Sep 20, 2024" name="docbuild:last-update"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<div class="skip-link d-print-none" id="pst-skip-link"><a href="#main-content">Skip to main content</a></div>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>Back to top</button>
<input class="sidebar-toggle" id="pst-primary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
<input class="sidebar-toggle" id="pst-secondary-sidebar-checkbox" type="checkbox"/>
<label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<div class="pst-async-banner-revealer d-none">
<aside aria-label="Version warning" class="d-none d-print-none" id="bd-header-version-warning"></aside>
</div>
<header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
<button aria-label="Site navigation" class="pst-navbar-icon sidebar-toggle primary-toggle">
<span class="fa-solid fa-bars"></span>
</button>
<div class="navbar-header-items__start">
<div class="navbar-item">
<a class="navbar-brand logo" href="../../index.html">
<img alt="🦜🔗 LangChain  documentation - Home" class="logo__image only-light" src="../../_static/wordmark-api.svg"/>
<script>document.write(`<img src="../../_static/wordmark-api-dark.svg" class="logo__image only-dark" alt="🦜🔗 LangChain  documentation - Home"/>`);</script>
</a></div>
</div>
<div class="navbar-header-items">
<div class="me-auto navbar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../../reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-external" href="https://api.python.langchain.com/">
    Legacy reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="navbar-header-items__end">
<div class="navbar-item navbar-persistent--container">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="navbar-persistent--mobile">
<form action="../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search" spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
<button aria-label="On this page" class="pst-navbar-icon sidebar-toggle secondary-toggle">
<span class="fa-solid fa-outdent"></span>
</button>
</div>
</header>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
<div class="sidebar-header-items__center">
<div class="navbar-item">
<nav>
<ul class="bd-navbar-elements navbar-nav">
<li class="nav-item current active">
<a class="nav-link nav-internal" href="../../reference.html">
    Reference
  </a>
</li>
<li class="nav-item">
<a class="nav-link nav-external" href="https://api.python.langchain.com/">
    Legacy reference
  </a>
</li>
</ul>
</nav></div>
</div>
<div class="sidebar-header-items__end">
<div class="navbar-item"><!-- This will display a link to LangChain docs -->
<head>
<style>
        .text-link {
            text-decoration: none; /* Remove underline */
            color: inherit;        /* Inherit color from parent element */
        }
    </style>
</head>
<body>
<a class="text-link" href="https://python.langchain.com/">Docs</a>
</body></div>
<div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
<div class="navbar-item"><ul aria-label="Quick Links" class="navbar-icon-links">
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://github.com/langchain-ai/langchain" rel="noopener" target="_blank" title="GitHub"><i aria-hidden="true" class="fa-brands fa-square-github fa-lg"></i>
<span class="sr-only">GitHub</span></a>
</li>
<li class="nav-item">
<a class="nav-link pst-navbar-icon" data-bs-placement="bottom" data-bs-toggle="tooltip" href="https://twitter.com/langchainai" rel="noopener" target="_blank" title="X / Twitter"><i aria-hidden="true" class="fab fa-twitter-square fa-lg"></i>
<span class="sr-only">X / Twitter</span></a>
</li>
</ul></div>
</div>
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<nav aria-label="Section Navigation" class="bd-docs-nav bd-links">
<p aria-level="1" class="bd-links__title" role="heading">Section Navigation</p>
<div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Base packages</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../langchain/index.html">Langchain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../text_splitters/index.html">Text Splitters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/index.html">Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../experimental/index.html">Experimental</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Integrations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ai21/index.html">AI21</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../airbyte/index.html">Airbyte</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../anthropic/index.html">Anthropic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../aws/index.html">AWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../azure_dynamic_sessions/index.html">Azure Dynamic Sessions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../box/index.html">Box</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chroma/index.html">Chroma</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cohere/index.html">Cohere</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../couchbase/index.html">Couchbase</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../elasticsearch/index.html">Elasticsearch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../exa/index.html">Exa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fireworks/index.html">Fireworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../google_community/index.html">Google Community</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../google_genai/index.html">Google GenAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../google_vertexai/index.html">Google VertexAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../groq/index.html">Groq</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../huggingface/index.html">Huggingface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../milvus/index.html">Milvus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mistralai/index.html">MistralAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mongodb/index.html">MongoDB</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nomic/index.html">Nomic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ollama/index.html">Ollama</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">OpenAI</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../chat_models.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">chat_models</span></code></a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="langchain_openai.chat_models.azure.AzureChatOpenAI.html">AzureChatOpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="langchain_openai.chat_models.base.BaseChatOpenAI.html">BaseChatOpenAI</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">ChatOpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="langchain_openai.chat_models.base.OpenAIRefusalError.html">OpenAIRefusalError</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../embeddings.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">embeddings</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../llms.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">llms</span></code></a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../pinecone/index.html">Pinecone</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../postgres/index.html">Postgres</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prompty/index.html">Prompty</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../qdrant/index.html">Qdrant</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../together/index.html">Together</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../unstructured/index.html">Unstructured</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../voyageai/index.html">VoyageAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../weaviate/index.html">Weaviate</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content" role="main">
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
<ul class="bd-breadcrumbs">
<li class="breadcrumb-item breadcrumb-home">
<a aria-label="Home" class="nav-link" href="../../index.html">
<i class="fa-solid fa-home"></i>
</a>
</li>
<li class="breadcrumb-item"><a class="nav-link" href="../../reference.html">LangChain Python API Reference</a></li>
<li class="breadcrumb-item"><i class="fa-solid fa-ellipsis"></i></li>
<li class="breadcrumb-item"><a class="nav-link" href="../chat_models.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">chat_models</span></code></a></li>
<li aria-current="page" class="breadcrumb-item active">ChatOpenAI</li>
</ul>
</nav>
</div>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article">
<section id="chatopenai">
<h1>ChatOpenAI<a class="headerlink" href="#chatopenai" title="Link to this heading">#</a></h1>
<dl class="py class pydantic_model">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">langchain_openai.chat_models.base.</span></span><span class="sig-name descname"><span class="pre">ChatOpenAI</span></span><a class="reference internal" href="../../_modules/langchain_openai/chat_models/base.html#ChatOpenAI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_openai.chat_models.base.BaseChatOpenAI" title="langchain_openai.chat_models.base.BaseChatOpenAI"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseChatOpenAI</span></code></a></p>
<p>OpenAI chat model integration.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" open="open">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Setup</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Install <code class="docutils literal notranslate"><span class="pre">langchain-openai</span></code> and set environment variable <code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>langchain-openai
<span class="nb">export</span><span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">"your-api-key"</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Key init args — completion params</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<dl class="simple">
<dt>model: str</dt><dd><p class="sd-card-text">Name of OpenAI model to use.</p>
</dd>
<dt>temperature: float</dt><dd><p class="sd-card-text">Sampling temperature.</p>
</dd>
<dt>max_tokens: Optional[int]</dt><dd><p class="sd-card-text">Max number of tokens to generate.</p>
</dd>
<dt>logprobs: Optional[bool]</dt><dd><p class="sd-card-text">Whether to return logprobs.</p>
</dd>
<dt>stream_options: Dict</dt><dd><p class="sd-card-text">Configure streaming outputs, like whether to return token usage when
streaming (<code class="docutils literal notranslate"><span class="pre">{"include_usage":</span> <span class="pre">True}</span></code>).</p>
</dd>
</dl>
<p class="sd-card-text">See full list of supported init args and their descriptions in the params section.</p>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Key init args — client params</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<dl class="simple">
<dt>timeout: Union[float, Tuple[float, float], Any, None]</dt><dd><p class="sd-card-text">Timeout for requests.</p>
</dd>
<dt>max_retries: int</dt><dd><p class="sd-card-text">Max number of retries.</p>
</dd>
<dt>api_key: Optional[str]</dt><dd><p class="sd-card-text">OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.</p>
</dd>
<dt>base_url: Optional[str]</dt><dd><p class="sd-card-text">Base URL for API requests. Only specify if using a proxy or service
emulator.</p>
</dd>
<dt>organization: Optional[str]</dt><dd><p class="sd-card-text">OpenAI organization ID. If not passed in will be read from env
var OPENAI_ORG_ID.</p>
</dd>
</dl>
<p class="sd-card-text">See full list of supported init args and their descriptions in the params section.</p>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Instantiate</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_retries</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="c1"># api_key="...",</span>
    <span class="c1"># base_url="...",</span>
    <span class="c1"># organization="...",</span>
    <span class="c1"># other params...</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text"><strong>NOTE</strong>: Any param which is not explicitly supported will be passed directly to the
<code class="docutils literal notranslate"><span class="pre">openai.OpenAI.chat.completions.create(...)</span></code> API every time to the model is
invoked. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">import</span> <span class="nn">openai</span>

<span class="n">ChatOpenAI</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">frequency_penalty</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="c1"># results in underlying API call of:</span>

<span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="o">..</span><span class="p">)</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">frequency_penalty</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># which is also equivalent to:</span>

<span class="n">ChatOpenAI</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">frequency_penalty</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Invoke</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span>
        <span class="s2">"system"</span><span class="p">,</span>
        <span class="s2">"You are a helpful translator. Translate the user sentence to French."</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="p">(</span><span class="s2">"human"</span><span class="p">,</span> <span class="s2">"I love programming."</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="go">AIMessage(</span>
<span class="go">    content="J'adore la programmation.",</span>
<span class="go">    response_metadata={</span>
<span class="go">        "token_usage": {</span>
<span class="go">            "completion_tokens": 5,</span>
<span class="go">            "prompt_tokens": 31,</span>
<span class="go">            "total_tokens": 36,</span>
<span class="go">        },</span>
<span class="go">        "model_name": "gpt-4o",</span>
<span class="go">        "system_fingerprint": "fp_43dfabdef1",</span>
<span class="go">        "finish_reason": "stop",</span>
<span class="go">        "logprobs": None,</span>
<span class="go">    },</span>
<span class="go">    id="run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0",</span>
<span class="go">    usage_metadata={"input_tokens": 31, "output_tokens": 5, "total_tokens": 36},</span>
<span class="go">)</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Stream</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AIMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span><span class="p">)</span>
<span class="n">AIMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"J"</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span><span class="p">)</span>
<span class="n">AIMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"'adore"</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span><span class="p">)</span>
<span class="n">AIMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">" la"</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span><span class="p">)</span>
<span class="n">AIMessageChunk</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="s2">" programmation"</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span>
<span class="p">)</span>
<span class="n">AIMessageChunk</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"."</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span><span class="p">)</span>
<span class="n">AIMessageChunk</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
    <span class="n">response_metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">"finish_reason"</span><span class="p">:</span> <span class="s2">"stop"</span><span class="p">},</span>
    <span class="nb">id</span><span class="o">=</span><span class="s2">"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stream</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">full</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="n">full</span> <span class="o">+=</span> <span class="n">chunk</span>
<span class="n">full</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AIMessageChunk</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="s2">"J'adore la programmation."</span><span class="p">,</span>
    <span class="n">response_metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">"finish_reason"</span><span class="p">:</span> <span class="s2">"stop"</span><span class="p">},</span>
    <span class="nb">id</span><span class="o">=</span><span class="s2">"run-bf917526-7f58-4683-84f7-36a6b671d140"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Async</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">await</span> <span class="n">llm</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>

<span class="c1"># stream:</span>
<span class="c1"># async for chunk in (await llm.astream(messages))</span>

<span class="c1"># batch:</span>
<span class="c1"># await llm.abatch([messages])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AIMessage</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="s2">"J'adore la programmation."</span><span class="p">,</span>
    <span class="n">response_metadata</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"token_usage"</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">"completion_tokens"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s2">"prompt_tokens"</span><span class="p">:</span> <span class="mi">31</span><span class="p">,</span>
            <span class="s2">"total_tokens"</span><span class="p">:</span> <span class="mi">36</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="s2">"model_name"</span><span class="p">:</span> <span class="s2">"gpt-4o"</span><span class="p">,</span>
        <span class="s2">"system_fingerprint"</span><span class="p">:</span> <span class="s2">"fp_43dfabdef1"</span><span class="p">,</span>
        <span class="s2">"finish_reason"</span><span class="p">:</span> <span class="s2">"stop"</span><span class="p">,</span>
        <span class="s2">"logprobs"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="nb">id</span><span class="o">=</span><span class="s2">"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0"</span><span class="p">,</span>
    <span class="n">usage_metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">"input_tokens"</span><span class="p">:</span> <span class="mi">31</span><span class="p">,</span> <span class="s2">"output_tokens"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">"total_tokens"</span><span class="p">:</span> <span class="mi">36</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Tool calling</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>


<span class="k">class</span> <span class="nc">GetWeather</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''Get the current weather in a given location'''</span>

    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"The city and state, e.g. San Francisco, CA"</span>
    <span class="p">)</span>


<span class="k">class</span> <span class="nc">GetPopulation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''Get the current population in a given location'''</span>

    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"The city and state, e.g. San Francisco, CA"</span>
    <span class="p">)</span>


<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span>
    <span class="p">[</span><span class="n">GetWeather</span><span class="p">,</span> <span class="n">GetPopulation</span><span class="p">]</span>
    <span class="c1"># strict = True  # enforce tool args schema is respected</span>
<span class="p">)</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"Which city is hotter today and which is bigger: LA or NY?"</span>
<span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">tool_calls</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"GetWeather"</span><span class="p">,</span>
        <span class="s2">"args"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"location"</span><span class="p">:</span> <span class="s2">"Los Angeles, CA"</span><span class="p">},</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="s2">"call_6XswGD5Pqk8Tt5atYr7tfenU"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"GetWeather"</span><span class="p">,</span>
        <span class="s2">"args"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"location"</span><span class="p">:</span> <span class="s2">"New York, NY"</span><span class="p">},</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="s2">"call_ZVL15vA8Y7kXqOy3dtmQgeCi"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"GetPopulation"</span><span class="p">,</span>
        <span class="s2">"args"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"location"</span><span class="p">:</span> <span class="s2">"Los Angeles, CA"</span><span class="p">},</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="s2">"call_49CFW8zqC9W7mh7hbMLSIrXw"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"GetPopulation"</span><span class="p">,</span>
        <span class="s2">"args"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"location"</span><span class="p">:</span> <span class="s2">"New York, NY"</span><span class="p">},</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="s2">"call_6ghfKxV264jEfe1mRIkS3PE7"</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text">Note that <code class="docutils literal notranslate"><span class="pre">openai</span> <span class="pre">&gt;=</span> <span class="pre">1.32</span></code> supports a <code class="docutils literal notranslate"><span class="pre">parallel_tool_calls</span></code> parameter
that defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>. This parameter can be set to <code class="docutils literal notranslate"><span class="pre">False</span></code> to
disable parallel tool calls:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"What is the weather in LA and NY?"</span><span class="p">,</span> <span class="n">parallel_tool_calls</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">tool_calls</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"GetWeather"</span><span class="p">,</span>
        <span class="s2">"args"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"location"</span><span class="p">:</span> <span class="s2">"Los Angeles, CA"</span><span class="p">},</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="s2">"call_4OoY0ZR99iEvC7fevsH8Uhtz"</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p class="sd-card-text">Like other runtime parameters, <code class="docutils literal notranslate"><span class="pre">parallel_tool_calls</span></code> can be bound to a model
using <code class="docutils literal notranslate"><span class="pre">llm.bind(parallel_tool_calls=False)</span></code> or during instantiation by
setting <code class="docutils literal notranslate"><span class="pre">model_kwargs</span></code>.</p>
<p class="sd-card-text">See <code class="docutils literal notranslate"><span class="pre">ChatOpenAI.bind_tools()</span></code> method for more.</p>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Structured output</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>


<span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''Joke to tell user.'''</span>

    <span class="n">setup</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The setup of the joke"</span><span class="p">)</span>
    <span class="n">punchline</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The punchline to the joke"</span><span class="p">)</span>
    <span class="n">rating</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"How funny the joke is, from 1 to 10"</span><span class="p">)</span>


<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Joke</span><span class="p">)</span>
<span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"Tell me a joke about cats"</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Joke</span><span class="p">(</span>
    <span class="n">setup</span><span class="o">=</span><span class="s2">"Why was the cat sitting on the computer?"</span><span class="p">,</span>
    <span class="n">punchline</span><span class="o">=</span><span class="s2">"To keep an eye on the mouse!"</span><span class="p">,</span>
    <span class="n">rating</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="sd-card-text">See <code class="docutils literal notranslate"><span class="pre">ChatOpenAI.with_structured_output()</span></code> for more.</p>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">JSON mode</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">json_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">response_format</span><span class="o">=</span><span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"json_object"</span><span class="p">})</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">json_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]"</span>
<span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s1">'</span><span class="se">\n</span><span class="s1">{</span><span class="se">\n</span><span class="s1">  "random_ints": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]</span><span class="se">\n</span><span class="s1">}'</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Image input</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">httpx</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"</span>
<span class="n">image_data</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">httpx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">)</span>
<span class="n">message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"text"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="s2">"describe the weather in this image"</span><span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">"type"</span><span class="p">:</span> <span class="s2">"image_url"</span><span class="p">,</span>
            <span class="s2">"image_url"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"url"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"data:image/jpeg;base64,</span><span class="si">{</span><span class="n">image_data</span><span class="si">}</span><span class="s2">"</span><span class="p">},</span>
        <span class="p">},</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">message</span><span class="p">])</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions."</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Token usage</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">usage_metadata</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">"input_tokens"</span><span class="p">:</span> <span class="mi">28</span><span class="p">,</span> <span class="s2">"output_tokens"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">"total_tokens"</span><span class="p">:</span> <span class="mi">33</span><span class="p">}</span>
</pre></div>
</div>
<p class="sd-card-text">When streaming, set the <code class="docutils literal notranslate"><span class="pre">stream_usage</span></code> kwarg:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stream</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">stream_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">full</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="n">full</span> <span class="o">+=</span> <span class="n">chunk</span>
<span class="n">full</span><span class="o">.</span><span class="n">usage_metadata</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">"input_tokens"</span><span class="p">:</span> <span class="mi">28</span><span class="p">,</span> <span class="s2">"output_tokens"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">"total_tokens"</span><span class="p">:</span> <span class="mi">33</span><span class="p">}</span>
</pre></div>
</div>
<p class="sd-card-text">Alternatively, setting <code class="docutils literal notranslate"><span class="pre">stream_usage</span></code> when instantiating the model can be
useful when incorporating <code class="docutils literal notranslate"><span class="pre">ChatOpenAI</span></code> into LCEL chains– or when using
methods like <code class="docutils literal notranslate"><span class="pre">.with_structured_output</span></code>, which generate chains under the
hood.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span> <span class="n">stream_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Logprobs</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprobs_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">logprobs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ai_msg</span> <span class="o">=</span> <span class="n">logprobs_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">response_metadata</span><span class="p">[</span><span class="s2">"logprobs"</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">"content"</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">"token"</span><span class="p">:</span> <span class="s2">"J"</span><span class="p">,</span>
            <span class="s2">"bytes"</span><span class="p">:</span> <span class="p">[</span><span class="mi">74</span><span class="p">],</span>
            <span class="s2">"logprob"</span><span class="p">:</span> <span class="o">-</span><span class="mf">4.9617593e-06</span><span class="p">,</span>
            <span class="s2">"top_logprobs"</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">"token"</span><span class="p">:</span> <span class="s2">"'adore"</span><span class="p">,</span>
            <span class="s2">"bytes"</span><span class="p">:</span> <span class="p">[</span><span class="mi">39</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="mi">114</span><span class="p">,</span> <span class="mi">101</span><span class="p">],</span>
            <span class="s2">"logprob"</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.25202933</span><span class="p">,</span>
            <span class="s2">"top_logprobs"</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">"token"</span><span class="p">:</span> <span class="s2">" la"</span><span class="p">,</span>
            <span class="s2">"bytes"</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">97</span><span class="p">],</span>
            <span class="s2">"logprob"</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.20141791</span><span class="p">,</span>
            <span class="s2">"top_logprobs"</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">"token"</span><span class="p">:</span> <span class="s2">" programmation"</span><span class="p">,</span>
            <span class="s2">"bytes"</span><span class="p">:</span> <span class="p">[</span>
                <span class="mi">32</span><span class="p">,</span>
                <span class="mi">112</span><span class="p">,</span>
                <span class="mi">114</span><span class="p">,</span>
                <span class="mi">111</span><span class="p">,</span>
                <span class="mi">103</span><span class="p">,</span>
                <span class="mi">114</span><span class="p">,</span>
                <span class="mi">97</span><span class="p">,</span>
                <span class="mi">109</span><span class="p">,</span>
                <span class="mi">109</span><span class="p">,</span>
                <span class="mi">97</span><span class="p">,</span>
                <span class="mi">116</span><span class="p">,</span>
                <span class="mi">105</span><span class="p">,</span>
                <span class="mi">111</span><span class="p">,</span>
                <span class="mi">110</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="s2">"logprob"</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.9361265e-07</span><span class="p">,</span>
            <span class="s2">"top_logprobs"</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">"token"</span><span class="p">:</span> <span class="s2">"."</span><span class="p">,</span>
            <span class="s2">"bytes"</span><span class="p">:</span> <span class="p">[</span><span class="mi">46</span><span class="p">],</span>
            <span class="s2">"logprob"</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.2233183e-05</span><span class="p">,</span>
            <span class="s2">"top_logprobs"</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">},</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Response metadata</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ai_msg</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">ai_msg</span><span class="o">.</span><span class="n">response_metadata</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">"token_usage"</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">"completion_tokens"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s2">"prompt_tokens"</span><span class="p">:</span> <span class="mi">28</span><span class="p">,</span>
        <span class="s2">"total_tokens"</span><span class="p">:</span> <span class="mi">33</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">"model_name"</span><span class="p">:</span> <span class="s2">"gpt-4o"</span><span class="p">,</span>
    <span class="s2">"system_fingerprint"</span><span class="p">:</span> <span class="s2">"fp_319be4768e"</span><span class="p">,</span>
    <span class="s2">"finish_reason"</span><span class="p">:</span> <span class="s2">"stop"</span><span class="p">,</span>
    <span class="s2">"logprobs"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</details><div class="admonition note">
<p class="admonition-title">Note</p>
<p>ChatOpenAI implements the standard <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Runnable</span> <span class="pre">Interface</span></code></a>. 🏃</p>
<p>The <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Runnable</span> <span class="pre">Interface</span></code></a> has additional methods that are available on runnables, such as <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_types" title="langchain_core.runnables.base.Runnable.with_types"><code class="xref py py-meth docutils literal notranslate"><span class="pre">with_types</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_retry" title="langchain_core.runnables.base.Runnable.with_retry"><code class="xref py py-meth docutils literal notranslate"><span class="pre">with_retry</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.assign" title="langchain_core.runnables.base.Runnable.assign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">assign</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.bind" title="langchain_core.runnables.base.Runnable.bind"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bind</span></code></a>, <a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.get_graph" title="langchain_core.runnables.base.Runnable.get_graph"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_graph</span></code></a>, and more.</p>
</div>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.cache">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">cache</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="../../core/caches/langchain_core.caches.BaseCache.html#langchain_core.caches.BaseCache" title="langchain_core.caches.BaseCache"><span class="pre">BaseCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.cache" title="Link to this definition">#</a></dt>
<dd><p>Whether to cache the response.</p>
<ul class="simple">
<li><p>If true, will use the global cache.</p></li>
<li><p>If false, will not use a cache</p></li>
<li><p>If None, will use the global cache if it’s set, otherwise no cache.</p></li>
<li><p>If instance of BaseCache, will use the provided cache.</p></li>
</ul>
<p>Caching is not currently supported for streaming methods of models.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.callback_manager">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">callback_manager</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager" title="langchain_core.callbacks.base.BaseCallbackManager"><span class="pre">BaseCallbackManager</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.callback_manager" title="Link to this definition">#</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.1.7: </span>Use <a class="reference internal" href="#langchain_openai.chat_models.base.ChatOpenAI.callbacks" title="langchain_openai.chat_models.base.ChatOpenAI.callbacks"><code class="xref py py-meth docutils literal notranslate"><span class="pre">callbacks()</span></code></a> instead.</p>
</div>
<p>Callback manager to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.callbacks">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">callbacks</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callbacks</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.callbacks" title="Link to this definition">#</a></dt>
<dd><p>Callbacks to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.custom_get_token_ids">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">custom_get_token_ids</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.custom_get_token_ids" title="Link to this definition">#</a></dt>
<dd><p>Optional encoder to use for counting tokens.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.default_headers">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">default_headers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.default_headers" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.default_query">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">default_query</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">object</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.default_query" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.disable_streaming">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">disable_streaming</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'tool_calling'</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.disable_streaming" title="Link to this definition">#</a></dt>
<dd><p>Whether to disable streaming for this model.</p>
<p>If streaming is bypassed, then <code class="docutils literal notranslate"><span class="pre">stream()/astream()</span></code> will defer to
<code class="docutils literal notranslate"><span class="pre">invoke()/ainvoke()</span></code>.</p>
<ul class="simple">
<li><p>If True, will always bypass streaming case.</p></li>
<li><p>If “tool_calling”, will bypass streaming case only when the model is called
with a <code class="docutils literal notranslate"><span class="pre">tools</span></code> keyword argument.</p></li>
<li><p>If False (default), will always use streaming case if available.</p></li>
</ul>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.extra_body">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">extra_body</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.extra_body" title="Link to this definition">#</a></dt>
<dd><p>Optional additional JSON properties to include in the request parameters when
making requests to OpenAI compatible APIs, such as vLLM.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.frequency_penalty">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">frequency_penalty</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.frequency_penalty" title="Link to this definition">#</a></dt>
<dd><p>Penalizes repeated tokens according to frequency.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.http_async_client">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">http_async_client</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.http_async_client" title="Link to this definition">#</a></dt>
<dd><p>Optional httpx.AsyncClient. Only used for async invocations. Must specify
http_client as well if you’d like a custom client for sync invocations.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.http_client">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">http_client</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.http_client" title="Link to this definition">#</a></dt>
<dd><p>Optional httpx.Client. Only used for sync invocations. Must specify
http_async_client as well if you’d like a custom client for async invocations.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.include_response_headers">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">include_response_headers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.include_response_headers" title="Link to this definition">#</a></dt>
<dd><p>Whether to include response headers in the output message response_metadata.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.logit_bias">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logit_bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.logit_bias" title="Link to this definition">#</a></dt>
<dd><p>Modify the likelihood of specified tokens appearing in the completion.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.logprobs">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logprobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.logprobs" title="Link to this definition">#</a></dt>
<dd><p>Whether to return logprobs.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.max_retries">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_retries</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">2</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.max_retries" title="Link to this definition">#</a></dt>
<dd><p>Maximum number of retries to make when generating.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.max_tokens">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">max_tokens</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.max_tokens" title="Link to this definition">#</a></dt>
<dd><p>Maximum number of tokens to generate.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.metadata">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">metadata</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.metadata" title="Link to this definition">#</a></dt>
<dd><p>Metadata to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.model_kwargs">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><em class="property"> <span class="pre">[Optional]</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.model_kwargs" title="Link to this definition">#</a></dt>
<dd><p>Holds any model parameters valid for <cite>create</cite> call not explicitly specified.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.model_name">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'gpt-3.5-turbo'</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'model')</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.model_name" title="Link to this definition">#</a></dt>
<dd><p>Model name to use.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.n">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.n" title="Link to this definition">#</a></dt>
<dd><p>Number of chat completions to generate for each prompt.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.openai_api_base">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_base</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'base_url')</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_api_base" title="Link to this definition">#</a></dt>
<dd><p>Base URL path for API requests, leave blank if not using a proxy or service
emulator.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.openai_api_key">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_api_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">SecretStr</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">[Optional]</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'api_key')</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_api_key" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.openai_organization">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_organization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'organization')</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_organization" title="Link to this definition">#</a></dt>
<dd><p>Automatically inferred from env var <cite>OPENAI_ORG_ID</cite> if not provided.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.openai_proxy">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">openai_proxy</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">[Optional]</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_proxy" title="Link to this definition">#</a></dt>
<dd></dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.presence_penalty">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">presence_penalty</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.presence_penalty" title="Link to this definition">#</a></dt>
<dd><p>Penalizes repeated tokens.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.rate_limiter">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">rate_limiter</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="../../core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter" title="langchain_core.rate_limiters.BaseRateLimiter"><span class="pre">BaseRateLimiter</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.rate_limiter" title="Link to this definition">#</a></dt>
<dd><p>An optional rate limiter to use for limiting the number of requests.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.request_timeout">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">request_timeout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'timeout')</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.request_timeout" title="Link to this definition">#</a></dt>
<dd><p>Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or
None.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.seed">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">seed</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.seed" title="Link to this definition">#</a></dt>
<dd><p>Seed for generation</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.stop">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stop</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"> <span class="pre">(alias</span> <span class="pre">'stop_sequences')</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.stop" title="Link to this definition">#</a></dt>
<dd><p>Default stop sequences.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.stream_usage">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stream_usage</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.stream_usage" title="Link to this definition">#</a></dt>
<dd><p>Whether to include usage metadata in streaming output. If True, additional
message chunks will be generated during the stream including usage metadata.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.streaming">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">streaming</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.streaming" title="Link to this definition">#</a></dt>
<dd><p>Whether to stream the results or not.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.tags">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">tags</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.tags" title="Link to this definition">#</a></dt>
<dd><p>Tags to add to the run trace.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.temperature">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">temperature</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.7</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.temperature" title="Link to this definition">#</a></dt>
<dd><p>What sampling temperature to use.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.tiktoken_model_name">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">tiktoken_model_name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.tiktoken_model_name" title="Link to this definition">#</a></dt>
<dd><p>The model name to pass to tiktoken when using this class.
Tiktoken is used to count the number of tokens in documents to constrain
them to be under a certain limit. By default, when set to None, this will
be the same as the embedding model name. However, there are some cases
where you may want to use this Embedding class with a model name not
supported by tiktoken. This can include when using Azure embeddings or
when using one of the many model providers that expose an OpenAI-like
API but with different models. In those cases, in order to avoid erroring
when tiktoken is called, you can specify a model name to use here.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.top_logprobs">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">top_logprobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.top_logprobs" title="Link to this definition">#</a></dt>
<dd><p>Number of most likely tokens to return at each token position, each with
an associated log probability. <cite>logprobs</cite> must be set to true
if this parameter is used.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.top_p">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">top_p</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.top_p" title="Link to this definition">#</a></dt>
<dd><p>Total probability mass of tokens to consider at each step.</p>
</dd></dl>
<dl class="py attribute pydantic_field">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.verbose">
<em class="property"><span class="pre">param</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">verbose</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"> <span class="pre">[Optional]</span></em><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.verbose" title="Link to this definition">#</a></dt>
<dd><p>Whether to print out response text.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain_core.callbacks.base.BaseCallbackHandler" title="langchain_core.callbacks.base.BaseCallbackHandler"><span class="pre">BaseCallbackHandler</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager" title="langchain_core.callbacks.base.BaseCallbackManager"><span class="pre">BaseCallbackManager</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.__call__" title="Link to this definition">#</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version langchain-core==0.1.7: </span>Use <a class="reference internal" href="#langchain_openai.chat_models.base.ChatOpenAI.invoke" title="langchain_openai.chat_models.base.ChatOpenAI.invoke"><code class="xref py py-meth docutils literal notranslate"><span class="pre">invoke()</span></code></a> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>messages</strong> (<em>list</em><em>[</em><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a><em>]</em>)</p></li>
<li><p><strong>stop</strong> (<em>list</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>)</p></li>
<li><p><strong>callbacks</strong> (<em>list</em><em>[</em><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain_core.callbacks.base.BaseCallbackHandler" title="langchain_core.callbacks.base.BaseCallbackHandler"><em>BaseCallbackHandler</em></a><em>] </em><em>| </em><a class="reference internal" href="../../core/callbacks/langchain_core.callbacks.base.BaseCallbackManager.html#langchain_core.callbacks.base.BaseCallbackManager" title="langchain_core.callbacks.base.BaseCallbackManager"><em>BaseCallbackManager</em></a><em> | </em><em>None</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.abatch">
<em class="property"><span class="pre">async</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">abatch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.abatch" title="Link to this definition">#</a></dt>
<dd><p>Default implementation runs ainvoke in parallel using asyncio.gather.</p>
<p>The default implementation of batch works well for IO bound runnables.</p>
<p>Subclasses should override this method if they can batch more efficiently;
e.g., if the underlying Runnable uses an API which supports a batch mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>list</em><em>[</em><em>Input</em><em>]</em>) – A list of inputs to the Runnable.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>list</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>) – A config to use when invoking the Runnable.
The config supports standard keys like ‘tags’, ‘metadata’ for tracing
purposes, ‘max_concurrency’ for controlling how much work to do
in parallel, and other keys. Please refer to the RunnableConfig
for more details. Defaults to None.</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>) – Whether to return exceptions instead of raising them.
Defaults to False.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of outputs from the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[<em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.abatch_as_completed">
<em class="property"><span class="pre">async</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">abatch_as_completed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">AsyncIterator</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Exception</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.abatch_as_completed" title="Link to this definition">#</a></dt>
<dd><p>Run ainvoke in parallel on a list of inputs,
yielding results as they complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Sequence</em><em>[</em><em>Input</em><em>]</em>) – A list of inputs to the Runnable.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>) – A config to use when invoking the Runnable.
The config supports standard keys like ‘tags’, ‘metadata’ for tracing
purposes, ‘max_concurrency’ for controlling how much work to do
in parallel, and other keys. Please refer to the RunnableConfig
for more details. Defaults to None. Defaults to None.</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>) – Whether to return exceptions instead of raising them.
Defaults to False.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple of the index of the input and the output from the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>AsyncIterator</em>[tuple[int, <em>Output</em> | Exception]]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.ainvoke">
<em class="property"><span class="pre">async</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ainvoke</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.ainvoke" title="Link to this definition">#</a></dt>
<dd><p>Default implementation of ainvoke, calls invoke from a thread.</p>
<p>The default implementation allows usage of async code even if
the Runnable did not implement a native async version of invoke.</p>
<p>Subclasses should override this method if they can run asynchronously.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>)</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>)</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage">BaseMessage</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.astream">
<em class="property"><span class="pre">async</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">astream</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">AsyncIterator</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk"><span class="pre">BaseMessageChunk</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.astream" title="Link to this definition">#</a></dt>
<dd><p>Default implementation of astream, which calls ainvoke.
Subclasses should override this method if they support streaming output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>) – The config to use for the Runnable. Defaults to None.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>AsyncIterator[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk">BaseMessageChunk</a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.astream_events">
<em class="property"><span class="pre">async</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">astream_events</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">version</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'v1'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'v2'</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_types</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_tags</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_types</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_tags</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">AsyncIterator</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.schema.StandardStreamEvent.html#langchain_core.runnables.schema.StandardStreamEvent" title="langchain_core.runnables.schema.StandardStreamEvent"><span class="pre">StandardStreamEvent</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.schema.CustomStreamEvent.html#langchain_core.runnables.schema.CustomStreamEvent" title="langchain_core.runnables.schema.CustomStreamEvent"><span class="pre">CustomStreamEvent</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.astream_events" title="Link to this definition">#</a></dt>
<dd><p>Generate a stream of events.</p>
<p>Use to create an iterator over StreamEvents that provide real-time information
about the progress of the Runnable, including StreamEvents from intermediate
results.</p>
<p>A StreamEvent is a dictionary with the following schema:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">event</span></code>: <strong>str</strong> - Event names are of the</dt><dd><p>format: on_[runnable_type]_(start|stream|end).</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code>: <strong>str</strong> - The name of the Runnable that generated the event.</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">run_id</span></code>: <strong>str</strong> - randomly generated ID associated with the given execution of</dt><dd><p>the Runnable that emitted the event.
A child Runnable that gets invoked as part of the execution of a
parent Runnable is assigned its own unique ID.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">parent_ids</span></code>: <strong>List[str]</strong> - The IDs of the parent runnables that</dt><dd><p>generated the event. The root Runnable will have an empty list.
The order of the parent IDs is from the root to the immediate parent.
Only available for v2 version of the API. The v1 version of the API
will return an empty list.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">tags</span></code>: <strong>Optional[List[str]]</strong> - The tags of the Runnable that generated</dt><dd><p>the event.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">metadata</span></code>: <strong>Optional[Dict[str, Any]]</strong> - The metadata of the Runnable</dt><dd><p>that generated the event.</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">data</span></code>: <strong>Dict[str, Any]</strong></p></li>
</ul>
<p>Below is a table that illustrates some evens that might be emitted by various
chains. Metadata fields have been omitted from the table for brevity.
Chain definitions have been included after the table.</p>
<p><strong>ATTENTION</strong> This reference table is for the V2 version of the schema.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>event</p></th>
<th class="head"><p>name</p></th>
<th class="head"><p>chunk</p></th>
<th class="head"><p>input</p></th>
<th class="head"><p>output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>on_chat_model_start</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>{“messages”: [[SystemMessage, HumanMessage]]}</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>on_chat_model_stream</p></td>
<td><p>[model name]</p></td>
<td><p>AIMessageChunk(content=”hello”)</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_chat_model_end</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>{“messages”: [[SystemMessage, HumanMessage]]}</p></td>
<td><p>AIMessageChunk(content=”hello world”)</p></td>
</tr>
<tr class="row-odd"><td><p>on_llm_start</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>{‘input’: ‘hello’}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_llm_stream</p></td>
<td><p>[model name]</p></td>
<td><p>‘Hello’</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>on_llm_end</p></td>
<td><p>[model name]</p></td>
<td></td>
<td><p>‘Hello human!’</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_chain_start</p></td>
<td><p>format_docs</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>on_chain_stream</p></td>
<td><p>format_docs</p></td>
<td><p>“hello world!, goodbye world!”</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_chain_end</p></td>
<td><p>format_docs</p></td>
<td></td>
<td><p>[Document(…)]</p></td>
<td><p>“hello world!, goodbye world!”</p></td>
</tr>
<tr class="row-odd"><td><p>on_tool_start</p></td>
<td><p>some_tool</p></td>
<td></td>
<td><p>{“x”: 1, “y”: “2”}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_tool_end</p></td>
<td><p>some_tool</p></td>
<td></td>
<td></td>
<td><p>{“x”: 1, “y”: “2”}</p></td>
</tr>
<tr class="row-odd"><td><p>on_retriever_start</p></td>
<td><p>[retriever name]</p></td>
<td></td>
<td><p>{“query”: “hello”}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_retriever_end</p></td>
<td><p>[retriever name]</p></td>
<td></td>
<td><p>{“query”: “hello”}</p></td>
<td><p>[Document(…), ..]</p></td>
</tr>
<tr class="row-odd"><td><p>on_prompt_start</p></td>
<td><p>[template_name]</p></td>
<td></td>
<td><p>{“question”: “hello”}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>on_prompt_end</p></td>
<td><p>[template_name]</p></td>
<td></td>
<td><p>{“question”: “hello”}</p></td>
<td><p>ChatPromptValue(messages: [SystemMessage, …])</p></td>
</tr>
</tbody>
</table>
</div>
<p>In addition to the standard events, users can also dispatch custom events (see example below).</p>
<p>Custom events will be only be surfaced with in the <cite>v2</cite> version of the API!</p>
<p>A custom event has following format:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>name</p></td>
<td><p>str</p></td>
<td><p>A user defined name for the event.</p></td>
</tr>
<tr class="row-odd"><td><p>data</p></td>
<td><p>Any</p></td>
<td><p>The data associated with the event. This can be anything, though we suggest making it JSON serializable.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Here are declarations associated with the standard events shown above:</p>
<p><cite>format_docs</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">'''Format the docs.'''</span>
    <span class="k">return</span> <span class="s2">", "</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">])</span>

<span class="n">format_docs</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">format_docs</span><span class="p">)</span>
</pre></div>
</div>
<p><cite>some_tool</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tool</span>
<span class="k">def</span> <span class="nf">some_tool</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">'''Some_tool.'''</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">"x"</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
</pre></div>
</div>
<p><cite>prompt</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[(</span><span class="s2">"system"</span><span class="p">,</span> <span class="s2">"You are Cat Agent 007"</span><span class="p">),</span> <span class="p">(</span><span class="s2">"human"</span><span class="p">,</span> <span class="s2">"</span><span class="si">{question}</span><span class="s2">"</span><span class="p">)]</span>
<span class="p">)</span><span class="o">.</span><span class="n">with_config</span><span class="p">({</span><span class="s2">"run_name"</span><span class="p">:</span> <span class="s2">"my_template"</span><span class="p">,</span> <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"my_template"</span><span class="p">]})</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">reverse</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">s</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">reverse</span><span class="p">)</span>

<span class="n">events</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">event</span> <span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">chain</span><span class="o">.</span><span class="n">astream_events</span><span class="p">(</span><span class="s2">"hello"</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">"v2"</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># will produce the following events (run_id, and parent_ids</span>
<span class="c1"># has been omitted for brevity):</span>
<span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">"data"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"input"</span><span class="p">:</span> <span class="s2">"hello"</span><span class="p">},</span>
        <span class="s2">"event"</span><span class="p">:</span> <span class="s2">"on_chain_start"</span><span class="p">,</span>
        <span class="s2">"metadata"</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"reverse"</span><span class="p">,</span>
        <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[],</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">"data"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"chunk"</span><span class="p">:</span> <span class="s2">"olleh"</span><span class="p">},</span>
        <span class="s2">"event"</span><span class="p">:</span> <span class="s2">"on_chain_stream"</span><span class="p">,</span>
        <span class="s2">"metadata"</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"reverse"</span><span class="p">,</span>
        <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[],</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">"data"</span><span class="p">:</span> <span class="p">{</span><span class="s2">"output"</span><span class="p">:</span> <span class="s2">"olleh"</span><span class="p">},</span>
        <span class="s2">"event"</span><span class="p">:</span> <span class="s2">"on_chain_end"</span><span class="p">,</span>
        <span class="s2">"metadata"</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"reverse"</span><span class="p">,</span>
        <span class="s2">"tags"</span><span class="p">:</span> <span class="p">[],</span>
    <span class="p">},</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Example: Dispatch Custom Event</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.callbacks.manager</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">adispatch_custom_event</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span><span class="p">,</span> <span class="n">RunnableConfig</span>
<span class="kn">import</span> <span class="nn">asyncio</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">slow_thing</span><span class="p">(</span><span class="n">some_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Do something that takes a long time."""</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Placeholder for some slow operation</span>
    <span class="k">await</span> <span class="n">adispatch_custom_event</span><span class="p">(</span>
        <span class="s2">"progress_event"</span><span class="p">,</span>
        <span class="p">{</span><span class="s2">"message"</span><span class="p">:</span> <span class="s2">"Finished step 1 of 3"</span><span class="p">},</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span> <span class="c1"># Must be included for python &lt; 3.10</span>
    <span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Placeholder for some slow operation</span>
    <span class="k">await</span> <span class="n">adispatch_custom_event</span><span class="p">(</span>
        <span class="s2">"progress_event"</span><span class="p">,</span>
        <span class="p">{</span><span class="s2">"message"</span><span class="p">:</span> <span class="s2">"Finished step 2 of 3"</span><span class="p">},</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span> <span class="c1"># Must be included for python &lt; 3.10</span>
    <span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Placeholder for some slow operation</span>
    <span class="k">return</span> <span class="s2">"Done"</span>

<span class="n">slow_thing</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">slow_thing</span><span class="p">)</span>

<span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">slow_thing</span><span class="o">.</span><span class="n">astream_events</span><span class="p">(</span><span class="s2">"some_input"</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">"v2"</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Any</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>None</em>) – The config to use for the Runnable.</p></li>
<li><p><strong>version</strong> (<em>Literal</em><em>[</em><em>'v1'</em><em>, </em><em>'v2'</em><em>]</em>) – The version of the schema to use either <cite>v2</cite> or <cite>v1</cite>.
Users should use <cite>v2</cite>.
<cite>v1</cite> is for backwards compatibility and will be deprecated
in 0.4.0.
No default will be assigned until the API is stabilized.
custom events will only be surfaced in <cite>v2</cite>.</p></li>
<li><p><strong>include_names</strong> (<em>Sequence</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – Only include events from runnables with matching names.</p></li>
<li><p><strong>include_types</strong> (<em>Sequence</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – Only include events from runnables with matching types.</p></li>
<li><p><strong>include_tags</strong> (<em>Sequence</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – Only include events from runnables with matching tags.</p></li>
<li><p><strong>exclude_names</strong> (<em>Sequence</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – Exclude events from runnables with matching names.</p></li>
<li><p><strong>exclude_types</strong> (<em>Sequence</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – Exclude events from runnables with matching types.</p></li>
<li><p><strong>exclude_tags</strong> (<em>Sequence</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – Exclude events from runnables with matching tags.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.
These will be passed to astream_log as this implementation
of astream_events is built on top of astream_log.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>An async stream of StreamEvents.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – If the version is not <cite>v1</cite> or <cite>v2</cite>.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>AsyncIterator</em>[<a class="reference internal" href="../../core/runnables/langchain_core.runnables.schema.StandardStreamEvent.html#langchain_core.runnables.schema.StandardStreamEvent" title="langchain_core.runnables.schema.StandardStreamEvent"><em>StandardStreamEvent</em></a> | <a class="reference internal" href="../../core/runnables/langchain_core.runnables.schema.CustomStreamEvent.html#langchain_core.runnables.schema.CustomStreamEvent" title="langchain_core.runnables.schema.CustomStreamEvent"><em>CustomStreamEvent</em></a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.batch">
<span class="sig-name descname"><span class="pre">batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.batch" title="Link to this definition">#</a></dt>
<dd><p>Default implementation runs invoke in parallel using a thread pool executor.</p>
<p>The default implementation of batch works well for IO bound runnables.</p>
<p>Subclasses should override this method if they can batch more efficiently;
e.g., if the underlying Runnable uses an API which supports a batch mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>list</em><em>[</em><em>Input</em><em>]</em>)</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>list</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>)</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>list[<em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.batch_as_completed">
<span class="sig-name descname"><span class="pre">batch_as_completed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_exceptions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Exception</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.batch_as_completed" title="Link to this definition">#</a></dt>
<dd><p>Run invoke in parallel on a list of inputs,
yielding results as they complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Sequence</em><em>[</em><em>Input</em><em>]</em>)</p></li>
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>] </em><em>| </em><em>None</em>)</p></li>
<li><p><strong>return_exceptions</strong> (<em>bool</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em><em> | </em><em>None</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Iterator</em>[tuple[int, <em>Output</em> | Exception]]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.bind">
<span class="sig-name descname"><span class="pre">bind</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.bind" title="Link to this definition">#</a></dt>
<dd><p>Bind arguments to a Runnable, returning a new Runnable.</p>
<p>Useful when a Runnable in a chain requires an argument that is not
in the output of the previous Runnable or included in the user input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>kwargs</strong> (<em>Any</em>) – The arguments to bind to the Runnable.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the arguments bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOllama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">'llama2'</span><span class="p">)</span>

<span class="c1"># Without bind.</span>
<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">llm</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"Repeat quoted words exactly: 'One two three four five.'"</span><span class="p">)</span>
<span class="c1"># Output is 'One two three four five.'</span>

<span class="c1"># With bind.</span>
<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">llm</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s2">"three"</span><span class="p">])</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"Repeat quoted words exactly: 'One two three four five.'"</span><span class="p">)</span>
<span class="c1"># Output is 'One two'</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.bind_functions">
<span class="sig-name descname"><span class="pre">bind_functions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">functions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">BaseModel</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><span class="pre">BaseTool</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">function_call</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">_FunctionCall</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'auto'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'none'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><span class="pre">PromptValue</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.bind_functions" title="Link to this definition">#</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version langchain-openai==0.2.1: </span>Use <a class="reference internal" href="#langchain_openai.chat_models.base.ChatOpenAI.bind_tools" title="langchain_openai.chat_models.base.ChatOpenAI.bind_tools"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bind_tools()</span></code></a> instead.</p>
</div>
<p>Bind functions (and other objects) to this chat model.</p>
<p>Assumes model is compatible with OpenAI function-calling API.</p>
<dl class="simple">
<dt>NOTE: Using bind_tools is recommended instead, as the <cite>functions</cite> and</dt><dd><p><cite>function_call</cite> request parameters are officially marked as deprecated by
OpenAI.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>functions</strong> (<em>Sequence</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>Type</em><em>[</em><em>BaseModel</em><em>] </em><em>| </em><em>Callable</em><em> | </em><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><em>BaseTool</em></a><em>]</em>) – A list of function definitions to bind to this chat model.
Can be  a dictionary, pydantic model, or callable. Pydantic
models and callables will be automatically converted to
their schema dictionary representation.</p></li>
<li><p><strong>function_call</strong> (<em>_FunctionCall</em><em> | </em><em>str</em><em> | </em><em>Literal</em><em>[</em><em>'auto'</em><em>, </em><em>'none'</em><em>] </em><em>| </em><em>None</em>) – Which function to require the model to call.
Must be the name of the single provided function or
“auto” to automatically determine which function to call
(if any).</p></li>
<li><p><strong>**kwargs</strong> (<em>Any</em>) – Any additional parameters to pass to the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Runnable</span></code> constructor.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><em>PromptValue</em></a> | str | <em>Sequence</em>[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a> | list[str] | tuple[str, str] | str | dict[str, <em>Any</em>]], <a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.bind_tools">
<span class="sig-name descname"><span class="pre">bind_tools</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tools</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Type</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><span class="pre">BaseTool</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tool_choice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'auto'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'none'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'required'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'any'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><span class="pre">PromptValue</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.bind_tools" title="Link to this definition">#</a></dt>
<dd><p>Bind tool-like objects to this chat model.</p>
<p>Assumes model is compatible with OpenAI tool-calling API.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tools</strong> (<em>Sequence</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>Type</em><em> | </em><em>Callable</em><em> | </em><a class="reference internal" href="../../core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool" title="langchain_core.tools.base.BaseTool"><em>BaseTool</em></a><em>]</em>) – A list of tool definitions to bind to this chat model.
Supports any tool definition handled by
<a class="reference internal" href="../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool" title="langchain_core.utils.function_calling.convert_to_openai_tool"><code class="xref py py-meth docutils literal notranslate"><span class="pre">langchain_core.utils.function_calling.convert_to_openai_tool()</span></code></a>.</p></li>
<li><p><strong>tool_choice</strong> (<em>dict</em><em> | </em><em>str</em><em> | </em><em>Literal</em><em>[</em><em>'auto'</em><em>, </em><em>'none'</em><em>, </em><em>'required'</em><em>, </em><em>'any'</em><em>] </em><em>| </em><em>bool</em><em> | </em><em>None</em>) – <p>Which tool to require the model to call. Options are:</p>
<ul>
<li><p>str of the form <code class="docutils literal notranslate"><span class="pre">"&lt;&lt;tool_name&gt;&gt;"</span></code>: calls &lt;&lt;tool_name&gt;&gt; tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"auto"</span></code>: automatically selects a tool (including no tool).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"none"</span></code>: does not call a tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">"any"</span></code> or <code class="docutils literal notranslate"><span class="pre">"required"</span></code> or <code class="docutils literal notranslate"><span class="pre">True</span></code>: force at least one tool to be called.</p></li>
<li><p>dict of the form <code class="docutils literal notranslate"><span class="pre">{"type":</span> <span class="pre">"function",</span> <span class="pre">"function":</span> <span class="pre">{"name":</span> <span class="pre">&lt;&lt;tool_name&gt;&gt;}}</span></code>: calls &lt;&lt;tool_name&gt;&gt; tool.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">False</span></code> or <code class="docutils literal notranslate"><span class="pre">None</span></code>: no effect, default OpenAI behavior.</p></li>
</ul>
</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em> | </em><em>None</em>) – If True, model output is guaranteed to exactly match the JSON Schema
provided in the tool definition. If True, the input schema will be
validated according to
<a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas">https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</a>.
If False, input schema will not be validated and model output will not
be validated.
If None, <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument will not be passed to the model.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Any additional parameters are passed directly to
<a class="reference internal" href="#langchain_openai.chat_models.base.ChatOpenAI.bind" title="langchain_openai.chat_models.base.ChatOpenAI.bind"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bind()</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><em>PromptValue</em></a> | str | <em>Sequence</em>[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a> | list[str] | tuple[str, str] | str | dict[str, <em>Any</em>]], <a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a>]</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.1.21: </span>Support for <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument added.</p>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.configurable_alternatives">
<span class="sig-name descname"><span class="pre">configurable_alternatives</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">which</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><span class="pre">ConfigurableField</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix_keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><span class="pre">RunnableSerializable</span></a></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.configurable_alternatives" title="Link to this definition">#</a></dt>
<dd><p>Configure alternatives for Runnables that can be set at runtime.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>which</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><em>ConfigurableField</em></a>) – The ConfigurableField instance that will be used to select the
alternative.</p></li>
<li><p><strong>default_key</strong> (<em>str</em>) – The default key to use if no alternative is selected.
Defaults to “default”.</p></li>
<li><p><strong>prefix_keys</strong> (<em>bool</em>) – Whether to prefix the keys with the ConfigurableField id.
Defaults to False.</p></li>
<li><p><strong>**kwargs</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>] </em><em>| </em><em>Callable</em><em>[</em><em>[</em><em>]</em><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>]</em><em>]</em>) – A dictionary of keys to Runnable instances or callables that
return Runnable instances.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the alternatives configured.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><em>RunnableSerializable</em></a></p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables.utils</span> <span class="kn">import</span> <span class="n">ConfigurableField</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">"claude-3-sonnet-20240229"</span>
<span class="p">)</span><span class="o">.</span><span class="n">configurable_alternatives</span><span class="p">(</span>
    <span class="n">ConfigurableField</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">"llm"</span><span class="p">),</span>
    <span class="n">default_key</span><span class="o">=</span><span class="s2">"anthropic"</span><span class="p">,</span>
    <span class="n">openai</span><span class="o">=</span><span class="n">ChatOpenAI</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># uses the default model ChatAnthropic</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"which organization created you?"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="c1"># uses ChatOpenAI</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">with_config</span><span class="p">(</span>
        <span class="n">configurable</span><span class="o">=</span><span class="p">{</span><span class="s2">"llm"</span><span class="p">:</span> <span class="s2">"openai"</span><span class="p">}</span>
    <span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"which organization created you?"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.configurable_fields">
<span class="sig-name descname"><span class="pre">configurable_fields</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><span class="pre">ConfigurableField</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldSingleOption.html#langchain_core.runnables.utils.ConfigurableFieldSingleOption" title="langchain_core.runnables.utils.ConfigurableFieldSingleOption"><span class="pre">ConfigurableFieldSingleOption</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldMultiOption.html#langchain_core.runnables.utils.ConfigurableFieldMultiOption" title="langchain_core.runnables.utils.ConfigurableFieldMultiOption"><span class="pre">ConfigurableFieldMultiOption</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><span class="pre">RunnableSerializable</span></a></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.configurable_fields" title="Link to this definition">#</a></dt>
<dd><p>Configure particular Runnable fields at runtime.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>**kwargs</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField" title="langchain_core.runnables.utils.ConfigurableField"><em>ConfigurableField</em></a><em> | </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldSingleOption.html#langchain_core.runnables.utils.ConfigurableFieldSingleOption" title="langchain_core.runnables.utils.ConfigurableFieldSingleOption"><em>ConfigurableFieldSingleOption</em></a><em> | </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.utils.ConfigurableFieldMultiOption.html#langchain_core.runnables.utils.ConfigurableFieldMultiOption" title="langchain_core.runnables.utils.ConfigurableFieldMultiOption"><em>ConfigurableFieldMultiOption</em></a>) – A dictionary of ConfigurableField instances to configure.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the fields configured.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable" title="langchain_core.runnables.base.RunnableSerializable"><em>RunnableSerializable</em></a></p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">ConfigurableField</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">configurable_fields</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">ConfigurableField</span><span class="p">(</span>
        <span class="nb">id</span><span class="o">=</span><span class="s2">"output_token_number"</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">"Max tokens in the output"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">"The maximum number of tokens in the output"</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># max_tokens = 20</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">"max_tokens_20: "</span><span class="p">,</span>
    <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"tell me something about chess"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="p">)</span>

<span class="c1"># max_tokens = 200</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"max_tokens_200: "</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">with_config</span><span class="p">(</span>
    <span class="n">configurable</span><span class="o">=</span><span class="p">{</span><span class="s2">"output_token_number"</span><span class="p">:</span> <span class="mi">200</span><span class="p">}</span>
    <span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"tell me something about chess"</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.get_num_tokens">
<span class="sig-name descname"><span class="pre">get_num_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.get_num_tokens" title="Link to this definition">#</a></dt>
<dd><p>Get the number of tokens present in the text.</p>
<p>Useful for checking if an input fits in a model’s context window.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>) – The string input to tokenize.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The integer number of tokens in the text.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.get_num_tokens_from_messages">
<span class="sig-name descname"><span class="pre">get_num_tokens_from_messages</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.get_num_tokens_from_messages" title="Link to this definition">#</a></dt>
<dd><p>Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.</p>
<p><strong>Requirements</strong>: You must have the <code class="docutils literal notranslate"><span class="pre">pillow</span></code> installed if you want to count
image tokens if you are specifying the image as a base64 string, and you must
have both <code class="docutils literal notranslate"><span class="pre">pillow</span></code> and <code class="docutils literal notranslate"><span class="pre">httpx</span></code> installed if you are specifying the image
as a URL. If these aren’t installed image inputs will be ignored in token
counting.</p>
<p>OpenAI reference: <a class="github reference external" href="https://github.com/openai/openai-cookbook/blob/">openai/openai-cookbook</a>
main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>messages</strong> (<em>List</em><em>[</em><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a><em>]</em>)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.get_token_ids">
<span class="sig-name descname"><span class="pre">get_token_ids</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.get_token_ids" title="Link to this definition">#</a></dt>
<dd><p>Get the tokens present in the text with tiktoken package.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>text</strong> (<em>str</em>)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>List</em>[int]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.invoke">
<span class="sig-name descname"><span class="pre">invoke</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.invoke" title="Link to this definition">#</a></dt>
<dd><p>Transform a single input into an output. Override to implement.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>) – A config to use when invoking the Runnable.
The config supports standard keys like ‘tags’, ‘metadata’ for tracing
purposes, ‘max_concurrency’ for controlling how much work to do
in parallel, and other keys. Please refer to the RunnableConfig
for more details.</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage">BaseMessage</a></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.stream">
<span class="sig-name descname"><span class="pre">stream</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LanguageModelInput</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk"><span class="pre">BaseMessageChunk</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.stream" title="Link to this definition">#</a></dt>
<dd><p>Default implementation of stream, which calls invoke.
Subclasses should override this method if they support streaming output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LanguageModelInput</em>) – The input to the Runnable.</p></li>
<li><p><strong>config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em>) – The config to use for the Runnable. Defaults to None.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the Runnable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Iterator[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessageChunk.html#langchain_core.messages.base.BaseMessageChunk" title="langchain_core.messages.base.BaseMessageChunk">BaseMessageChunk</a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.with_alisteners">
<span class="sig-name descname"><span class="pre">with_alisteners</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_start</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncListener</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncListener</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncListener</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.with_alisteners" title="Link to this definition">#</a></dt>
<dd><p>Bind asynchronous lifecycle listeners to a Runnable, returning a new Runnable.</p>
<p>on_start: Asynchronously called before the Runnable starts running.
on_end: Asynchronously called after the Runnable finishes running.
on_error: Asynchronously called if the Runnable throws an error.</p>
<p>The Run object contains information about the run, including its id,
type, input, output, error, start_time, end_time, and any tags or metadata
added to the run.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>on_start</strong> (<em>Optional</em><em>[</em><em>AsyncListener</em><em>]</em>) – Asynchronously called before the Runnable starts running.
Defaults to None.</p></li>
<li><p><strong>on_end</strong> (<em>Optional</em><em>[</em><em>AsyncListener</em><em>]</em>) – Asynchronously called after the Runnable finishes running.
Defaults to None.</p></li>
<li><p><strong>on_error</strong> (<em>Optional</em><em>[</em><em>AsyncListener</em><em>]</em>) – Asynchronously called if the Runnable throws an error.
Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the listeners bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable">Runnable</a>[Input, Output]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">test_runnable</span><span class="p">(</span><span class="n">time_to_sleep</span> <span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Runnable[</span><span class="si">{</span><span class="n">time_to_sleep</span><span class="si">}</span><span class="s2">s]: starts at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">time_to_sleep</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Runnable[</span><span class="si">{</span><span class="n">time_to_sleep</span><span class="si">}</span><span class="s2">s]: ends at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">fn_start</span><span class="p">(</span><span class="n">run_obj</span> <span class="p">:</span> <span class="n">Runnable</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on start callback starts at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on start callback ends at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">fn_end</span><span class="p">(</span><span class="n">run_obj</span> <span class="p">:</span> <span class="n">Runnable</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on end callback starts at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"on end callback ends at </span><span class="si">{</span><span class="n">format_t</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">runnable</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">test_runnable</span><span class="p">)</span><span class="o">.</span><span class="n">with_alisteners</span><span class="p">(</span>
    <span class="n">on_start</span><span class="o">=</span><span class="n">fn_start</span><span class="p">,</span>
    <span class="n">on_end</span><span class="o">=</span><span class="n">fn_end</span>
<span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">concurrent_runs</span><span class="p">():</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">runnable</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">runnable</span><span class="o">.</span><span class="n">ainvoke</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

<span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">concurrent_runs</span><span class="p">())</span>
<span class="n">Result</span><span class="p">:</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">29.637053</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">29.637150</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">32.638305</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">start</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">32.638383</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">3</span><span class="n">s</span><span class="p">]:</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">32.638849</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">5</span><span class="n">s</span><span class="p">]:</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">32.638999</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">3</span><span class="n">s</span><span class="p">]:</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">35.640016</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">35.640534</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">Runnable</span><span class="p">[</span><span class="mi">5</span><span class="n">s</span><span class="p">]:</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">37.640169</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">starts</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">37.640574</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">37.640654</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
<span class="n">on</span> <span class="n">end</span> <span class="n">callback</span> <span class="n">ends</span> <span class="n">at</span> <span class="mi">2024</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">16</span><span class="n">T14</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mf">39.641751</span><span class="o">+</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.with_config">
<span class="sig-name descname"><span class="pre">with_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.with_config" title="Link to this definition">#</a></dt>
<dd><p>Bind config to a Runnable, returning a new Runnable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em> | </em><em>None</em>) – The config to bind to the Runnable.</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword arguments to pass to the Runnable.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the config bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.with_fallbacks">
<span class="sig-name descname"><span class="pre">with_fallbacks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">fallbacks:</span> <span class="pre">Sequence[Runnable[Input,</span> <span class="pre">Output]],</span> <span class="pre">*,</span> <span class="pre">exceptions_to_handle:</span> <span class="pre">tuple[type[BaseException],</span> <span class="pre">...]</span> <span class="pre">=</span> <span class="pre">(&lt;class</span> <span class="pre">'Exception'&gt;,),</span> <span class="pre">exception_key:</span> <span class="pre">Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">RunnableWithFallbacksT</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.with_fallbacks" title="Link to this definition">#</a></dt>
<dd><p>Add fallbacks to a Runnable, returning a new Runnable.</p>
<p>The new Runnable will try the original Runnable, and then each fallback
in order, upon failures.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fallbacks</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>]</em><em>]</em>) – A sequence of runnables to try if the original Runnable fails.</p></li>
<li><p><strong>exceptions_to_handle</strong> (<em>tuple</em><em>[</em><em>type</em><em>[</em><em>BaseException</em><em>]</em><em>, </em><em>...</em><em>]</em>) – A tuple of exception types to handle.
Defaults to (Exception,).</p></li>
<li><p><strong>exception_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – If string is specified then handled exceptions will be passed
to fallbacks as part of the input under the specified key. If None,
exceptions will not be passed to fallbacks. If used, the base Runnable
and its fallbacks must accept a dictionary as input. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable that will try the original Runnable, and then each
fallback in order, upon failures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>RunnableWithFallbacksT[Input, Output]</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Iterator</span>

<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableGenerator</span>


<span class="k">def</span> <span class="nf">_generate_immediate_error</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">()</span>
    <span class="k">yield</span> <span class="s2">""</span>


<span class="k">def</span> <span class="nf">_generate</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="k">yield from</span> <span class="s2">"foo bar"</span>


<span class="n">runnable</span> <span class="o">=</span> <span class="n">RunnableGenerator</span><span class="p">(</span><span class="n">_generate_immediate_error</span><span class="p">)</span><span class="o">.</span><span class="n">with_fallbacks</span><span class="p">(</span>
    <span class="p">[</span><span class="n">RunnableGenerator</span><span class="p">(</span><span class="n">_generate</span><span class="p">)]</span>
    <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">runnable</span><span class="o">.</span><span class="n">stream</span><span class="p">({})))</span> <span class="c1">#foo bar</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fallbacks</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a><em>[</em><em>Input</em><em>, </em><em>Output</em><em>]</em><em>]</em>) – A sequence of runnables to try if the original Runnable fails.</p></li>
<li><p><strong>exceptions_to_handle</strong> (<em>tuple</em><em>[</em><em>type</em><em>[</em><em>BaseException</em><em>]</em><em>, </em><em>...</em><em>]</em>) – A tuple of exception types to handle.</p></li>
<li><p><strong>exception_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – If string is specified then handled exceptions will be passed
to fallbacks as part of the input under the specified key. If None,
exceptions will not be passed to fallbacks. If used, the base Runnable
and its fallbacks must accept a dictionary as input.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable that will try the original Runnable, and then each
fallback in order, upon failures.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>RunnableWithFallbacksT[Input, Output]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.with_listeners">
<span class="sig-name descname"><span class="pre">with_listeners</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_start</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><span class="pre">Run</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><span class="pre">Run</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_end</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><span class="pre">Run</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><span class="pre">Run</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_error</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><span class="pre">Run</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><span class="pre">Run</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><span class="pre">RunnableConfig</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.with_listeners" title="Link to this definition">#</a></dt>
<dd><p>Bind lifecycle listeners to a Runnable, returning a new Runnable.</p>
<p>on_start: Called before the Runnable starts running, with the Run object.
on_end: Called after the Runnable finishes running, with the Run object.
on_error: Called if the Runnable throws an error, with the Run object.</p>
<p>The Run object contains information about the run, including its id,
type, input, output, error, start_time, end_time, and any tags or metadata
added to the run.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>on_start</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><em>Run</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>, </em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><em>Run</em></a><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>]</em><em>]</em>) – Called before the Runnable starts running. Defaults to None.</p></li>
<li><p><strong>on_end</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><em>Run</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>, </em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><em>Run</em></a><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>]</em><em>]</em>) – Called after the Runnable finishes running. Defaults to None.</p></li>
<li><p><strong>on_error</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><em>Run</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>, </em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="../../core/tracers/langchain_core.tracers.schemas.Run.html#langchain_core.tracers.schemas.Run" title="langchain_core.tracers.schemas.Run"><em>Run</em></a><em>, </em><a class="reference internal" href="../../core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig" title="langchain_core.runnables.config.RunnableConfig"><em>RunnableConfig</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>]</em><em>]</em>) – Called if the Runnable throws an error. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the listeners bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable">Runnable</a>[Input, Output]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>
<span class="kn">from</span> <span class="nn">langchain_core.tracers.schemas</span> <span class="kn">import</span> <span class="n">Run</span>

<span class="kn">import</span> <span class="nn">time</span>

<span class="k">def</span> <span class="nf">test_runnable</span><span class="p">(</span><span class="n">time_to_sleep</span> <span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">time_to_sleep</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">fn_start</span><span class="p">(</span><span class="n">run_obj</span><span class="p">:</span> <span class="n">Run</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"start_time:"</span><span class="p">,</span> <span class="n">run_obj</span><span class="o">.</span><span class="n">start_time</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">fn_end</span><span class="p">(</span><span class="n">run_obj</span><span class="p">:</span> <span class="n">Run</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"end_time:"</span><span class="p">,</span> <span class="n">run_obj</span><span class="o">.</span><span class="n">end_time</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">test_runnable</span><span class="p">)</span><span class="o">.</span><span class="n">with_listeners</span><span class="p">(</span>
    <span class="n">on_start</span><span class="o">=</span><span class="n">fn_start</span><span class="p">,</span>
    <span class="n">on_end</span><span class="o">=</span><span class="n">fn_end</span>
<span class="p">)</span>
<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.with_retry">
<span class="sig-name descname"><span class="pre">with_retry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">*,</span> <span class="pre">retry_if_exception_type:</span> <span class="pre">tuple[type[BaseException],</span> <span class="pre">...]</span> <span class="pre">=</span> <span class="pre">(&lt;class</span> <span class="pre">'Exception'&gt;,),</span> <span class="pre">wait_exponential_jitter:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">stop_after_attempt:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.with_retry" title="Link to this definition">#</a></dt>
<dd><p>Create a new Runnable that retries the original Runnable on exceptions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>retry_if_exception_type</strong> (<em>tuple</em><em>[</em><em>type</em><em>[</em><em>BaseException</em><em>]</em><em>, </em><em>...</em><em>]</em>) – A tuple of exception types to retry on.
Defaults to (Exception,).</p></li>
<li><p><strong>wait_exponential_jitter</strong> (<em>bool</em>) – Whether to add jitter to the wait
time between retries. Defaults to True.</p></li>
<li><p><strong>stop_after_attempt</strong> (<em>int</em>) – The maximum number of attempts to make before
giving up. Defaults to 3.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable that retries the original Runnable on exceptions.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>

<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">_lambda</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">global</span> <span class="n">count</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"x is 1"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
         <span class="k">pass</span>


<span class="n">runnable</span> <span class="o">=</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">_lambda</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">runnable</span><span class="o">.</span><span class="n">with_retry</span><span class="p">(</span>
        <span class="n">stop_after_attempt</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">retry_if_exception_type</span><span class="o">=</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">,),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="k">assert</span> <span class="p">(</span><span class="n">count</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>retry_if_exception_type</strong> (<em>tuple</em><em>[</em><em>type</em><em>[</em><em>BaseException</em><em>]</em><em>, </em><em>...</em><em>]</em>) – A tuple of exception types to retry on</p></li>
<li><p><strong>wait_exponential_jitter</strong> (<em>bool</em>) – Whether to add jitter to the wait time
between retries</p></li>
<li><p><strong>stop_after_attempt</strong> (<em>int</em>) – The maximum number of attempts to make before giving up</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable that retries the original Runnable on exceptions.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.with_structured_output">
<span class="sig-name descname"><span class="pre">with_structured_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">schema</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Type</span><span class="p"><span class="pre">[</span></span><span class="pre">_BM</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Type</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'function_calling'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'json_mode'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'json_schema'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'function_calling'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_raw</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><span class="pre">PromptValue</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><span class="pre">BaseMessage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">_BM</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.with_structured_output" title="Link to this definition">#</a></dt>
<dd><p>Model wrapper that returns outputs formatted to match the given schema.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>schema</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>Type</em><em>[</em><em>_BM</em><em>] </em><em>| </em><em>Type</em><em> | </em><em>None</em>) – <p>The output schema. Can be passed in as:</p>
<ul>
<li><p>an OpenAI function/tool schema,</p></li>
<li><p>a JSON Schema,</p></li>
<li><p>a TypedDict class (support added in 0.1.20),</p></li>
<li><p>or a Pydantic class.</p></li>
</ul>
<p>If <code class="docutils literal notranslate"><span class="pre">schema</span></code> is a Pydantic class then the model output will be a
Pydantic instance of that class, and the model-generated fields will be
validated by the Pydantic class. Otherwise the model output will be a
dict and will not be validated. See <a class="reference internal" href="../../core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html#langchain_core.utils.function_calling.convert_to_openai_tool" title="langchain_core.utils.function_calling.convert_to_openai_tool"><code class="xref py py-meth docutils literal notranslate"><span class="pre">langchain_core.utils.function_calling.convert_to_openai_tool()</span></code></a>
for more on how to properly specify types and descriptions of
schema fields when specifying a Pydantic or TypedDict class.</p>
</p></li>
<li><p><strong>method</strong> (<em>Literal</em><em>[</em><em>'function_calling'</em><em>, </em><em>'json_mode'</em><em>, </em><em>'json_schema'</em><em>]</em>) – <p>The method for steering model generation, one of:</p>
<ul>
<li><dl class="simple">
<dt>”function_calling”:</dt><dd><p>Uses OpenAI’s tool-calling (formerly called function calling)
API: <a class="reference external" href="https://platform.openai.com/docs/guides/function-calling">https://platform.openai.com/docs/guides/function-calling</a></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>”json_schema”:</dt><dd><p>Uses OpenAI’s Structured Output API: <a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs">https://platform.openai.com/docs/guides/structured-outputs</a>
Supported for “gpt-4o-mini”, “gpt-4o-2024-08-06”, and later
models.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>”json_mode”:</dt><dd><p>Uses OpenAI’s JSON mode. Note that if using JSON mode then you
must include instructions for formatting the output into the
desired schema into the model call:
<a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/json-mode">https://platform.openai.com/docs/guides/structured-outputs/json-mode</a></p>
</dd>
</dl>
</li>
</ul>
<p>Learn more about the differences between the methods and which models
support which methods here:</p>
<ul>
<li><p><a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode">https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode</a></p></li>
<li><p><a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format">https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format</a></p></li>
</ul>
</p></li>
<li><p><strong>include_raw</strong> (<em>bool</em>) – If False then only the parsed structured output is returned. If
an error occurs during model output parsing it will be raised. If True
then both the raw model response (a BaseMessage) and the parsed model
response will be returned. If an error occurs during output parsing it
will be caught and returned as well. The final output is always a dict
with keys “raw”, “parsed”, and “parsing_error”.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em> | </em><em>None</em>) – <ul>
<li><dl class="simple">
<dt>True:</dt><dd><p>Model output is guaranteed to exactly match the schema.
The input schema will also be validated according to
<a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas">https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</a></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>False:</dt><dd><p>Input schema will not be validated and model output will not be
validated.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>None:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">strict</span></code> argument will not be passed to the model.</p>
</dd>
</dl>
</li>
</ul>
<p>If <code class="docutils literal notranslate"><span class="pre">method</span></code> is “json_schema” defaults to True. If <code class="docutils literal notranslate"><span class="pre">method</span></code> is
“function_calling” or “json_mode” defaults to None. Can only be
non-null if <code class="docutils literal notranslate"><span class="pre">method</span></code> is “function_calling” or “json_schema”.</p>
</p></li>
<li><p><strong>kwargs</strong> (<em>Any</em>) – Additional keyword args aren’t supported.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><p>A Runnable that takes same inputs as a <code class="xref py py-class docutils literal notranslate"><span class="pre">langchain_core.language_models.chat.BaseChatModel</span></code>.</p>
<div class="line-block">
<div class="line">If <code class="docutils literal notranslate"><span class="pre">include_raw</span></code> is False and <code class="docutils literal notranslate"><span class="pre">schema</span></code> is a Pydantic class, Runnable outputs an instance of <code class="docutils literal notranslate"><span class="pre">schema</span></code> (i.e., a Pydantic object). Otherwise, if <code class="docutils literal notranslate"><span class="pre">include_raw</span></code> is False then Runnable outputs a dict.</div>
</div>
<div class="line-block">
<div class="line">If <code class="docutils literal notranslate"><span class="pre">include_raw</span></code> is True, then Runnable outputs a dict with keys:</div>
</div>
<ul class="simple">
<li><p>”raw”: BaseMessage</p></li>
<li><p>”parsed”: None if there was a parsing error, otherwise the type depends on the <code class="docutils literal notranslate"><span class="pre">schema</span></code> as described above.</p></li>
<li><p>”parsing_error”: Optional[BaseException]</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<a class="reference internal" href="../../core/prompt_values/langchain_core.prompt_values.PromptValue.html#langchain_core.prompt_values.PromptValue" title="langchain_core.prompt_values.PromptValue"><em>PromptValue</em></a> | str | <em>Sequence</em>[<a class="reference internal" href="../../core/messages/langchain_core.messages.base.BaseMessage.html#langchain_core.messages.base.BaseMessage" title="langchain_core.messages.base.BaseMessage"><em>BaseMessage</em></a> | list[str] | tuple[str, str] | str | dict[str, <em>Any</em>]], <em>Dict</em> | <em>_BM</em>]</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.1.20: </span>Added support for TypedDict class <code class="docutils literal notranslate"><span class="pre">schema</span></code>.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.1.21: </span>Support for <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument added.
Support for <code class="docutils literal notranslate"><span class="pre">method</span></code> = “json_schema” added.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Planned breaking changes in version <cite>0.2.0</cite></p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">method</span></code> default will be changed to “json_schema” from</dt><dd><p>“function_calling”.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">strict</span></code> will default to True when <code class="docutils literal notranslate"><span class="pre">method</span></code> is</dt><dd><p>“function_calling” as of version <cite>0.2.0</cite>.</p>
</dd>
</dl>
</li>
</ul>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Example: schema=Pydantic class, method=”function_calling”, include_raw=False, strict=True</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Note, OpenAI has a number of restrictions on what types of schemas can be
provided if <code class="docutils literal notranslate"><span class="pre">strict</span></code> = True. When using Pydantic, our model cannot
specify any Field metadata (like min/max constraints) and fields cannot
have default values.</p>
<p class="sd-card-text">See all constraints here: <a class="reference external" href="https://platform.openai.com/docs/guides/structured-outputs/supported-schemas">https://platform.openai.com/docs/guides/structured-outputs/supported-schemas</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>


<span class="k">class</span> <span class="nc">AnswerWithJustification</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''An answer to the user question along with justification for the answer.'''</span>

    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">justification</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"A justification for the answer."</span>
    <span class="p">)</span>


<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span>
    <span class="n">AnswerWithJustification</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"What weighs more a pound of bricks or a pound of feathers"</span>
<span class="p">)</span>

<span class="c1"># -&gt; AnswerWithJustification(</span>
<span class="c1">#     answer='They weigh the same',</span>
<span class="c1">#     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Example: schema=Pydantic class, method=”function_calling”, include_raw=True</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>


<span class="k">class</span> <span class="nc">AnswerWithJustification</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''An answer to the user question along with justification for the answer.'''</span>

    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">justification</span><span class="p">:</span> <span class="nb">str</span>


<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span>
    <span class="n">AnswerWithJustification</span><span class="p">,</span> <span class="n">include_raw</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"What weighs more a pound of bricks or a pound of feathers"</span>
<span class="p">)</span>
<span class="c1"># -&gt; {</span>
<span class="c1">#     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{"answer":"They weigh the same.","justification":"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ."}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),</span>
<span class="c1">#     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),</span>
<span class="c1">#     'parsing_error': None</span>
<span class="c1"># }</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Example: schema=TypedDict class, method=”function_calling”, include_raw=False</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># IMPORTANT: If you are using Python &lt;=3.8, you need to import Annotated</span>
<span class="c1"># from typing_extensions, not from typing.</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">TypedDict</span>

<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>


<span class="k">class</span> <span class="nc">AnswerWithJustification</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''An answer to the user question along with justification for the answer.'''</span>

    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">justification</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span>
        <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">"A justification for the answer."</span>
    <span class="p">]</span>


<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">AnswerWithJustification</span><span class="p">)</span>

<span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"What weighs more a pound of bricks or a pound of feathers"</span>
<span class="p">)</span>
<span class="c1"># -&gt; {</span>
<span class="c1">#     'answer': 'They weigh the same',</span>
<span class="c1">#     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'</span>
<span class="c1"># }</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Example: schema=OpenAI function schema, method=”function_calling”, include_raw=False</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

 <span class="n">oai_schema</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s1">'name'</span><span class="p">:</span> <span class="s1">'AnswerWithJustification'</span><span class="p">,</span>
     <span class="s1">'description'</span><span class="p">:</span> <span class="s1">'An answer to the user question along with justification for the answer.'</span><span class="p">,</span>
     <span class="s1">'parameters'</span><span class="p">:</span> <span class="p">{</span>
         <span class="s1">'type'</span><span class="p">:</span> <span class="s1">'object'</span><span class="p">,</span>
         <span class="s1">'properties'</span><span class="p">:</span> <span class="p">{</span>
             <span class="s1">'answer'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'type'</span><span class="p">:</span> <span class="s1">'string'</span><span class="p">},</span>
             <span class="s1">'justification'</span><span class="p">:</span> <span class="p">{</span><span class="s1">'description'</span><span class="p">:</span> <span class="s1">'A justification for the answer.'</span><span class="p">,</span> <span class="s1">'type'</span><span class="p">:</span> <span class="s1">'string'</span><span class="p">}</span>
         <span class="p">},</span>
        <span class="s1">'required'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'answer'</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">}</span>

 <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
 <span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">oai_schema</span><span class="p">)</span>

 <span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
     <span class="s2">"What weighs more a pound of bricks or a pound of feathers"</span>
 <span class="p">)</span>
 <span class="c1"># -&gt; {</span>
 <span class="c1">#     'answer': 'They weigh the same',</span>
 <span class="c1">#     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'</span>
 <span class="c1"># }</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Example: schema=Pydantic class, method=”json_mode”, include_raw=True</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>

<span class="k">class</span> <span class="nc">AnswerWithJustification</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">justification</span><span class="p">:</span> <span class="nb">str</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt-4o"</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span>
    <span class="n">AnswerWithJustification</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">"json_mode"</span><span class="p">,</span>
    <span class="n">include_raw</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"Answer the following question. "</span>
    <span class="s2">"Make sure to return a JSON blob with keys 'answer' and 'justification'.</span><span class="se">\n\n</span><span class="s2">"</span>
    <span class="s2">"What's heavier a pound of bricks or a pound of feathers?"</span>
<span class="p">)</span>
<span class="c1"># -&gt; {</span>
<span class="c1">#     'raw': AIMessage(content='{\n    "answer": "They are both the same weight.",\n    "justification": "Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight." \n}'),</span>
<span class="c1">#     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),</span>
<span class="c1">#     'parsing_error': None</span>
<span class="c1"># }</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Example: schema=None, method=”json_mode”, include_raw=True</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg aria-hidden="true" class="sd-octicon sd-octicon-chevron-right" height="1.5em" version="1.1" viewbox="0 0 24 24" width="1.5em"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">"json_mode"</span><span class="p">,</span> <span class="n">include_raw</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="s2">"Answer the following question. "</span>
    <span class="s2">"Make sure to return a JSON blob with keys 'answer' and 'justification'.</span><span class="se">\n\n</span><span class="s2">"</span>
    <span class="s2">"What's heavier a pound of bricks or a pound of feathers?"</span>
<span class="p">)</span>
<span class="c1"># -&gt; {</span>
<span class="c1">#     'raw': AIMessage(content='{\n    "answer": "They are both the same weight.",\n    "justification": "Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight." \n}'),</span>
<span class="c1">#     'parsed': {</span>
<span class="c1">#         'answer': 'They are both the same weight.',</span>
<span class="c1">#         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'</span>
<span class="c1">#     },</span>
<span class="c1">#     'parsing_error': None</span>
<span class="c1"># }</span>
</pre></div>
</div>
</div>
</details></dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="langchain_openai.chat_models.base.ChatOpenAI.with_types">
<span class="sig-name descname"><span class="pre">with_types</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">type</span><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">type</span><span class="p"><span class="pre">[</span></span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><span class="pre">Runnable</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Input</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Output</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#langchain_openai.chat_models.base.ChatOpenAI.with_types" title="Link to this definition">#</a></dt>
<dd><p>Bind input and output types to a Runnable, returning a new Runnable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_type</strong> (<em>type</em><em>[</em><em>Input</em><em>] </em><em>| </em><em>None</em>) – The input type to bind to the Runnable. Defaults to None.</p></li>
<li><p><strong>output_type</strong> (<em>type</em><em>[</em><em>Output</em><em>] </em><em>| </em><em>None</em>) – The output type to bind to the Runnable. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Runnable with the types bound.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../../core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable" title="langchain_core.runnables.base.Runnable"><em>Runnable</em></a>[<em>Input</em>, <em>Output</em>]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<p class="rubric">Examples using ChatOpenAI</p>
<ul class="simple">
<li><p><a class="reference external" href="https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain/"># Example</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/versions/migrating_chains/llm_router_chain/"># Legacy</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/ainetwork/">AINetwork Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/aws_dynamodb/">AWS DynamoDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/activeloop/">Activeloop Deep Memory</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/amadeus/">Amadeus Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/amazon_neptune_open_cypher/">Amazon Neptune with Cypher</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/apache_age/">Apache AGE</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/cassandra/">Apache Cassandra</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/arxiv/">ArXiv</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/arangodb/">ArangoDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/arthur_tracking/">Arthur</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/asknews/">AskNews</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/azure_dynamic_sessions/">Azure Container Apps dynamic sessions</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/azure_ai_search/">AzureAISearchRetriever</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/bearly/">Bearly Code Interpreter</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders/browserbase/">Browserbase</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/tutorials/query_analysis/">Build a Query Analysis System</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/tutorials/graph/">Build a Question Answering application over a Graph Database</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/tutorials/llm_chain/">Build a Simple LLM Application with LCEL</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/cassandra_database/">Cassandra Database Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/chatgpt_plugins/">ChatGPT Plugins</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat/openai/">ChatOpenAI</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/tutorials/classification/">Classify Text into Labels</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/cnosdb/">CnosDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/cogniswitch/">Cogniswitch Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/">Conceptual guide</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/connery_toolkit/">Connery Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/connery/">Connery Toolkit and Tools</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/context/">Context</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/tutorials/qa_chat_history/">Conversational RAG</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/couchbase_chat_message_history/">Couchbase</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/dataherald/">Dataherald</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/diffbot/">Diffbot</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/discord/">Discord</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/">E2B Data Analysis</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/">ElasticsearchRetriever</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/exa_search/">Exa Search</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/facebook/">Facebook Messenger</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/falkordb/">FalkorDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders/figma/">Figma</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/financial_datasets/">FinancialDatasets Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/flashrank-reranker/">FlashRank reranker</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/fleet_context/">Fleet AI Context</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/flyte/">Flyte</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/tutorials/data_generation/">Generate Synthetic Data</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/hippo/">Hippo</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/query_high_cardinality/">How deal with high cardinality categoricals when doing query analysis</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/graph_semantic/">How to add a semantic layer over graph database</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/qa_chat_history_how_to/">How to add chat history</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/binding/">How to add default invocation args to a Runnable</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/query_few_shot/">How to add examples to the prompt for query analysis</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/fallbacks/">How to add fallbacks to a runnable</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/chatbots_memory/">How to add memory to chatbots</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/chatbots_retrieval/">How to add retrieval to chatbots</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/add_scores_retriever/">How to add scores to retriever results</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/chatbots_tools/">How to add tools to chatbots</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/assign/">How to add values to a chain’s state</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/graph_prompting/">How to best prompt for Graph-RAG</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/tools_model_specific/">How to bind model-specific tools</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/configure/">How to configure runtime chain internals</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/graph_constructing/">How to construct knowledge graphs</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/tools_as_openai_functions/">How to convert tools to OpenAI Functions</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/tool_calling_parallel/">How to disable parallel tool calling</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/self_query/">How to do “self-querying” retrieval</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/contextual_compression/">How to do retrieval with contextual compression</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/logprobs/">How to get log probabilities</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/query_no_queries/">How to handle cases where no queries are generated</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/query_multiple_queries/">How to handle multiple queries when doing query analysis</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/query_multiple_retrievers/">How to handle multiple retrievers when doing query analysis</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/inspect/">How to inspect runnables</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/parallel/">How to invoke runnables in parallel</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/graph_mapping/">How to map values to a graph database</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/migrate_agent/">How to migrate from legacy LangChain agents to LangGraph</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/output_parser_json/">How to parse JSON output</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/output_parser_yaml/">How to parse YAML output</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/multimodal_inputs/">How to pass multimodal data directly to models</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/passthrough/">How to pass through arguments from one step to the next</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/output_parser_retry/">How to retry when a parsing error occurs</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/functions/">How to run custom functions</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/serialization/">How to save and load LangChain objects</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/tool_streaming/">How to stream tool calls</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/chat_token_usage_tracking/">How to track token usage in ChatModels</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/trim_messages/">How to trim messages</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/pydantic_compatibility/">How to use LangChain with different Pydantic versions</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/few_shot_examples_chat/">How to use few shot examples in chat models</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/tools_few_shot/">How to use few-shot prompting with tool calling</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/multimodal_prompts/">How to use multimodal prompts</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/extraction_examples/">How to use reference examples when doing extraction</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/MultiQueryRetriever/">How to use the MultiQueryRetriever</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/output_parser_fixing/">How to use the output-fixing parser</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/hugegraph/">HugeGraph</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/human_tools/">Human as a tool</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/hybrid/">Hybrid Search</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders/image_captions/">Image captions</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/infino/">Infino</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/infobip/">Infobip</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/jaguar/">Jaguar Vector Database</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_transformers/jina_rerank/">Jina Reranker</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/kdbai/">KDB.AI</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/kay/">Kay.ai</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/kuzu_db/">Kuzu</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/llmlingua/">LLMLingua Document Compressor</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/llmonitor/">LLMonitor</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/labelstudio/">Label Studio</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/langsmith_dataset/">LangSmith Chat Datasets</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/langsmith_llm_runs/">LangSmith LLM Runs</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/">Load docs</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/portkey/logging_tracing_portkey/">Log, Trace, and Monitor</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/log10/">Log10</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/mlflow_tracking/">MLflow</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/memgraph/">Memgraph</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/milvus_hybrid_search/">Milvus Hybrid Search Retriever</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/llm_caching/">Model caches</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/momento_vector_index/">Momento Vector Index (MVI)</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/">MongoDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/multion/">MultiOn Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/self_query/myscale_self_query/">MyScale</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/nebula_graph/">NebulaGraph</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/">Neo4j</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/neo4jvector/">Neo4j Vector Index</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/ontotext/">Ontotext GraphDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/platforms/openai/">OpenAI</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_transformers/openai_metadata_tagger/">OpenAI metadata tagger</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/openapi/">OpenAPI Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/outline/">Outline</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/pandas/">Pandas Dataframe</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/passio_nutrition_ai/">Passio NutritionAI</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/polygon_toolkit/">Polygon IO Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/polygon/">Polygon IO Toolkit and Tools</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/providers/portkey/index/">Portkey</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/powerbi/">PowerBI Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/promptlayer/">PromptLayer</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/ragatouille/">RAGatouille</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/graphs/rdflib_sparql/">RDFLib</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_transformers/rankllm-reranker/">RankLLM Reranker</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/re_phrase/">RePhraseQuery</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/reddit_search/">Reddit Search </a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/redis_chat_message_history/">Redis</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/rememberizer/">Rememberizer</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/remembrall/">Remembrall</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/requests/">Requests Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/response_metadata/">Response metadata</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/robocorp/">Robocorp Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/">SAP HANA Cloud Vector Engine</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/sec_filings/">SEC filing</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/">SQL (SQLAlchemy)</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/sqlite/">SQLite</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/semanticscholar/">Semantic Scholar API Tool</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/bash/">Shell (bash)</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/slack/">Slack</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/slack/">Slack Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/spark_sql/">Spark SQL Toolkit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/">Streamlit</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/tavily/">TavilySearchAPIRetriever</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/telegram/">Telegram</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/self_query/tencentvectordb/">Tencent Cloud VectorDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/tidb_chat_message_history/">TiDB</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/timescalevector/">Timescale Vector (Postgres)</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/trubrics/">Trubrics</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/uptrain/">UpTrain</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/callbacks/upstash_ratelimit/">Upstash Ratelimit Callback</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/vectara/">Vectara</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/self_query/vectara_self_query/">Vectara self-querying </a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/wechat/">WeChat</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/weaviate/">Weaviate</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/whatsapp/">WhatsApp</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/xata_chat_message_history/">Xata</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/yahoo_finance_news/">Yahoo Finance News</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/yellowbrick/">Yellowbrick</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/retrievers/you-retriever/">You.com</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/tools/you/">You.com Search</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders/youtube_audio/">YouTube audio</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/memory/zep_cloud_chat_message_history/">ZepCloudChatMessageHistory</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat_loaders/imessage/">iMessage</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/integrations/chat/vllm/">vLLM Chat</a></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/langserve/">🦜️🏓 LangServe</a></p></li>
</ul>
</section>
</article>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage" id="pst-page-navigation-heading-2">
<i class="fa-solid fa-list"></i> On this page
  </div>
<nav aria-labelledby="pst-page-navigation-heading-2" class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI"><code class="docutils literal notranslate"><span class="pre">ChatOpenAI</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.cache"><code class="docutils literal notranslate"><span class="pre">cache</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.callback_manager"><code class="docutils literal notranslate"><span class="pre">callback_manager</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.callbacks"><code class="docutils literal notranslate"><span class="pre">callbacks</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.custom_get_token_ids"><code class="docutils literal notranslate"><span class="pre">custom_get_token_ids</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.default_headers"><code class="docutils literal notranslate"><span class="pre">default_headers</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.default_query"><code class="docutils literal notranslate"><span class="pre">default_query</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.disable_streaming"><code class="docutils literal notranslate"><span class="pre">disable_streaming</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.extra_body"><code class="docutils literal notranslate"><span class="pre">extra_body</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.frequency_penalty"><code class="docutils literal notranslate"><span class="pre">frequency_penalty</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.http_async_client"><code class="docutils literal notranslate"><span class="pre">http_async_client</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.http_client"><code class="docutils literal notranslate"><span class="pre">http_client</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.include_response_headers"><code class="docutils literal notranslate"><span class="pre">include_response_headers</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.logit_bias"><code class="docutils literal notranslate"><span class="pre">logit_bias</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.logprobs"><code class="docutils literal notranslate"><span class="pre">logprobs</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.max_retries"><code class="docutils literal notranslate"><span class="pre">max_retries</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.max_tokens"><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.metadata"><code class="docutils literal notranslate"><span class="pre">metadata</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.model_kwargs"><code class="docutils literal notranslate"><span class="pre">model_kwargs</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.model_name"><code class="docutils literal notranslate"><span class="pre">model_name</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.n"><code class="docutils literal notranslate"><span class="pre">n</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_api_base"><code class="docutils literal notranslate"><span class="pre">openai_api_base</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_api_key"><code class="docutils literal notranslate"><span class="pre">openai_api_key</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_organization"><code class="docutils literal notranslate"><span class="pre">openai_organization</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.openai_proxy"><code class="docutils literal notranslate"><span class="pre">openai_proxy</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.presence_penalty"><code class="docutils literal notranslate"><span class="pre">presence_penalty</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.rate_limiter"><code class="docutils literal notranslate"><span class="pre">rate_limiter</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.request_timeout"><code class="docutils literal notranslate"><span class="pre">request_timeout</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.seed"><code class="docutils literal notranslate"><span class="pre">seed</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.stop"><code class="docutils literal notranslate"><span class="pre">stop</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.stream_usage"><code class="docutils literal notranslate"><span class="pre">stream_usage</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.streaming"><code class="docutils literal notranslate"><span class="pre">streaming</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.tags"><code class="docutils literal notranslate"><span class="pre">tags</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.temperature"><code class="docutils literal notranslate"><span class="pre">temperature</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.tiktoken_model_name"><code class="docutils literal notranslate"><span class="pre">tiktoken_model_name</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.top_logprobs"><code class="docutils literal notranslate"><span class="pre">top_logprobs</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.top_p"><code class="docutils literal notranslate"><span class="pre">top_p</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.verbose"><code class="docutils literal notranslate"><span class="pre">verbose</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.__call__"><code class="docutils literal notranslate"><span class="pre">__call__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.abatch"><code class="docutils literal notranslate"><span class="pre">abatch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.abatch_as_completed"><code class="docutils literal notranslate"><span class="pre">abatch_as_completed()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.ainvoke"><code class="docutils literal notranslate"><span class="pre">ainvoke()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.astream"><code class="docutils literal notranslate"><span class="pre">astream()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.astream_events"><code class="docutils literal notranslate"><span class="pre">astream_events()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.batch"><code class="docutils literal notranslate"><span class="pre">batch()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.batch_as_completed"><code class="docutils literal notranslate"><span class="pre">batch_as_completed()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.bind"><code class="docutils literal notranslate"><span class="pre">bind()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.bind_functions"><code class="docutils literal notranslate"><span class="pre">bind_functions()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.bind_tools"><code class="docutils literal notranslate"><span class="pre">bind_tools()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.configurable_alternatives"><code class="docutils literal notranslate"><span class="pre">configurable_alternatives()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.configurable_fields"><code class="docutils literal notranslate"><span class="pre">configurable_fields()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.get_num_tokens"><code class="docutils literal notranslate"><span class="pre">get_num_tokens()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.get_num_tokens_from_messages"><code class="docutils literal notranslate"><span class="pre">get_num_tokens_from_messages()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.get_token_ids"><code class="docutils literal notranslate"><span class="pre">get_token_ids()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.invoke"><code class="docutils literal notranslate"><span class="pre">invoke()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.stream"><code class="docutils literal notranslate"><span class="pre">stream()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.with_alisteners"><code class="docutils literal notranslate"><span class="pre">with_alisteners()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.with_config"><code class="docutils literal notranslate"><span class="pre">with_config()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.with_fallbacks"><code class="docutils literal notranslate"><span class="pre">with_fallbacks()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.with_listeners"><code class="docutils literal notranslate"><span class="pre">with_listeners()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.with_retry"><code class="docutils literal notranslate"><span class="pre">with_retry()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.with_structured_output"><code class="docutils literal notranslate"><span class="pre">with_structured_output()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#langchain_openai.chat_models.base.ChatOpenAI.with_types"><code class="docutils literal notranslate"><span class="pre">with_types()</span></code></a></li>
</ul>
</li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>
<footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
<div class="footer-items__start">
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2023, LangChain Inc.
      <br/>
</p>
</div>
</div>
</div>
</footer>
</body>
</html>